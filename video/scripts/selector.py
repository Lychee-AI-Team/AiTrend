#!/usr/bin/env python3
"""
çƒ­ç‚¹ç²¾é€‰è„šæœ¬
ä»24å°æ—¶æ•°æ®ä¸­ç²¾é€‰5-10æ¡çƒ­ç‚¹
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ  AiTrend è·¯å¾„
sys.path.insert(0, '/home/ubuntu/.openclaw/workspace/AiTrend')


class HotspotSelector:
    """çƒ­ç‚¹ç²¾é€‰å™¨"""
    
    def __init__(self, max_items: int = 8, min_heat_score: int = 50):
        """
        åˆå§‹åŒ–ç²¾é€‰å™¨
        
        Args:
            max_items: æœ€å¤§ç²¾é€‰æ•°é‡ï¼ˆé»˜è®¤8æ¡ï¼‰
            min_heat_score: æœ€å°çƒ­åº¦åˆ†æ•°é˜ˆå€¼
        """
        self.max_items = max_items
        self.min_heat_score = min_heat_score
    
    def select(self, input_file: str, output_file: str = None) -> Dict[str, Any]:
        """
        ä»è¾“å…¥æ–‡ä»¶ä¸­ç²¾é€‰çƒ­ç‚¹
        
        Args:
            input_file: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆdaily_raw_YYYY-MM-DD.jsonï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç²¾é€‰åçš„çƒ­ç‚¹æ•°æ®
        """
        # åŠ è½½æ•°æ®
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–æ‰€æœ‰é¡¹ç›®
        all_items = []
        for source in data.get('sources', []):
            source_name = source.get('source', 'unknown')
            for item in source.get('items', []):
                item['source_origin'] = source_name
                all_items.append(item)
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(all_items)} æ¡")
        
        # ç­›é€‰ï¼šçƒ­åº¦åˆ†æ•°è¾¾æ ‡
        filtered_items = [
            item for item in all_items 
            if item.get('heat_score', 0) >= self.min_heat_score
        ]
        
        print(f"ğŸ”¥ çƒ­åº¦è¾¾æ ‡(>{self.min_heat_score}): {len(filtered_items)} æ¡")
        
        # å»é‡ï¼šåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦
        deduplicated = self._deduplicate(filtered_items)
        print(f"ğŸ§¹ å»é‡å: {len(deduplicated)} æ¡")
        
        # æ’åºï¼šæŒ‰çƒ­åº¦åˆ†æ•°
        sorted_items = sorted(
            deduplicated, 
            key=lambda x: x.get('heat_score', 0), 
            reverse=True
        )
        
        # å¤šæ ·æ€§ç­›é€‰ï¼šåŒç±»ä¸»é¢˜ä¸è¶…è¿‡2æ¡
        selected = self._apply_diversity(sorted_items)
        
        print(f"âœ… æœ€ç»ˆç²¾é€‰: {len(selected)} æ¡")
        
        # æ„å»ºè¾“å‡º
        output = {
            'date': data.get('date', datetime.now().strftime('%Y-%m-%d')),
            'selection_time': datetime.now().isoformat(),
            'selection_params': {
                'max_items': self.max_items,
                'min_heat_score': self.min_heat_score
            },
            'selected_count': len(selected),
            'hotspots': selected
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
        
        return output
    
    def _deduplicate(self, items: List[Dict]) -> List[Dict]:
        """åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡"""
        unique_items = []
        seen_titles = []
        
        for item in items:
            title = item.get('title', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ ‡é¢˜ç›¸ä¼¼
            is_duplicate = False
            for seen in seen_titles:
                # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼šåŒ…å«å…³ç³»æˆ–ç¼–è¾‘è·ç¦»
                if self._is_similar(title, seen):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_items.append(item)
                seen_titles.append(title)
        
        return unique_items
    
    def _is_similar(self, title1: str, title2: str, threshold: float = 0.7) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼"""
        # ç®€å•å®ç°ï¼šåŒ…å«æ£€æŸ¥
        if title1 in title2 or title2 in title1:
            return True
        
        # è®¡ç®—ç®€å•ç›¸ä¼¼åº¦ï¼ˆå…±åŒå­—ç¬¦æ¯”ä¾‹ï¼‰
        set1 = set(title1)
        set2 = set(title2)
        if not set1 or not set2:
            return False
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        similarity = intersection / union if union > 0 else 0
        
        return similarity > threshold
    
    def _apply_diversity(self, items: List[Dict]) -> List[Dict]:
        """åº”ç”¨å¤šæ ·æ€§ç­›é€‰ï¼šåŒç±»ä¸»é¢˜ä¸è¶…è¿‡2æ¡"""
        selected = []
        category_count = {}
        
        for item in items:
            # è·å–ç±»åˆ«ï¼ˆå¦‚æœæ²¡æœ‰åˆ™åŸºäºæ¥æºæ¨æ–­ï¼‰
            category = item.get('category', 'å…¶ä»–')
            if category == 'å…¶ä»–':
                # åŸºäºæ¥æºæ¨æ–­ç±»åˆ«
                source = item.get('source_origin', '')
                category = self._infer_category(source)
            
            # æ£€æŸ¥ç±»åˆ«æ•°é‡
            if category_count.get(category, 0) < 2:
                # æ·»åŠ æ’åä¿¡æ¯
                item['rank'] = len(selected) + 1
                selected.append(item)
                category_count[category] = category_count.get(category, 0) + 1
            
            if len(selected) >= self.max_items:
                break
        
        return selected
    
    def _infer_category(self, source: str) -> str:
        """åŸºäºæ¥æºæ¨æ–­ç±»åˆ«"""
        category_map = {
            'hackernews': 'æŠ€æœ¯å¼€å‘',
            'producthunt': 'äº§å“å‘å¸ƒ',
            'github_trending': 'å¼€æºé¡¹ç›®',
            'reddit': 'ç¤¾åŒºè®¨è®º',
            'twitter': 'ç¤¾äº¤åª’ä½“',
            'moltbook': 'AIç¤¾åŒº',
            'tavily': 'AIæ–°é—»'
        }
        return category_map.get(source.lower(), 'å…¶ä»–')


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='çƒ­ç‚¹ç²¾é€‰è„šæœ¬')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max', '-m', type=int, default=8, help='æœ€å¤§ç²¾é€‰æ•°é‡')
    parser.add_argument('--min-heat', type=int, default=50, help='æœ€å°çƒ­åº¦åˆ†æ•°')
    
    args = parser.parse_args()
    
    selector = HotspotSelector(max_items=args.max, min_heat_score=args.min_heat)
    result = selector.select(args.input, args.output)
    
    print(f"\nğŸ“‹ ç²¾é€‰ç»“æœ:")
    for item in result['hotspots']:
        print(f"  {item['rank']}. {item.get('title', 'N/A')[:50]}... (çƒ­åº¦: {item.get('heat_score', 0)})")


if __name__ == '__main__':
    main()
