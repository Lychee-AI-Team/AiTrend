#!/usr/bin/env python3
"""
AiTrend å‘å¸ƒæ—¥å¿—åˆ†æžå·¥å…·
ç”¨äºŽè§‚å¯Ÿå‘å¸ƒå†…å®¹è´¨é‡å’Œæ¥æºåˆ†å¸ƒ
"""

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict

LOG_FILE = os.path.join(os.path.dirname(__file__), '..', 'memory', 'sent_articles.json')
QUALITY_LOG = os.path.join(os.path.dirname(__file__), '..', 'memory', 'publish_quality.json')

def load_logs():
    """åŠ è½½å‘å¸ƒæ—¥å¿—"""
    try:
        with open(LOG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"articles": []}

def load_quality_log():
    """åŠ è½½è´¨é‡æ—¥å¿—"""
    try:
        with open(QUALITY_LOG, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"sessions": []}

def analyze_sources(articles):
    """åˆ†æžæ¥æºåˆ†å¸ƒ"""
    sources = defaultdict(int)
    for article in articles:
        # ä»Žæ ‡é¢˜æˆ–URLæŽ¨æ–­æ¥æº
        title = article.get('title', '')
        url = article.get('url', '')
        
        if 'Product Hunt' in title or 'producthunt' in url.lower():
            sources['Product Hunt'] += 1
        elif 'HackerNews' in title or 'news.ycombinator' in url:
            sources['HackerNews'] += 1
        elif 'github' in url.lower():
            sources['GitHub'] += 1
        elif 'reddit' in url.lower():
            sources['Reddit'] += 1
        elif 'tavily' in str(article.get('metadata', {})).lower():
            sources['Tavily'] += 1
        else:
            sources['Other'] += 1
    
    return sources

def analyze_time_distribution(articles):
    """åˆ†æžæ—¶é—´åˆ†å¸ƒ"""
    hours = defaultdict(int)
    for article in articles:
        sent_time = datetime.fromtimestamp(article['sent_at'])
        hour_key = sent_time.strftime('%H:00')
        hours[hour_key] += 1
    
    return sorted(hours.items())

def generate_report():
    """ç”Ÿæˆåˆ†æžæŠ¥å‘Š"""
    data = load_logs()
    articles = data.get('articles', [])
    
    print("ðŸ“Š AiTrend å‘å¸ƒè´¨é‡æŠ¥å‘Š")
    print("=" * 70)
    print(f"\nðŸ“ˆ æ€»è®¡å‘å¸ƒ: {len(articles)} æ¡")
    
    # 24å°æ—¶ç»Ÿè®¡
    twenty_four_hours_ago = datetime.now().timestamp() - 86400
    recent_articles = [a for a in articles if a['sent_at'] > twenty_four_hours_ago]
    print(f"ðŸ“… 24å°æ—¶å†…: {len(recent_articles)} æ¡")
    
    # 7å¤©ç»Ÿè®¡
    seven_days_ago = datetime.now().timestamp() - 604800
    week_articles = [a for a in articles if a['sent_at'] > seven_days_ago]
    print(f"ðŸ“† 7å¤©å†…: {len(week_articles)} æ¡")
    
    # æ¥æºåˆ†æž
    print("\nðŸ“Œ æ¥æºåˆ†å¸ƒ:")
    sources = analyze_sources(articles)
    total = sum(sources.values())
    for source, count in sorted(sources.items(), key=lambda x: -x[1]):
        pct = count / total * 100 if total > 0 else 0
        bar = "â–ˆ" * int(pct / 5)
        print(f"  {source:15} {count:3}æ¡ ({pct:5.1f}%) {bar}")
    
    # æ—¶é—´åˆ†å¸ƒï¼ˆæœ€è¿‘24å°æ—¶ï¼‰
    if recent_articles:
        print("\nðŸ• ä»Šæ—¥å‘å¸ƒæ—¶æ®µ:")
        hours = defaultdict(int)
        for a in recent_articles:
            sent_time = datetime.fromtimestamp(a['sent_at'])
            hour_key = sent_time.strftime('%H:00')
            hours[hour_key] += 1
        
        for hour, count in sorted(hours.items()):
            bar = "â—" * count
            print(f"  {hour}: {bar} ({count})")
    
    # æœ€è¿‘å‘å¸ƒè¯¦æƒ…
    print("\nðŸ“ æœ€è¿‘5æ¡å‘å¸ƒ:")
    for article in articles[-5:]:
        sent_time = datetime.fromtimestamp(article['sent_at']).strftime('%m-%d %H:%M')
        title = article['title'][:45] + "..." if len(article['title']) > 45 else article['title']
        print(f"  {sent_time} | {title}")
    
    # é‡å¤å†…å®¹æ£€æŸ¥
    urls = [a['url'] for a in articles]
    duplicates = len(urls) - len(set(urls))
    if duplicates > 0:
        print(f"\nâš ï¸ é‡å¤å†…å®¹: {duplicates} æ¡")
    else:
        print("\nâœ… æ— é‡å¤å†…å®¹")
    
    # è´¨é‡è¯„åˆ†ï¼ˆåŸºäºŽæ ‡é¢˜é•¿åº¦å’Œæ¥æºå¤šæ ·æ€§ï¼‰
    avg_title_len = sum(len(a['title']) for a in articles) / len(articles) if articles else 0
    source_diversity = len(sources) / len(articles) * 100 if articles else 0
    
    print(f"\nðŸ“Š è´¨é‡æŒ‡æ ‡:")
    print(f"  å¹³å‡æ ‡é¢˜é•¿åº¦: {avg_title_len:.0f} å­—ç¬¦")
    print(f"  æ¥æºå¤šæ ·æ€§: {len(sources)} ä¸ªæ¥æº")
    print(f"  å¹³å‡å‘å¸ƒé¢‘çŽ‡: {len(articles) / max(len(set([datetime.fromtimestamp(a['sent_at']).strftime('%Y-%m-%d') for a in articles])), 1):.1f} æ¡/å¤©")
    
    print("\n" + "=" * 70)

def log_publish_session(articles, success_count, duration_ms):
    """è®°å½•å‘å¸ƒä¼šè¯"""
    quality_data = load_quality_log()
    
    session = {
        "timestamp": datetime.now().isoformat(),
        "total_selected": len(articles),
        "success_count": success_count,
        "duration_ms": duration_ms,
        "sources": list(set(a.source for a in articles)),
        "titles": [a.get('title', '') for a in articles]
    }
    
    quality_data["sessions"].append(session)
    
    # åªä¿ç•™æœ€è¿‘30å¤©çš„è®°å½•
    thirty_days_ago = datetime.now() - timedelta(days=30)
    quality_data["sessions"] = [
        s for s in quality_data["sessions"]
        if datetime.fromisoformat(s["timestamp"]) > thirty_days_ago
    ]
    
    with open(QUALITY_LOG, 'w', encoding='utf-8') as f:
        json.dump(quality_data, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    generate_report()
