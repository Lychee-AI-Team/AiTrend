#!/usr/bin/env python3
"""
AiTrend å¤šæºè¦†ç›–æµ‹è¯•ç³»ç»Ÿ
ç¡®ä¿å†…å®¹æ¥æºåˆ†å¸ƒå‡è¡¡ã€è´¨é‡è¾¾æ ‡
"""

import json
import os
import sys
from datetime import datetime
from collections import Counter
from typing import Dict, List, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

TEST_LOG_PATH = os.path.join(os.path.dirname(__file__), '..', 'memory', 'test_log.json')

class MultiSourceTester:
    """å¤šæºè¦†ç›–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.requirements = {
            'min_sources': 3,           # è‡³å°‘3ä¸ªä¸åŒæº
            'max_source_percentage': 0.4,  # å•æºæœ€å¤š40%
            'must_include_realtime': True,   # å¿…é¡»æœ‰å®æ—¶æº
            'must_include_deep': True,       # å¿…é¡»æœ‰æ·±åº¦æº
        }
        self.realtime_sources = ['twitter', 'producthunt']
        self.deep_sources = ['hackernews', 'reddit']
    
    def test_source_distribution(self, articles: List[Dict]) -> Dict:
        """æµ‹è¯•æ¥æºåˆ†å¸ƒæ˜¯å¦å‡è¡¡"""
        results = {
            'passed': True,
            'score': 0,
            'issues': [],
            'details': {}
        }
        
        # ç»Ÿè®¡æ¥æº
        sources = [a.get('source', 'unknown') for a in articles]
        source_counts = Counter(sources)
        total = len(articles)
        
        results['details']['source_distribution'] = dict(source_counts)
        results['details']['total_articles'] = total
        results['details']['unique_sources'] = len(source_counts)
        
        # æµ‹è¯•1ï¼šè‡³å°‘3ä¸ªä¸åŒæº
        if len(source_counts) < self.requirements['min_sources']:
            results['passed'] = False
            results['issues'].append(
                f"æ¥æºä¸è¶³: åªæœ‰{len(source_counts)}ä¸ªæºï¼Œéœ€è¦è‡³å°‘{self.requirements['min_sources']}ä¸ª"
            )
        else:
            results['score'] += 30
        
        # æµ‹è¯•2ï¼šå•æºä¸è¶…è¿‡40%
        for source, count in source_counts.items():
            percentage = count / total
            if percentage > self.requirements['max_source_percentage']:
                results['passed'] = False
                results['issues'].append(
                    f"æ¥æºä¸å‡è¡¡: {source}å æ¯”{percentage:.1%}ï¼Œè¶…è¿‡{self.requirements['max_source_percentage']:.0%}"
                )
            else:
                results['score'] += 20
        
        # æµ‹è¯•3ï¼šå¿…é¡»åŒ…å«å®æ—¶æº
        has_realtime = any(s in self.realtime_sources for s in source_counts.keys())
        if self.requirements['must_include_realtime'] and not has_realtime:
            results['passed'] = False
            results['issues'].append("ç¼ºå°‘å®æ—¶æºï¼šéœ€è¦åŒ…å«Twitteræˆ–Product Hunt")
        else:
            results['score'] += 25
        
        # æµ‹è¯•4ï¼šå¿…é¡»åŒ…å«æ·±åº¦æº
        has_deep = any(s in self.deep_sources for s in source_counts.keys())
        if self.requirements['must_include_deep'] and not has_deep:
            results['passed'] = False
            results['issues'].append("ç¼ºå°‘æ·±åº¦æºï¼šéœ€è¦åŒ…å«HNæˆ–Reddit")
        else:
            results['score'] += 25
        
        return results
    
    def test_content_diversity(self, articles: List[Dict]) -> Dict:
        """æµ‹è¯•å†…å®¹ç±»å‹å¤šæ ·æ€§"""
        results = {
            'passed': True,
            'score': 0,
            'issues': [],
            'details': {}
        }
        
        # æ£€æµ‹å†…å®¹å…³é”®è¯
        type_keywords = {
            'ai_model': ['model', 'llm', 'gpt', 'ai', 'claude', 'gemini', 'openai'],
            'dev_tool': ['tool', 'cli', 'api', 'library', 'sdk', 'framework'],
            'product': ['app', 'product', 'platform', 'service'],
            'open_source': ['github', 'open source', 'å¼€æº'],
            'research': ['paper', 'research', 'study', 'novel']
        }
        
        type_counts = Counter()
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            text = title + ' ' + summary
            
            detected_types = []
            for type_name, keywords in type_keywords.items():
                if any(kw in text for kw in keywords):
                    detected_types.append(type_name)
            
            if detected_types:
                type_counts[detected_types[0]] += 1
            else:
                type_counts['other'] += 1
        
        results['details']['type_distribution'] = dict(type_counts)
        
        # æµ‹è¯•ï¼šè‡³å°‘3ç§ä¸åŒç±»å‹
        if len(type_counts) >= 3:
            results['score'] = 100
        elif len(type_counts) >= 2:
            results['score'] = 70
            results['issues'].append(f"ç±»å‹å•ä¸€ï¼šåªæœ‰{len(type_counts)}ç§ç±»å‹")
        else:
            results['passed'] = False
            results['score'] = 40
            results['issues'].append(f"ç±»å‹ä¸¥é‡ä¸è¶³ï¼šåªæœ‰{len(type_counts)}ç§ç±»å‹")
        
        return results
    
    def test_information_density(self, articles: List[Dict]) -> Dict:
        """æµ‹è¯•ä¿¡æ¯å¯†åº¦"""
        results = {
            'passed': True,
            'score': 0,
            'issues': [],
            'details': {}
        }
        
        empty_phrases = [
            'é’ˆå¯¹ç—›ç‚¹', 'è§£å†³éœ€æ±‚', 'åŠŸèƒ½è®¾è®¡', 'åŠ¡å®', 'ä¸“æ³¨',
            'è®¨è®ºçš„ç„¦ç‚¹', 'å…³æ³¨ç‚¹ä¸»è¦', 'ä»...æ¥çœ‹', 'æ•´ä½“æ¥è¯´',
            'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å'
        ]
        
        scores = []
        for article in articles:
            content = article.get('content', '')
            word_count = len(content.replace(' ', '').replace('\n', ''))
            
            # æ£€æŸ¥ç©ºè¯æ•°é‡
            empty_count = sum(1 for phrase in empty_phrases if phrase in content)
            
            # æ£€æŸ¥å…·ä½“ä¿¡æ¯
            has_numbers = any(char.isdigit() for char in content)
            has_tech_detail = any(kw in content.lower() for kw in ['ä½¿ç”¨', 'åŸºäº', 'é‡‡ç”¨', 'ä»£ç ', 'æ¶æ„'])
            has_usage = any(kw in content.lower() for kw in ['å®‰è£…', 'ä½¿ç”¨', 'è¿è¡Œ', 'é…ç½®'])
            
            # è®¡ç®—å•ç¯‡å¾—åˆ†
            score = 0
            if word_count >= 200: score += 20
            if word_count >= 400: score += 20
            if has_numbers: score += 20
            if has_tech_detail: score += 20
            if has_usage: score += 20
            score -= empty_count * 10  # ç©ºè¯æ‰£åˆ†
            
            scores.append(max(0, score))
        
        avg_score = sum(scores) / len(scores) if scores else 0
        results['score'] = avg_score
        results['details']['individual_scores'] = scores
        results['details']['average_score'] = avg_score
        
        if avg_score < 60:
            results['passed'] = False
            results['issues'].append(f"ä¿¡æ¯å¯†åº¦ä¸è¶³ï¼šå¹³å‡åˆ†{avg_score:.1f}ï¼Œéœ€è¦â‰¥60")
        
        return results
    
    def run_full_test(self, articles: List[Dict]) -> Dict:
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ§ª AiTrend å¤šæºè¦†ç›–æµ‹è¯•")
        print("="*60)
        
        # æµ‹è¯•1ï¼šæ¥æºåˆ†å¸ƒ
        print("\nğŸ“Š æµ‹è¯•1: æ¥æºåˆ†å¸ƒ...")
        source_test = self.test_source_distribution(articles)
        print(f"  ç»“æœ: {'âœ… é€šè¿‡' if source_test['passed'] else 'âŒ å¤±è´¥'}")
        print(f"  å¾—åˆ†: {source_test['score']}/100")
        print(f"  æ¥æº: {source_test['details']['unique_sources']}ä¸ª")
        for issue in source_test['issues'][:2]:
            print(f"  âš ï¸ {issue}")
        
        # æµ‹è¯•2ï¼šå†…å®¹å¤šæ ·æ€§
        print("\nğŸ“Š æµ‹è¯•2: å†…å®¹ç±»å‹å¤šæ ·æ€§...")
        diversity_test = self.test_content_diversity(articles)
        print(f"  ç»“æœ: {'âœ… é€šè¿‡' if diversity_test['passed'] else 'âŒ å¤±è´¥'}")
        print(f"  å¾—åˆ†: {diversity_test['score']}/100")
        print(f"  ç±»å‹: {list(diversity_test['details']['type_distribution'].keys())}")
        
        # æµ‹è¯•3ï¼šä¿¡æ¯å¯†åº¦
        print("\nğŸ“Š æµ‹è¯•3: ä¿¡æ¯å¯†åº¦...")
        density_test = self.test_information_density(articles)
        print(f"  ç»“æœ: {'âœ… é€šè¿‡' if density_test['passed'] else 'âŒ å¤±è´¥'}")
        print(f"  å¾—åˆ†: {density_test['score']:.1f}/100")
        
        # æ±‡æ€»
        total_score = (source_test['score'] + diversity_test['score'] + density_test['score']) / 3
        all_passed = source_test['passed'] and diversity_test['passed'] and density_test['passed']
        
        print("\n" + "="*60)
        print("ğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        print(f"æ¥æºåˆ†å¸ƒ: {source_test['score']}/100")
        print(f"ç±»å‹å¤šæ ·: {diversity_test['score']}/100")
        print(f"ä¿¡æ¯å¯†åº¦: {density_test['score']:.1f}/100")
        print(f"æ€»å¹³å‡åˆ†: {total_score:.1f}/100")
        print(f"æ•´ä½“çŠ¶æ€: {'âœ… é€šè¿‡' if all_passed else 'âŒ æœªé€šè¿‡'}")
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_score': total_score,
            'passed': all_passed,
            'source_test': source_test,
            'diversity_test': diversity_test,
            'density_test': density_test
        }

def main():
    """æµ‹è¯•å…¥å£"""
    # ç¤ºä¾‹ï¼šæµ‹è¯•ä¸€æ‰¹å†…å®¹
    test_articles = [
        {'source': 'hackernews', 'title': 'Test', 'content': '...'},
        # æ›´å¤šæµ‹è¯•æ•°æ®...
    ]
    
    tester = MultiSourceTester()
    results = tester.run_full_test(test_articles)
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open(TEST_LOG_PATH, 'w') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    main()
