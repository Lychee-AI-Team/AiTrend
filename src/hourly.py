#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆ
æ¯ç¯‡å†…å®¹åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆï¼Œç¡®ä¿ç‹¬ç‰¹æ€§
"""

import json
import sys
import os
import time
import random
import hashlib
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« ï¼Œæ¯ä¸ªæ•°æ®æºæœ€å¤š 30 ç§’"""
    import signal
    
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            articles = []
            try:
                # ä½¿ç”¨ä¿¡å·è®¾ç½®ç¡¬æ€§è¶…æ—¶ï¼ˆä»… Unix/Linuxï¼‰
                def timeout_handler(signum, frame):
                    raise TimeoutError(f"{source.name} è¶…æ—¶")
                
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30 ç§’è¶…æ—¶
                
                try:
                    articles = source.fetch()
                finally:
                    signal.alarm(0)  # å–æ¶ˆé—¹é’Ÿ
                    signal.signal(signal.SIGALRM, old_handler)
                
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except TimeoutError as e:
                print(f"âœ— {source.name}: è¶…æ—¶ (30s)", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜"""
    title = article.title
    summary = article.summary or ""
    
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»summaryæå–æ ¸å¿ƒåŠŸèƒ½ï¼ˆå‰50å­—ï¼‰
    highlight = summary[:50].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨å¯ä»¥")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_ati_id() -> str:
    """ç”Ÿæˆ ATI å†…å®¹ ID - æ ¼å¼: ATI-YYYYMMDD-[6å­—ç¬¦åå…­è¿›åˆ¶]"""
    import hashlib
    now = datetime.now()
    date_str = now.strftime("%Y%m%d")
    # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³ç”Ÿæˆ6å­—ç¬¦åå…­è¿›åˆ¶å“ˆå¸Œ
    hash_input = f"{now.timestamp()}{random.randint(1000, 9999)}"
    short_hash = hashlib.sha256(hash_input.encode()).hexdigest()[:6].upper()
    return f"ATI-{date_str}-{short_hash}"

def generate_unique_content(article: Article, is_test: bool = False) -> str:
    """
    åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆå®Œå…¨ç‹¬ç‰¹çš„å†…å®¹
    ä½¿ç”¨LLMç”Ÿæˆï¼Œç¦æ­¢æ¨¡æ¿åŒ–æ–‡å­—
    
    Args:
        article: æ–‡ç« æ•°æ®
        is_test: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œæµ‹è¯•æ¨¡å¼ä¼šæ·»åŠ ATI ID
    """
    from .llm_content_generator import get_llm_generator
    
    # ä½¿ç”¨LLMç”Ÿæˆç‹¬ç‰¹å†…å®¹
    generator = get_llm_generator()
    
    article_data = {
        'title': article.title,
        'summary': article.summary or '',
        'url': article.url,
        'source': article.source,
        'metadata': article.metadata or {}
    }
    
    content = generator.generate(article_data)
    
    # æµ‹è¯•æ¨¡å¼æ·»åŠ  ATI IDï¼ˆä¸çœŸå®å†…å®¹æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
    if is_test:
        ati_id = generate_ati_id()
        # å°† ATI ID æ·»åŠ åˆ°å†…å®¹æœ«å°¾ï¼ˆåœ¨URLä¹‹åï¼‰
        content = f"{content}\n\nATI ID: {ati_id}"
    
    return content

def post_single_article(article: Article, webhook_url: str, delay: int = 0, is_test: bool = False) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›
    
    Args:
        article: æ–‡ç« æ•°æ®
        webhook_url: Webhook URL
        delay: å»¶è¿Ÿç§’æ•°
        is_test: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
    """
    if delay > 0:
        time.sleep(delay)
    
    content = generate_unique_content(article, is_test=is_test)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°
    
    æ”¯æŒå‚æ•°:
        --test: æµ‹è¯•æ¨¡å¼ï¼ˆè·³è¿‡å»é‡ï¼Œæ·»åŠ ATI IDï¼‰
        python3 -m src.hourly --test
    """
    start_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼æˆ–å…¨é‡æµ‹è¯•æ¨¡å¼
    is_test_mode = '--test' in sys.argv or os.getenv('AITREND_TEST_MODE') == '1'
    is_full_test_mode = '--full-test' in sys.argv or os.getenv('AITREND_FULL_TEST_MODE') == '1'
    
    if is_full_test_mode:
        print("ğŸ§ªğŸ”¥ AiTrend å…¨é‡æµ‹è¯•æ¨¡å¼ï¼ˆæ‰€æœ‰æ•°æ®æºï¼Œæœ€å¤§åŒ–è¾“å‡ºï¼Œè·³è¿‡å»é‡ï¼‰", file=sys.stderr)
    elif is_test_mode:
        print("ğŸ§ª AiTrend æµ‹è¯•æ¨¡å¼ï¼ˆè·³è¿‡å»é‡ï¼Œæ·»åŠ ATI IDï¼‰", file=sys.stderr)
    else:
        print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡ï¼ˆæµ‹è¯•æ¨¡å¼è·³è¿‡ï¼‰
    deduplicator = ArticleDeduplicator()  # å§‹ç»ˆåˆ›å»ºï¼Œæµ‹è¯•æ¨¡å¼ä¸ä½¿ç”¨
    if is_test_mode:
        articles = all_articles
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: è·³è¿‡å»é‡æ£€æŸ¥", file=sys.stderr)
    else:
        articles = deduplicator.filter_new_articles(all_articles)
        print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    # URLå»é‡ï¼ˆä¿æŒURLå”¯ä¸€æ€§ï¼Œå³ä½¿æ˜¯æµ‹è¯•æ¨¡å¼ï¼‰
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    if not is_test_mode:
        print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æ–‡ç« ï¼ˆå…¨é‡æµ‹è¯•æ¨¡å¼è¾“å‡ºæ‰€æœ‰ï¼Œæ™®é€šæ¨¡å¼é™åˆ¶æ•°é‡ï¼‰
    if is_full_test_mode:
        # å…¨é‡æµ‹è¯•ï¼šè¾“å‡ºæ‰€æœ‰æ”¶é›†åˆ°çš„å†…å®¹ï¼ˆæŒ‰çƒ­åº¦æ’åºï¼‰
        top_articles = select_best_articles(articles, top_n=len(articles))
        print(f"\nğŸ”¥ å…¨é‡æµ‹è¯•æ¨¡å¼: é€‰ä¸­ {len(top_articles)} æ¡ (æœ€å¤§åŒ–è¾“å‡º)", file=sys.stderr)
    else:
        # æ™®é€šæ¨¡å¼ï¼šé€‰æ‹©æœ€çƒ­é—¨çš„3æ¡ï¼Œç¡®ä¿å¤šæ ·æ€§
        top_articles = select_best_articles(articles, top_n=5)  # å…ˆé€‰5æ¡
        
        # ç¡®ä¿æ¥æºå¤šæ ·æ€§
        source_count = {}
        diverse_articles = []
        for article in top_articles:
            src = article.source
            if source_count.get(src, 0) < 2:  # æ¯ä¸ªæ¥æºæœ€å¤š2æ¡
                diverse_articles.append(article)
                source_count[src] = source_count.get(src, 0) + 1
            if len(diverse_articles) >= 3:
                break
        
        top_articles = diverse_articles[:3]
    
    if not is_full_test_mode:
        print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡ (å·²ä¼˜åŒ–æ¥æºå¤šæ ·æ€§):", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. [{article.source}] {article.title[:45]}...", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    mode_str = ""
    if is_full_test_mode:
        mode_str = "å…¨é‡æµ‹è¯•"
    elif is_test_mode:
        mode_str = "æµ‹è¯•"
    
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ{mode_str}å†…å®¹...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        # å…¨é‡æµ‹è¯•æˆ–æµ‹è¯•æ¨¡å¼éƒ½æ·»åŠ ATI ID
        is_test_flag = is_test_mode or is_full_test_mode
        result = post_single_article(article, webhook_url, delay=delay, is_test=is_test_flag)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result,
            'is_test': is_test_flag
        })
        status = "âœ…" if result else "âŒ"
        test_mark = " [TEST]" if is_test_flag else ""
        print(f"   {status} ç¬¬{i+1}æ¡{test_mark}å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€ï¼ˆæµ‹è¯•æ¨¡å¼ä¸è®°å½•ï¼‰
    if not is_test_mode and not is_full_test_mode:
        deduplicator.record_sent_articles(top_articles)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
