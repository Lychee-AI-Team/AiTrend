#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆ
æ¯ç¯‡å†…å®¹åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆï¼Œç¡®ä¿ç‹¬ç‰¹æ€§
"""

import json
import sys
import os
import time
import random
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« ï¼Œæ¯ä¸ªæ•°æ®æºæœ€å¤š 30 ç§’"""
    import signal
    
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            articles = []
            try:
                # ä½¿ç”¨ä¿¡å·è®¾ç½®ç¡¬æ€§è¶…æ—¶ï¼ˆä»… Unix/Linuxï¼‰
                def timeout_handler(signum, frame):
                    raise TimeoutError(f"{source.name} è¶…æ—¶")
                
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30 ç§’è¶…æ—¶
                
                try:
                    articles = source.fetch()
                finally:
                    signal.alarm(0)  # å–æ¶ˆé—¹é’Ÿ
                    signal.signal(signal.SIGALRM, old_handler)
                
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except TimeoutError as e:
                print(f"âœ— {source.name}: è¶…æ—¶ (30s)", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜"""
    title = article.title
    summary = article.summary or ""
    
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»summaryæå–æ ¸å¿ƒåŠŸèƒ½ï¼ˆå‰50å­—ï¼‰
    highlight = summary[:50].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨å¯ä»¥")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_unique_content(article: Article) -> str:
    """
    åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆå®Œå…¨ç‹¬ç‰¹çš„å†…å®¹
    ä½¿ç”¨LLMç”Ÿæˆï¼Œç¦æ­¢æ¨¡æ¿åŒ–æ–‡å­—
    """
    from .llm_content_generator import get_llm_generator
    
    # ä½¿ç”¨LLMç”Ÿæˆç‹¬ç‰¹å†…å®¹
    generator = get_llm_generator()
    
    article_data = {
        'title': article.title,
        'summary': article.summary or '',
        'url': article.url,
        'source': article.source,
        'metadata': article.metadata or {}
    }
    
    return generator.generate(article_data)

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_unique_content(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡ï¼Œç¡®ä¿å¤šæ ·æ€§ï¼ˆä¼˜å…ˆä¸åŒæ¥æºï¼‰
    top_articles = select_best_articles(articles, top_n=5)  # å…ˆé€‰5æ¡
    
    # ç¡®ä¿æ¥æºå¤šæ ·æ€§
    source_count = {}
    diverse_articles = []
    for article in top_articles:
        src = article.source
        if source_count.get(src, 0) < 2:  # æ¯ä¸ªæ¥æºæœ€å¤š2æ¡
            diverse_articles.append(article)
            source_count[src] = source_count.get(src, 0) + 1
        if len(diverse_articles) >= 3:
            break
    
    top_articles = diverse_articles[:3]
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡ (å·²ä¼˜åŒ–æ¥æºå¤šæ ·æ€§):", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. [{article.source}] {article.title[:45]}...", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
