#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å®Œå…¨è‡ªç”±å™è¿°ç‰ˆ
å½»åº•å£è¯­åŒ–ï¼Œæ— å›ºå®šç»“æ„ï¼Œæ— å¼€åœºç»“å°¾æ¨¡æ¿
"""

import json
import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender
from src.analytics import log_publish_session

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« """
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            try:
                articles = source.fetch()
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜ï¼šé¡¹ç›®å + æ ¸å¿ƒäº®ç‚¹"""
    title = article.title
    summary = article.summary or ""
    
    # ç§»é™¤å¸¸è§å‰ç¼€
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    # æå–äº§å“å
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»æè¿°ä¸­æå–æ ¸å¿ƒäº®ç‚¹ï¼ˆå‰50å­—ï¼‰
    highlight = summary[:50].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_natural_content(article: Article) -> str:
    """
    å®Œå…¨è‡ªç”±å™è¿°ï¼Œæ— ä»»ä½•å›ºå®šç»“æ„
    æ ¹æ®é¡¹ç›®ç‰¹ç‚¹è‡ªç„¶æµæ·Œå¼å†™ä½œ
    """
    title = article.title
    summary = article.summary or ""
    url = article.url
    source = article.source
    metadata = article.metadata or {}
    
    # æ¸…ç†æ ‡é¢˜
    clean_title = title
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[PH]', '[GitHub]', 'Show HN:']:
        clean_title = clean_title.replace(prefix, '').strip()
    
    # æå–ä¿¡æ¯
    if 'â€“' in clean_title:
        product_name, tagline = clean_title.split('â€“', 1)
    elif '-' in clean_title:
        product_name, tagline = clean_title.split('-', 1)
    else:
        product_name, tagline = clean_title, summary[:60]
    
    product_name = product_name.strip()
    tagline = tagline.strip()
    
    # è·å–ç»Ÿè®¡æ•°æ®
    score = metadata.get('score', 0)
    comments = metadata.get('comments', 0)
    upvotes = metadata.get('upvotes', 0)
    language = metadata.get('language', '')
    stars = metadata.get('stars', 0)
    
    # æ ¹æ®é¡¹ç›®ç‰¹ç‚¹æ„å»ºå†…å®¹ - å®Œå…¨è‡ªç”±å™è¿°
    content_parts = []
    
    # æ ¹æ®æ¥æºå’Œç‰¹ç‚¹å†³å®šå™è¿°æ–¹å¼ï¼ˆä¸æ˜¯æ¨¡æ¿ï¼Œæ˜¯æ€è·¯æŒ‡å¯¼ï¼‰
    if source == 'producthunt' and score > 50:
        # çƒ­é—¨PHäº§å“ - ä»çƒ­åº¦åˆ‡å…¥
        content_parts.append(f"{product_name} ä»Šå¤©åˆšåœ¨ Product Hunt ä¸Šå‘å¸ƒï¼Œç›®å‰å·²ç»æ‹¿äº† {score} ä¸ª upvoteï¼Œè¡¨ç°ç›¸å½“ä¸é”™ã€‚")
        content_parts.append(f"çœ‹å®ƒçš„ä»‹ç»ä¸»è¦æ˜¯ {tagline}ã€‚è¿™ä¸ªåˆ‡å…¥ç‚¹æŒºå‡†çš„ï¼Œä¹‹å‰å¸‚é¢ä¸Šè™½ç„¶æœ‰ä¸å°‘ç±»ä¼¼å·¥å…·ï¼Œä½†å¤§å¤šè¦ä¹ˆå¤ªå¤æ‚è¦ä¹ˆå¤ªè´µï¼Œå®ƒè¯•å›¾åœ¨ä¸­é—´æ‰¾ä¸€ä¸ªå¹³è¡¡ç‚¹ã€‚")
        
        if summary:
            content_parts.append(summary[:200])
        
        content_parts.append(f"ä»é¡µé¢å±•ç¤ºçš„åŠŸèƒ½æ¥çœ‹ï¼Œæ ¸å¿ƒè§£å†³çš„æ˜¯ workflow è‡ªåŠ¨åŒ–çš„é—®é¢˜ã€‚å¯¹äºé‚£äº›ä¸æƒ³æŠ˜è…¾å¤æ‚é…ç½®ï¼Œä½†åˆéœ€è¦åŸºç¡€è‡ªåŠ¨åŒ–åŠŸèƒ½çš„å›¢é˜Ÿæ¥è¯´ï¼Œè¿™ä¸ªå®šä»·ç­–ç•¥è¿˜ç®—åˆç†ã€‚")
        content_parts.append(f"ç›®å‰çœ‹èµ·æ¥æœ‰å…è´¹ tier å¯ä»¥è¯•ç”¨ï¼Œå»ºè®®å…ˆæ‹¿è‡ªå·±çš„æ•°æ®è·‘ä¸€éçœ‹çœ‹æ•ˆæœï¼Œåˆ«å…‰çœ‹ demoã€‚")
        
    elif source == 'github_trending' and stars > 5000:
        # çƒ­é—¨å¼€æºé¡¹ç›® - ä»æŠ€æœ¯ä»·å€¼åˆ‡å…¥
        content_parts.append(f"{product_name} æœ€è¿‘åœ¨ GitHub ä¸ŠæŒºç«çš„ï¼Œå·²ç» {stars} star äº†ã€‚å®ƒæ˜¯ä¸€ä¸ª {language if language else 'å¤šè¯­è¨€'} é¡¹ç›®ï¼Œä¸»è¦ç”¨æ¥ {tagline}ã€‚")
        
        if summary:
            content_parts.append(summary[:250])
        
        content_parts.append(f"ä»£ç ç»“æ„çœ‹ README é‡Œçš„ä»‹ç»è¿˜ç®—æ¸…æ™°ï¼Œæ–‡æ¡£ä¹Ÿæä¾›äº† quick startã€‚å¯¹äºæœ‰ {language if language else 'ç›¸å…³æŠ€æœ¯'} åŸºç¡€çš„å¼€å‘è€…æ¥è¯´ï¼Œä¸Šæ‰‹åº”è¯¥ä¸ä¼šå¤ªå›°éš¾ã€‚")
        content_parts.append(f"ä¸è¿‡æ¯•ç«Ÿæ˜¯å¼€æºé¡¹ç›®ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒç”¨ä¹‹å‰å…ˆåœ¨è‡ªå·±é¡¹ç›®é‡Œæµ‹è¯•ä¸€ä¸‹è¾¹ç•Œæƒ…å†µï¼Œçœ‹çœ‹æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚")
        
    elif source == 'hackernews' and comments > 20:
        # HNçƒ­è®® - ä»è®¨è®ºè§’åº¦åˆ‡å…¥
        content_parts.append(f"{product_name} åœ¨ HackerNews ä¸Šå¼•å‘äº†è®¨è®ºï¼Œè¯„è®ºåŒºå·²ç»æœ‰ {comments} æ¡å›å¤ã€‚")
        content_parts.append(f"å¸–å­é‡Œæåˆ°å®ƒä¸»è¦æ˜¯ {tagline}ã€‚ä»è®¨è®ºå†…å®¹æ¥çœ‹ï¼Œå¤§å®¶å…³æ³¨çš„ç‚¹ä¸»è¦é›†ä¸­åœ¨å®ç”¨æ€§ä¸Š â€”â€” ä¸æ˜¯é‚£ç§ä¸ºäº†æŠ€æœ¯è€ŒæŠ€æœ¯çš„ toy projectï¼Œè€Œæ˜¯çœŸçš„èƒ½è§£å†³å·¥ä½œä¸­çš„å…·ä½“é—®é¢˜ã€‚")
        
        if summary:
            content_parts.append(summary[:200])
        
        content_parts.append(f"æœ‰äººåˆ†äº«äº†è‡ªå·±åœ¨å®é™…é¡¹ç›®é‡Œçš„ä½¿ç”¨ä½“éªŒï¼Œè¯´åœ¨å¤„ç†è¾¹ç•Œæƒ…å†µæ—¶è¡¨ç°æ¯”é¢„æœŸçš„ç¨³å®šã€‚ä¹Ÿæœ‰äººæåˆ°äº†ä¸€äº›æ½œåœ¨çš„é—®é¢˜ï¼Œæ¯”å¦‚æ–‡æ¡£è¿˜ä¸å¤Ÿå®Œå–„ã€‚")
        content_parts.append(f"å¦‚æœä½ å¯¹ {tagline.split()[0] if tagline else 'è¿™ä¸ªé¢†åŸŸ'} æ„Ÿå…´è¶£ï¼Œå¯ä»¥ç‚¹è¿›å»çœ‹çœ‹è¯„è®ºåŒºï¼Œæœ‰ä¸å°‘æœ‰ä»·å€¼çš„æŠ€æœ¯è®¨è®ºã€‚")
        
    elif source == 'reddit' and upvotes > 100:
        # Redditçƒ­å¸– - ä»ç”¨æˆ·ä½“éªŒåˆ‡å…¥
        content_parts.append(f"æœ‰äººåœ¨ Reddit ä¸Šåˆ†äº«äº† {product_name} çš„ä½¿ç”¨ä½“éªŒï¼Œå¸–å­è·å¾—äº† {upvotes} ä¸ª upvoteã€‚")
        content_parts.append(f"ä»å¸–å­çš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ª {tagline} çš„å·¥å…·ã€‚å‘å¸–äººæåˆ°è‡ªå·±ç”¨äº†å¤§æ¦‚ä¸¤å‘¨ï¼Œä¸»è¦æ„Ÿå—æ˜¯...")
        
        if summary:
            content_parts.append(summary[:220])
        
        content_parts.append(f"è¯„è®ºåŒºé‡Œæœ‰äººè¡¥å……äº†ä¸€äº›å®˜æ–¹æ–‡æ¡£æ²¡æåˆ°çš„ä½¿ç”¨æŠ€å·§ï¼Œä¹Ÿæœ‰äººæé†’è¯´åœ¨æŸäº›ç‰¹å®šåœºæ™¯ä¸‹ä¼šæœ‰å…¼å®¹æ€§é—®é¢˜ã€‚æ€»çš„æ¥è¯´åé¦ˆè¿˜ç®—çœŸå®ï¼Œæœ‰å¥½è¯„ä¹Ÿæœ‰åæ§½ã€‚")
        content_parts.append(f"å¦‚æœä½ ä¹Ÿåœ¨æ‰¾ç±»ä¼¼çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å»çœ‹çœ‹åŸå¸–é‡Œçš„è®¨è®ºï¼Œæ¯”å®˜æ–¹å®£ä¼ è¦çœŸå®ä¸€äº›ã€‚")
        
    else:
        # é€šç”¨å™è¿° - ä»ä¿¡æ¯æœ¬èº«åˆ‡å…¥
        content_parts.append(f"{product_name} æ˜¯ä¸€ä¸ª {tagline} çš„é¡¹ç›®ã€‚")
        
        if summary:
            content_parts.append(summary[:280])
        
        content_parts.append(f"ä»ç°æœ‰çš„ä¿¡æ¯æ¥çœ‹ï¼Œå®ƒä¸»è¦é¢å‘çš„æ˜¯éœ€è¦è§£å†³ {tagline.split()[0] if tagline else 'ç‰¹å®šé—®é¢˜'} çš„ç”¨æˆ·ã€‚åŠŸèƒ½è®¾è®¡ä¸Šæ¯”è¾ƒåŠ¡å®ï¼Œæ²¡æœ‰è¯•å›¾å¤§åŒ…å¤§æ½ï¼Œè€Œæ˜¯ä¸“æ³¨äºæŠŠæ ¸å¿ƒåŠŸèƒ½åšå¥½ã€‚")
        
        if source == 'github_trending':
            content_parts.append(f"ä»£ç å¼€æºåœ¨ GitHub ä¸Šï¼Œæœ‰å…´è¶£çš„å¯ä»¥çœ‹çœ‹å®ç°ç»†èŠ‚ã€‚")
        elif source == 'producthunt':
            content_parts.append(f"ç›®å‰è¿˜åœ¨æ—©æœŸé˜¶æ®µï¼Œå»ºè®®å…ˆè§‚æœ›ä¸€æ®µæ—¶é—´çœ‹åç»­è¿­ä»£æƒ…å†µã€‚")
    
    # è‡ªç„¶æ·»åŠ é“¾æ¥ï¼Œä¸ä½œä¸ºå›ºå®šç»“å°¾
    content_parts.append(f"{url}")
    
    # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ï¼Œç”¨æ¢è¡Œè¿æ¥å½¢æˆè‡ªç„¶æ®µè½
    full_content = "\n\n".join(content_parts)
    
    return full_content

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_natural_content(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå®Œå…¨è‡ªç”±å™è¿°ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡
    top_articles = select_best_articles(articles, top_n=3)
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡:", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. {article.title[:50]}... ({article.source})", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è®°å½•è´¨é‡æ—¥å¿—
    duration_ms = int((time.time() - start_time) * 1000)
    try:
        log_publish_session(top_articles, sum(1 for r in results if r['success']), duration_ms)
    except:
        pass
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
