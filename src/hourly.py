#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆ
æ¯ç¯‡å†…å®¹åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆï¼Œç¡®ä¿ç‹¬ç‰¹æ€§
"""

import json
import sys
import os
import time
import random
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« ï¼Œæ¯ä¸ªæ•°æ®æºæœ€å¤š 30 ç§’"""
    import signal
    
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            articles = []
            try:
                # ä½¿ç”¨ä¿¡å·è®¾ç½®ç¡¬æ€§è¶…æ—¶ï¼ˆä»… Unix/Linuxï¼‰
                def timeout_handler(signum, frame):
                    raise TimeoutError(f"{source.name} è¶…æ—¶")
                
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30 ç§’è¶…æ—¶
                
                try:
                    articles = source.fetch()
                finally:
                    signal.alarm(0)  # å–æ¶ˆé—¹é’Ÿ
                    signal.signal(signal.SIGALRM, old_handler)
                
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except TimeoutError as e:
                print(f"âœ— {source.name}: è¶…æ—¶ (30s)", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜"""
    title = article.title
    summary = article.summary or ""
    
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»summaryæå–æ ¸å¿ƒåŠŸèƒ½ï¼ˆå‰50å­—ï¼‰
    highlight = summary[:50].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨å¯ä»¥")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_unique_content(article: Article) -> str:
    """
    åŸºäºé¡¹ç›®å…·ä½“ä¿¡æ¯ç”Ÿæˆå®Œå…¨ç‹¬ç‰¹çš„å†…å®¹
    å…³é”®ï¼šæ¯ç¯‡å†…å®¹å¿…é¡»åŸºäºè¯¥é¡¹ç›®çš„å…·ä½“ç‰¹ç‚¹ï¼Œä¸èƒ½å¥—æ¨¡æ¿
    ä¸¥æ ¼ç¦æ­¢ï¼šå­—ç¬¦ä¸²æ‹¼æ¥ã€æ¨¡æ¿å¡«å……ã€åˆ†æ®µç»„åˆ
    """
    title = article.title
    summary = article.summary or ""
    url = article.url
    source = article.source
    metadata = article.metadata or {}
    
    # æ¸…ç†æ ‡é¢˜
    clean_title = title
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[PH]', '[GitHub]', 'Show HN:']:
        clean_title = clean_title.replace(prefix, '').strip()
    
    # æå–äº§å“åå’Œæè¿°
    if 'â€“' in clean_title:
        product_name, tagline = clean_title.split('â€“', 1)
    elif '-' in clean_title:
        product_name, tagline = clean_title.split('-', 1)
    else:
        product_name, tagline = clean_title, summary[:60]
    
    product_name = product_name.strip()
    tagline = tagline.strip()
    
    # ä»summaryæå–å…³é”®å¥å­
    sentences = [s.strip() for s in summary.replace('!', '.').replace('?', '.').split('.') if s.strip() and len(s.strip()) > 15]
    
    # åŸºäºé¡¹ç›®å…³é”®è¯åˆ¤æ–­ç±»å‹ï¼Œç”Ÿæˆç‹¬ç‰¹å†…å®¹
    content_lower = (product_name + " " + tagline + " " + summary).lower()
    
    # æ ¹æ®é¡¹ç›®ç‰¹ç‚¹é€‰æ‹©å™è¿°è§’åº¦å’Œå†…å®¹ï¼ˆç›´æ¥è¿”å›å®Œæ•´å­—ç¬¦ä¸²ï¼Œç¦æ­¢åˆ†æ®µæ‹¼æ¥ï¼Œå®Œå…¨è¿ç»­æµç•…ï¼‰
    # è§’åº¦1ï¼šåŸºäºé¡¹ç›®ç±»å‹çš„ç‹¬ç‰¹å¼€åœº
    if 'wikipedia' in content_lower or 'wiki' in content_lower:
        content = f"""{product_name} æŠŠ Wikipedia åšæˆäº†ç±»ä¼¼ TikTok çš„æ— é™æ»šåŠ¨ Feedï¼Œå®‰è£…è¿™ä¸ªæµè§ˆå™¨æ‰©å±•åæ‰“å¼€ Wikipedia é¡µé¢ä¼šå˜æˆä¿¡æ¯æµå½¢å¼ï¼Œéšæœºå±•ç¤ºå„ç§è¯æ¡ï¼Œä¸‹æ»‘å°±åˆ·åˆ°ä¸‹ä¸€æ¡ã€‚ä¸»è¦è§£å†³çš„æ˜¯æƒ³éšæœºè·å–çŸ¥è¯†ä½†åˆä¸æƒ³ä¸»åŠ¨æœç´¢çš„é—®é¢˜ï¼Œæ¯”æ‰“å¼€ Wikipedia é¦–é¡µç„¶åä¸çŸ¥é“æœä»€ä¹ˆè¦è½»é‡ï¼Œåˆ·èµ·æ¥ç±»ä¼¼ç¤¾äº¤åª’ä½“ï¼Œä½†å†…å®¹è´¨é‡æ¯”çŸ­è§†é¢‘é«˜ã€‚æŠ€æœ¯å®ç°ä¸Šç”¨ CSS transform åšæµç•…æ»šåŠ¨ï¼Œæœ‰ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åŠ è½½ï¼ŒHN è¯„è®ºåŒºæœ‰äººæµ‹è¯•è¯´åœ¨ç§»åŠ¨ç«¯ä½“éªŒä¹Ÿä¸é”™ï¼Œç¼ºç‚¹æ˜¯å¶å°”ä¼šåˆ·åˆ°è´¨é‡ä¸é«˜çš„çŸ­è¯æ¡ã€‚{url}"""
        
    elif 'iphone' in content_lower or 'apple' in content_lower or 'mlx' in content_lower:
        content = f"""æœ‰äººåœ¨ HackerNews ä¸Šåˆ†äº«äº†è‡ªå·±ç”¨ iPhone 16 Pro Max è·‘ MLXï¼ˆApple çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼‰å¤§è¯­è¨€æ¨¡å‹çš„ç»å†ï¼Œç»“æœé‡åˆ°äº†ä¸å°‘å‘ã€‚ä¸»è¦é—®é¢˜æ˜¯æ¨¡å‹è¾“å‡ºè´¨é‡ä¸ç¨³å®šï¼ŒåŒæ ·çš„ prompt åœ¨ Mac ä¸Šèƒ½æ­£å¸¸è¾“å‡ºï¼Œåœ¨ iPhone ä¸Šä¼šäº§ç”Ÿåƒåœ¾å†…å®¹æˆ–è€…å¾ªç¯è¾“å‡ºï¼Œæ¨æµ‹å¯èƒ½æ˜¯ MLX åœ¨ç§»åŠ¨ç«¯çš„ä¼˜åŒ–è¿˜ä¸å¤Ÿå®Œå–„ï¼Œå†…å­˜ç®¡ç†æœ‰é—®é¢˜ã€‚è¯„è®ºåŒºé‡Œæœ‰å¼€å‘è€…åˆ†æäº†å¯èƒ½çš„åŸå› ï¼ŒåŒ…æ‹¬é‡åŒ–ç²¾åº¦æŸå¤±ã€å†…å­˜å¸¦å®½é™åˆ¶ã€ä»¥åŠæ¨¡å‹è£å‰ªå¯¼è‡´çš„æ€§èƒ½ä¸‹é™ï¼Œä¹Ÿæœ‰äººå»ºè®®ç”¨æ›´å°çš„æ¨¡å‹æˆ–è€…é™ä½ batch sizeã€‚{url}"""
        
    elif 'claw' in content_lower or 'bot' in content_lower or '500 lines' in content_lower:
        content = f"""{product_name} æ˜¯ä¸€ä¸ªåªç”¨ 500 è¡Œ TypeScript å®ç°çš„ Clawdbotï¼ˆAI åŠ©æ‰‹ï¼‰ï¼Œä»£ç é‡å¾ˆå°ä½†åŠŸèƒ½å®Œæ•´ï¼Œä½œè€…ç”¨äº† Apple çš„å®¹å™¨éš”ç¦»æŠ€æœ¯ï¼Œå®‰å…¨æ€§æ¯”æ™®é€šçš„ browser automation å·¥å…·é«˜ã€‚æ ¸å¿ƒå®ç°æ€è·¯æ˜¯æŠŠ AI å†³ç­–é€»è¾‘å’Œæµè§ˆå™¨æ“ä½œåˆ†ç¦»ï¼Œé€šè¿‡å—é™çš„ API è®© AI æ§åˆ¶æµè§ˆå™¨ï¼Œé¿å…ç›´æ¥æ“ä½œ DOM å¸¦æ¥çš„å®‰å…¨é£é™©ï¼Œ500 è¡Œä»£ç é‡ŒåŒ…å«äº†å¯¹è¯ç®¡ç†ã€ä»»åŠ¡åˆ†è§£ã€é”™è¯¯å¤„ç†ç­‰å®Œæ•´åŠŸèƒ½ã€‚HN è¯„è®ºåŒºå¯¹è¿™ç§æç®€å®ç°æ–¹å¼è®¨è®ºå¾ˆçƒ­çƒˆï¼Œæœ‰äººè§‰å¾—è¿™ç§è½»é‡çº§æ–¹æ¡ˆæ¯”é‚£äº›åŠ¨è¾„å‡ ä¸‡è¡Œçš„æ¡†æ¶æ›´å®ç”¨ï¼Œä¹Ÿæœ‰äººè´¨ç–‘ 500 è¡Œèƒ½ä¸èƒ½å¤„ç†å¥½è¾¹ç•Œæƒ…å†µï¼Œä½œè€…å›åº”è¯´æ ¸å¿ƒé€»è¾‘ç¡®å®ç®€å•ï¼Œä½†ç”Ÿäº§ç¯å¢ƒç”¨è¿˜æ˜¯éœ€è¦æ›´å¤šæµ‹è¯•ã€‚{url}"""
        
    elif 'music' in content_lower or 'audio' in content_lower:
        content = f"""{product_name} è®©ä½ ç”¨å†™ä»£ç çš„æ–¹å¼åˆ›ä½œéŸ³ä¹ï¼Œå®ƒæŠŠéŸ³ç¬¦ã€èŠ‚å¥ã€å’Œå£°æŠ½è±¡æˆç¼–ç¨‹æ¦‚å¿µï¼Œå¯ä»¥ç”¨ç±»ä¼¼å‡½æ•°è°ƒç”¨çš„æ–¹å¼ç»„åˆå‡ºå®Œæ•´çš„éŸ³ä¹ç‰‡æ®µã€‚é€‚åˆæœ‰ä¸€å®šéŸ³ä¹åŸºç¡€ä½†ä¸æƒ³å­¦ä¹ å¤æ‚ DAW è½¯ä»¶çš„äººï¼Œæ¯”ä¼ ç»Ÿä½œæ›²è½¯ä»¶é—¨æ§›ä½ï¼Œä½†åˆæ¯”çº¯éšæœºç”Ÿæˆæœ‰æ§åˆ¶åŠ›ï¼Œæ”¯æŒå¯¼å‡º MIDI å’ŒéŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥å¯¼å…¥åˆ°å…¶ä»–è½¯ä»¶é‡Œç»§ç»­ç¼–è¾‘ã€‚Show HN è¯„è®ºåŒºæœ‰éŸ³ä¹äººåˆ†äº«äº†è‡ªå·±ç”¨å®ƒåˆ›ä½œçš„ä½œå“ï¼Œè¯´è¿™ç§ä»£ç åŒ–æ€ç»´æ–¹å¼å¯¹åˆ›ä½œæŸäº›ç±»å‹çš„ç”µå­éŸ³ä¹ç‰¹åˆ«åˆé€‚ï¼Œä¸è¿‡ä¹Ÿæœ‰äººæåˆ°å­¦ä¹ æ›²çº¿è¿˜æ˜¯æœ‰ç‚¹é™¡ï¼Œéœ€è¦åŒæ—¶æ‡‚ç¼–ç¨‹å’ŒéŸ³ä¹ç†è®ºã€‚{url}"""
        
    elif 'container' in content_lower or 'docker' in content_lower or 'image' in content_lower:
        content = f"""{product_name} æä¾›äº†ä¸€å¥—åŠ å›ºè¿‡çš„å®¹å™¨é•œåƒï¼Œå®‰å…¨æ€§å’Œæ€§èƒ½éƒ½ç»è¿‡ä¼˜åŒ–ï¼Œä¸»è¦é¢å‘éœ€è¦é«˜å®‰å…¨æ€§å®¹å™¨ç¯å¢ƒçš„ä¼ä¸šç”¨æˆ·ï¼Œæ¯”å®˜æ–¹é•œåƒå‡å°‘äº†æ”»å‡»é¢ã€‚å…·ä½“ä¼˜åŒ–åŒ…æ‹¬ç§»é™¤äº†ä¸å¿…è¦çš„ç³»ç»Ÿç»„ä»¶ã€å¯ç”¨äº†å„ç§å®‰å…¨åŠ å›ºé€‰é¡¹ã€å®šæœŸæ›´æ–°åŸºç¡€é•œåƒï¼Œæ”¯æŒå¤šç§è¿è¡Œæ—¶ç¯å¢ƒåŒ…æ‹¬ Dockerã€containerdã€Podmanã€‚å¼€æºç¤¾åŒºå¯¹è¿™ç§åŠ å›ºé•œåƒçš„éœ€æ±‚æŒºå¤§ï¼Œç‰¹åˆ«æ˜¯é‡‘èå’ŒåŒ»ç–—è¡Œä¸šçš„ç”¨æˆ·ï¼Œç¼ºç‚¹æ˜¯é•œåƒä½“ç§¯æ¯”å®˜æ–¹ç‰ˆå¤§ä¸€äº›ï¼Œå¯åŠ¨æ—¶é—´ä¹Ÿç¨é•¿ã€‚{url}"""
        
    elif 'github' in url.lower() or source == 'github_trending':
        lang = metadata.get('language', '')
        stars = metadata.get('stars', 0)
        first_sentence = sentences[0][:200] if sentences else f"ä¸»è¦è§£å†³ {tagline} çš„é—®é¢˜ã€‚"
        star_info = f"ï¼Œç›®å‰ {stars} star" if stars > 1000 else ""
        content = f"""{product_name} æ˜¯ä¸€ä¸ªç”¨ {lang if lang else 'ä¸»æµè¯­è¨€'} å†™çš„å¼€æºé¡¹ç›®ï¼Œä¸»è¦è§£å†³ {tagline} çš„é—®é¢˜ã€‚{first_sentence} ä»£ç åœ¨ GitHub ä¸Šå¼€æº{star_info}ï¼ŒREADME æä¾›äº†å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼Œæœ‰åŸºç¡€çš„å¼€å‘è€…åº”è¯¥èƒ½æ¯”è¾ƒå¿«ä¸Šæ‰‹ã€‚{url}"""
        
    elif 'producthunt' in url.lower() or source == 'producthunt':
        score = metadata.get('score', 0)
        score_info = f"ï¼Œå·²ç»æ‹¿äº† {score} ä¸ª upvote" if score > 50 else ""
        first_sentence = sentences[0][:200] if sentences else f"æ˜¯ä¸€ä¸ª {tagline} çš„å·¥å…·ã€‚"
        content = f"""{product_name} ä»Šå¤©åˆšåœ¨ Product Hunt ä¸Šå‘å¸ƒ{score_info}ï¼Œå®ƒæ˜¯ä¸€ä¸ª {tagline} çš„å·¥å…·ã€‚{first_sentence} ä»é¡µé¢ä»‹ç»æ¥çœ‹ä¸»è¦é¢å‘éœ€è¦ç®€åŒ–å·¥ä½œæµç¨‹çš„ç”¨æˆ·ï¼Œæœ‰å…è´¹ tier å¯ä»¥è¯•ç”¨ï¼Œå»ºè®®æ‹¿è‡ªå·±çš„æ•°æ®æµ‹è¯•ä¸€ä¸‹æ•ˆæœã€‚{url}"""
        
    else:
        # ä¸¥æ ¼æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œç«‹å³æŠ¥é”™ï¼Œä¸ç”Ÿæˆé€šç”¨å†…å®¹
        if not sentences:
            raise RuntimeError(f"âŒ å†…å®¹ç”Ÿæˆå¤±è´¥ï¼š{product_name} æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ç”Ÿæˆç‹¬ç‰¹å†…å®¹ã€‚summaryä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆé«˜è´¨é‡ä»‹ç»ã€‚")
        
        first_sentence = sentences[0][:220]
        second_sentence = f" {sentences[1][:180]}" if len(sentences) > 1 else ""
        
        # å¿…é¡»åŸºäºçœŸå®ä¿¡æ¯ï¼Œä¸èƒ½æ˜¯é€šç”¨æè¿°ï¼Œä¸¥æ ¼æ¨¡å¼
        if not summary or len(summary) < 50:
            raise RuntimeError(f"âŒ å†…å®¹ç”Ÿæˆå¤±è´¥ï¼š{product_name} ä¿¡æ¯ä¸è¶³ã€‚summaryé•¿åº¦{len(summary) if summary else 0}å­—ç¬¦ï¼Œéœ€è¦è‡³å°‘50å­—ç¬¦æ‰èƒ½ç”Ÿæˆå†…å®¹ã€‚")
        
        # ç›´æ¥å¼•ç”¨çœŸå®æ•°æ®ï¼Œä¸åŠ ä»»ä½•æ¨¡æ¿å‰ç¼€
        content = f"{product_name} æ˜¯ä¸€ä¸ª {tagline} çš„é¡¹ç›®ã€‚{first_sentence}{second_sentence} {summary[:120]}... {url}"
    
    return content

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_unique_content(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå®Œå…¨ç‹¬ç‰¹å™è¿°ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡ï¼Œç¡®ä¿å¤šæ ·æ€§ï¼ˆä¼˜å…ˆä¸åŒæ¥æºï¼‰
    top_articles = select_best_articles(articles, top_n=5)  # å…ˆé€‰5æ¡
    
    # ç¡®ä¿æ¥æºå¤šæ ·æ€§
    source_count = {}
    diverse_articles = []
    for article in top_articles:
        src = article.source
        if source_count.get(src, 0) < 2:  # æ¯ä¸ªæ¥æºæœ€å¤š2æ¡
            diverse_articles.append(article)
            source_count[src] = source_count.get(src, 0) + 1
        if len(diverse_articles) >= 3:
            break
    
    top_articles = diverse_articles[:3]
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡ (å·²ä¼˜åŒ–æ¥æºå¤šæ ·æ€§):", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. [{article.source}] {article.title[:45]}...", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
