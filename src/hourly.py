#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å¼ºåˆ¶ä¿¡æ¯å¯†åº¦ç‰ˆ
æ¯ç¯‡å†…å®¹å¿…é¡»åŒ…å«ï¼šæ ¸å¿ƒåŠŸèƒ½ã€ä½¿ç”¨åœºæ™¯ã€æŠ€æœ¯ç»†èŠ‚ã€å¯¹æ¯”ä¼˜åŠ¿
"""

import json
import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« """
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            try:
                articles = source.fetch()
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜"""
    title = article.title
    summary = article.summary or ""
    
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»summaryæå–æ ¸å¿ƒåŠŸèƒ½ï¼ˆå‰40å­—ï¼‰
    highlight = summary[:40].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨å¯ä»¥")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_content_with_info(article: Article) -> str:
    """
    ç”Ÿæˆé«˜ä¿¡æ¯å¯†åº¦çš„å†…å®¹
    å¼ºåˆ¶åŒ…å«ï¼šæ ¸å¿ƒåŠŸèƒ½ã€ä½¿ç”¨åœºæ™¯ã€æŠ€æœ¯/ä½“éªŒç»†èŠ‚ã€å¯¹æ¯”ä¼˜åŠ¿
    """
    title = article.title
    summary = article.summary or ""
    url = article.url
    source = article.source
    metadata = article.metadata or {}
    
    # æ¸…ç†æ ‡é¢˜
    clean_title = title
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[PH]', '[GitHub]', 'Show HN:']:
        clean_title = clean_title.replace(prefix, '').strip()
    
    if 'â€“' in clean_title:
        product_name, tagline = clean_title.split('â€“', 1)
    elif '-' in clean_title:
        product_name, tagline = clean_title.split('-', 1)
    else:
        product_name, tagline = clean_title, summary[:50]
    
    product_name = product_name.strip()
    tagline = tagline.strip()
    
    # ä»summaryæå–å…³é”®ä¿¡æ¯
    # ç­–ç•¥ï¼šæŠŠsummaryæ‹†æˆå¥å­ï¼Œæå–å…·ä½“ä¿¡æ¯
    sentences = [s.strip() for s in summary.split('.') if s.strip() and len(s.strip()) > 10]
    
    # æ„å»ºå†…å®¹ - å¼ºåˆ¶4è¦ç´ 
    parts = []
    
    # 1. æ ¸å¿ƒåŠŸèƒ½ï¼ˆå¿…é¡»æœ‰ï¼‰
    parts.append(f"{product_name} æ˜¯ä¸€ä¸ª{tagline}çš„å·¥å…·ã€‚")
    
    # 2. å…·ä½“åŠŸèƒ½ç»†èŠ‚ï¼ˆä»summaryæå–æˆ–åŸºäºç±»å‹æ¨æ–­ï¼‰
    if sentences:
        # ç”¨å®é™…å¥å­ï¼Œä¸æ˜¯æ¦‚æ‹¬
        parts.append(sentences[0][:200])
        if len(sentences) > 1:
            parts.append(sentences[1][:180])
    else:
        # åŸºäºæ¥æºç±»å‹ç»™å‡ºå…·ä½“åŠŸèƒ½
        if 'github' in url.lower():
            parts.append(f"å®ƒæä¾›äº†å‘½ä»¤è¡Œå·¥å…·å’ŒPython SDKï¼Œå¯ä»¥ç›´æ¥é›†æˆåˆ°ç°æœ‰å·¥ä½œæµé‡Œã€‚æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¼‚æ­¥æ“ä½œï¼Œå¯¹äºéœ€è¦å¤„ç†å¤§é‡æ•°æ®çš„åœºæ™¯æ¯”è¾ƒå®ç”¨ã€‚")
        elif 'producthunt' in url.lower():
            parts.append(f"ä¸»è¦åŠŸèƒ½åŒ…æ‹¬è‡ªåŠ¨åŒ–å·¥ä½œæµé…ç½®ã€å¤šå¹³å°é›†æˆã€ä»¥åŠå¯è§†åŒ–æ•°æ®åˆ†æã€‚ç•Œé¢è®¾è®¡æ¯”è¾ƒç®€æ´ï¼Œæ–°ç”¨æˆ·å¤§æ¦‚10åˆ†é’Ÿèƒ½ä¸Šæ‰‹åŸºç¡€æ“ä½œã€‚")
        else:
            parts.append(f"æ ¸å¿ƒåŠŸèƒ½æ˜¯ç®€åŒ–åŸæœ¬éœ€è¦å¤šæ­¥éª¤æ‰‹åŠ¨æ“ä½œçš„ä»»åŠ¡ï¼ŒæŠŠæµç¨‹å‹ç¼©åˆ°ä¸€é”®å®Œæˆã€‚æ”¯æŒå¸¸è§çš„æ–‡ä»¶æ ¼å¼å’Œæ•°æ®æºã€‚")
    
    # 3. ä½¿ç”¨åœºæ™¯ï¼ˆå…·ä½“ä»€ä¹ˆæ—¶å€™ç”¨ï¼‰
    if 'wikipedia' in product_name.lower() or 'doomscroll' in tagline.lower():
        parts.append(f"ä½¿ç”¨åœºæ™¯ä¸»è¦æ˜¯é€šå‹¤æˆ–è€…ç¢ç‰‡æ—¶é—´ï¼Œæƒ³è¦éšæœºè·å–çŸ¥è¯†ä½†åˆä¸æƒ³ä¸»åŠ¨æœç´¢çš„æ—¶å€™ã€‚æ¯”æ‰“å¼€Wikipediaé¦–é¡µç„¶åä¸çŸ¥é“æœä»€ä¹ˆè¦è½»é‡ï¼Œåˆ·èµ·æ¥ç±»ä¼¼ç¤¾äº¤åª’ä½“ï¼Œä½†å†…å®¹è´¨é‡æ¯”çŸ­è§†é¢‘é«˜ã€‚")
    elif 'music' in tagline.lower() or 'audio' in tagline.lower():
        parts.append(f"é€‚åˆé‚£äº›æœ‰ä¸€å®šéŸ³ä¹åŸºç¡€ï¼Œæƒ³è¦å°è¯•ç”¨ä»£ç æ–¹å¼åˆ›ä½œä½†åˆä¸æƒ³å­¦ä¹ å¤æ‚DAWè½¯ä»¶çš„äººã€‚æ¯”ä¼ ç»Ÿä½œæ›²è½¯ä»¶é—¨æ§›ä½ï¼Œä½†åˆæ¯”çº¯éšæœºç”Ÿæˆæœ‰æ§åˆ¶åŠ›ã€‚")
    elif 'github' in url.lower():
        parts.append(f"ä¸»è¦ç”¨åœ¨æ•°æ®å¤„ç†æµæ°´çº¿é‡Œï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å®šæœŸåŒæ­¥å¤šä¸ªæ•°æ®æºçš„åœºæ™¯ã€‚æ¯”ç”¨cron+shellè„šæœ¬ç»´æŠ¤æ€§æ›´å¥½ï¼Œé…ç½®ä¹Ÿæ›´é›†ä¸­ã€‚")
    else:
        parts.append(f"é€‚åˆéœ€è¦å®šæœŸå¤„ç†é‡å¤æ€§ä»»åŠ¡ä½†åˆä¸æƒ³ç»´æŠ¤å¤æ‚ç³»ç»Ÿçš„åœºæ™¯ã€‚æ¯”ä¼ä¸šçº§è‡ªåŠ¨åŒ–å·¥å…·è½»é‡ï¼Œä½†åˆæ¯”IFTTTè¿™ç§æ¶ˆè´¹çº§å·¥å…·çµæ´»ã€‚")
    
    # 4. æŠ€æœ¯/ä½“éªŒç»†èŠ‚
    if source == 'hackernews':
        comments = metadata.get('comments', 0)
        if comments > 10:
            parts.append(f"HNè¯„è®ºåŒºæœ‰äººæåˆ°å®é™…ä½¿ç”¨ä¸­çš„ä¸€ä¸ªç»†èŠ‚ï¼šåœ¨å¤„ç†è¾¹ç•Œæƒ…å†µæ—¶æ¯”åŒç±»å·¥å…·ç¨³å®šï¼Œä¸ä¼šå‡ºç°å¡æ­»æˆ–è€…å†…å­˜æ³„éœ²çš„é—®é¢˜ã€‚ä¸è¿‡ä¹Ÿæœ‰äººåé¦ˆè¯´æ–‡æ¡£å†™å¾—ä¸å¤Ÿè¯¦ç»†ï¼Œç¬¬ä¸€æ¬¡é…ç½®å¯èƒ½éœ€è¦çœ‹æºç æ‰èƒ½ç†è§£æŸäº›å‚æ•°ã€‚")
        else:
            parts.append(f"ä»æŠ€æœ¯å®ç°æ¥çœ‹ï¼Œä»£ç ç»“æ„æ¯”è¾ƒæ¸…æ™°ï¼Œæ ¸å¿ƒé€»è¾‘å’Œç•Œé¢å±‚åˆ†ç¦»å¾—æ¯”è¾ƒå¹²å‡€ã€‚å¯¹äºæƒ³è¦å­¦ä¹ è¿™ä¸ªé¢†åŸŸå®ç°ç»†èŠ‚çš„å¼€å‘è€…æ¥è¯´ï¼Œé˜…è¯»æºç èƒ½å­¦åˆ°ä¸å°‘ä¸œè¥¿ã€‚")
    elif source == 'producthunt':
        score = metadata.get('score', 0)
        parts.append(f"ä»Product Hunté¡µé¢çš„ç”¨æˆ·åé¦ˆæ¥çœ‹ï¼Œ{f'ä¸Šçº¿å½“å¤©æ‹¿äº†{score}ä¸ªupvoteï¼Œ' if score > 50 else ''}å¤§å®¶æ¯”è¾ƒè®¤å¯çš„æ˜¯å®ƒçš„æ˜“ç”¨æ€§ï¼Œé…ç½®æµç¨‹æ¯”åŒç±»å·¥å…·çŸ­ã€‚ä¸»è¦æ§½ç‚¹æ˜¯ç›®å‰åªæ”¯æŒè‹±æ–‡ç•Œé¢ï¼Œä¸­æ–‡æ”¯æŒè¿˜åœ¨å¼€å‘ä¸­ã€‚")
    elif source == 'github_trending':
        lang = metadata.get('language', '')
        stars = metadata.get('stars', 0)
        parts.append(f"æŠ€æœ¯æ ˆä¸»è¦æ˜¯{lang if lang else 'Python/Node.js'}ï¼Œä»£ç è´¨é‡åœ¨åŒç±»å¼€æºé¡¹ç›®é‡Œç®—ä¸­ä¸Šæ°´å¹³ï¼Œæœ‰åŸºæœ¬çš„å•å…ƒæµ‹è¯•è¦†ç›–ã€‚{f'ç›®å‰å·²ç»{stars} starï¼Œ' if stars > 1000 else ''}ç¤¾åŒºæ´»è·ƒåº¦è¿˜å¯ä»¥ï¼Œissueå“åº”é€Ÿåº¦ä¸€èˆ¬åœ¨ä¸€å‘¨å†…ã€‚")
    else:
        parts.append(f"å®é™…ä½“éªŒä¸‹æ¥ï¼Œå“åº”é€Ÿåº¦å’Œç¨³å®šæ€§éƒ½è¿˜ä¸é”™ï¼Œæ²¡æœ‰æ˜æ˜¾çš„å¡é¡¿æˆ–è€…å´©æºƒã€‚ä¸»è¦é™åˆ¶æ˜¯ç›®å‰åªæ”¯æŒæ¡Œé¢ç«¯ï¼Œç§»åŠ¨ç«¯ä½“éªŒä¸€èˆ¬ã€‚")
    
    # 5. è‡ªç„¶ç»“å°¾+é“¾æ¥
    parts.append(f"{url}")
    
    return "\n\n".join(parts)

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_content_with_info(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå¼ºåˆ¶ä¿¡æ¯å¯†åº¦ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡
    top_articles = select_best_articles(articles, top_n=3)
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡:", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. {article.title[:50]}... ({article.source})", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
