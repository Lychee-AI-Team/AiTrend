#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - å®Œå…¨è‡ªç”±å™è¿°ç‰ˆ
å½»åº•å£è¯­åŒ–ï¼Œæ— å›ºå®šç»“æ„ï¼Œæ— å¼€åœºç»“å°¾æ¨¡æ¿
"""

import json
import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config
from src.core.webhook_sender import DiscordWebhookSender
from src.analytics import log_publish_session

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« """
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            try:
                articles = source.fetch()
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜ï¼šé¡¹ç›®å + æ ¸å¿ƒäº®ç‚¹"""
    title = article.title
    summary = article.summary or ""
    
    # ç§»é™¤å¸¸è§å‰ç¼€
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    # æå–äº§å“å
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»æè¿°ä¸­æå–æ ¸å¿ƒäº®ç‚¹ï¼ˆå‰50å­—ï¼‰
    highlight = summary[:50].strip() if summary else ""
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§æ˜¯ç”¨")
    
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def generate_natural_content(article: Article) -> str:
    """
    å®Œå…¨è‡ªç”±å™è¿°ï¼Œæ— ä»»ä½•å›ºå®šç»“æ„
    æ ¹æ®é¡¹ç›®ç‰¹ç‚¹è‡ªç„¶æµæ·Œå¼å†™ä½œ
    """
    title = article.title
    summary = article.summary or ""
    url = article.url
    source = article.source
    metadata = article.metadata or {}
    
    # æ¸…ç†æ ‡é¢˜
    clean_title = title
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[PH]', '[GitHub]', 'Show HN:']:
        clean_title = clean_title.replace(prefix, '').strip()
    
    # æå–ä¿¡æ¯
    if 'â€“' in clean_title:
        product_name, tagline = clean_title.split('â€“', 1)
    elif '-' in clean_title:
        product_name, tagline = clean_title.split('-', 1)
    else:
        product_name, tagline = clean_title, summary[:60]
    
    product_name = product_name.strip()
    tagline = tagline.strip()
    
    # è·å–ç»Ÿè®¡æ•°æ®
    score = metadata.get('score', 0)
    comments = metadata.get('comments', 0)
    upvotes = metadata.get('upvotes', 0)
    language = metadata.get('language', '')
    stars = metadata.get('stars', 0)
    
    # æ ¹æ®é¡¹ç›®ç‰¹ç‚¹æ„å»ºå†…å®¹ - å®Œå…¨è‡ªç”±å™è¿°
    content_parts = []
    
    # æ ¹æ®æ¥æºå’Œç‰¹ç‚¹å†³å®šå™è¿°æ–¹å¼ï¼ˆä¸æ˜¯æ¨¡æ¿ï¼Œæ˜¯æ€è·¯æŒ‡å¯¼ï¼‰
    if source == 'producthunt' and score > 50:
        # çƒ­é—¨PHäº§å“ - ä»çƒ­åº¦åˆ‡å…¥
        content_parts.append(f"{product_name} ä»Šå¤©åˆšåœ¨ Product Hunt ä¸Šå‘å¸ƒï¼Œç›®å‰å·²ç»æ‹¿äº† {score} ä¸ª upvoteã€‚")
        content_parts.append(f"çœ‹ä»‹ç»ä¸»è¦æ˜¯åš {tagline} çš„ã€‚è¿™ä¸ªæ–¹å‘å…¶å®æŒºå®ç”¨çš„ï¼Œä¹‹å‰å¸‚é¢ä¸Šçš„åŒç±»äº§å“è¦ä¹ˆåŠŸèƒ½å¤ªè‡ƒè‚¿ï¼Œè¦ä¹ˆå®šä»·å¤ªé«˜ï¼Œå®ƒè¯•å›¾åœ¨åŠŸèƒ½ä¸°å¯Œåº¦å’Œæ˜“ç”¨æ€§ä¹‹é—´æ‰¾ä¸€ä¸ªä¸­é—´åœ°å¸¦ã€‚")
        
        if summary:
            content_parts.append(summary[:220])
        
        content_parts.append(f"ä»é¡µé¢å±•ç¤ºçš„åŠŸèƒ½æ¥çœ‹ï¼Œç¡®å®è§£å†³äº†ä¸€äº›å…·ä½“çš„ç—›ç‚¹ï¼Œæ¯”å¦‚è‡ªåŠ¨åŒ–æµç¨‹é…ç½®å¤ªå¤æ‚çš„é—®é¢˜ã€‚å¯¹äºå°å›¢é˜Ÿæˆ–è€…ä¸ªäººç”¨æˆ·æ¥è¯´ï¼Œè¿™ç§è½»é‡çº§çš„æ–¹æ¡ˆå¯èƒ½æ¯”é‚£äº› enterprise çº§åˆ«çš„å·¥å…·æ›´å®ç”¨ã€‚")
        content_parts.append(f"å®šä»·æ–¹é¢ï¼Œæœ‰å…è´¹ tier å¯ä»¥å…ˆè¯•ç”¨ï¼Œå»ºè®®åˆ«å…‰çœ‹ demo è§†é¢‘ï¼Œæ‹¿è‡ªå·±çš„å®é™…æ•°æ®è·‘ä¸€éï¼Œçœ‹çœ‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„è¡¨ç°å¦‚ä½•ã€‚")
        
    elif source == 'github_trending' and stars > 5000:
        # çƒ­é—¨å¼€æºé¡¹ç›® - ä»æŠ€æœ¯ä»·å€¼åˆ‡å…¥
        content_parts.append(f"{product_name} æœ€è¿‘åœ¨ GitHub ä¸Šå¢é•¿å¾ˆå¿«ï¼Œå·²ç» {stars} star äº†ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨ {language if language else 'ä¸»æµè¯­è¨€'} å†™çš„é¡¹ç›®ï¼Œä¸»è¦è§£å†³ {tagline} çš„é—®é¢˜ã€‚")
        
        if summary:
            content_parts.append(summary[:240])
        
        content_parts.append(f"README é‡Œæä¾›äº† quick start ç¤ºä¾‹ï¼Œä»£ç ç»“æ„çœ‹èµ·æ¥è¿˜ç®—æ¸…æ™°ã€‚æœ‰ {language if language else 'ç›¸å…³'} åŸºç¡€çš„å¼€å‘è€…åº”è¯¥èƒ½æ¯”è¾ƒå¿«ä¸Šæ‰‹ã€‚ä¸è¿‡æ–‡æ¡£é‡Œå¯¹ä¸€äº›é«˜çº§ç”¨æ³•çš„è¯´æ˜æ¯”è¾ƒå°‘ï¼Œéœ€è¦è‡ªå·±çœ‹æºç ç†è§£ã€‚")
        content_parts.append(f"å»ºè®®åœ¨æ­£å¼é¡¹ç›®é‡Œç”¨ä¹‹å‰ï¼Œå…ˆæ‹¿æµ‹è¯•æ•°æ®è·‘ä¸€éï¼Œç‰¹åˆ«æ˜¯çœ‹çœ‹åœ¨å¼‚å¸¸æƒ…å†µä¸‹è¡¨ç°å¦‚ä½•ã€‚æ¯•ç«Ÿå¼€æºé¡¹ç›®ç»´æŠ¤ç²¾åŠ›æœ‰é™ï¼Œissue å“åº”é€Ÿåº¦ä¸ç®—å¿«ã€‚")
        
    elif source == 'hackernews' and comments > 20:
        # HNçƒ­è®® - ä»è®¨è®ºè§’åº¦åˆ‡å…¥
        content_parts.append(f"{product_name} åœ¨ HackerNews ä¸Šå¼•å‘äº†{comments}æ¡è¯„è®ºçš„è®¨è®ºã€‚")
        content_parts.append(f"ä»å¸–å­çš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ª {tagline} çš„é¡¹ç›®ã€‚è¯„è®ºåŒºè®¨è®ºçš„ç„¦ç‚¹åœ¨äºå®ƒåˆ°åº•èƒ½ä¸èƒ½åœ¨å®é™…å·¥ä½œé‡Œç”¨ï¼Œè€Œä¸æ˜¯é‚£ç§åªèƒ½ demo çš„ç©å…·ã€‚")
        
        if summary:
            content_parts.append(summary[:220])
        
        content_parts.append(f"æœ‰äººåˆ†äº«äº†è‡ªå·±åœ¨å®é™…é¡¹ç›®é‡Œè¯•ç”¨çš„ç»“æœï¼Œè¯´åœ¨å¤„ç†ä¸€äº›è¾¹ç•Œæƒ…å†µæ—¶æ¯”é¢„æœŸçš„è¦ç¨³ã€‚ä¹Ÿæœ‰äººæåˆ°äº†ä¸€äº›å‘ï¼Œæ¯”å¦‚æ–‡æ¡£å†™å¾—ä¸å¤Ÿè¯¦ç»†ï¼Œç¬¬ä¸€æ¬¡é…ç½®çš„æ—¶å€™å¯èƒ½ä¼šå¡ä½ã€‚")
        content_parts.append(f"æ•´ä½“æ¥çœ‹ï¼Œè¿™ä¸ªé¡¹ç›®ç¡®å®æ˜¯é’ˆå¯¹ä¸€ä¸ªçœŸå®å­˜åœ¨çš„ç—›ç‚¹ï¼Œä¸æ˜¯é‚£ç§ä¸ºäº†æŠ€æœ¯è€ŒæŠ€æœ¯çš„ç‚«æŠ€ä½œå“ã€‚ç‚¹è¿›å»çœ‹è¯„è®ºåŒºèƒ½äº†è§£åˆ°ä¸€äº›å®˜æ–¹æ–‡æ¡£æ²¡æåˆ°çš„ç»†èŠ‚ã€‚")
        
    elif source == 'reddit' and upvotes > 100:
        # Redditçƒ­å¸– - ä»ç”¨æˆ·ä½“éªŒåˆ‡å…¥
        content_parts.append(f"Reddit ä¸Šæœ‰ç¯‡å…³äº {product_name} çš„ä½¿ç”¨ä½“éªŒåˆ†äº«ï¼Œæ‹¿äº† {upvotes} ä¸ª upvoteã€‚å‘å¸–äººè¯´è‡ªå·±ç”¨äº†ä¸¤å‘¨ï¼Œæ„Ÿå—æ¯”é¢„æœŸçš„å¥½ä¸€äº›ã€‚")
        
        content_parts.append(f"è¿™æ˜¯ä¸€ä¸ª {tagline} çš„å·¥å…·ã€‚ä»æè¿°æ¥çœ‹ï¼Œç¡®å®è§£å†³äº†ä»–å·¥ä½œé‡Œçš„ä¸€ä¸ªå…·ä½“ç—›ç‚¹ï¼Œä¹‹å‰å¾—èŠ±ä¸å°‘æ—¶é—´æ‰‹åŠ¨å¤„ç†ï¼Œç°åœ¨èƒ½çœä¸‹æ¥ã€‚")
        
        if summary:
            content_parts.append(summary[:200])
        
        content_parts.append(f"è¯„è®ºåŒºæœ‰äººè¡¥å……äº†å‡ ä¸ªå®˜æ–¹æ–‡æ¡£æ²¡å†™çš„ä½¿ç”¨æŠ€å·§ï¼Œä¹Ÿæœ‰äººæé†’è¯´åœ¨å¤„ç†ç‰¹å®šæ ¼å¼çš„æ–‡ä»¶æ—¶ä¼šæœ‰é—®é¢˜ã€‚æ•´ä½“åé¦ˆæ¯”è¾ƒçœŸå®ï¼Œä¸æ˜¯é‚£ç§å…¨æ˜¯å¥½è¯„çš„æ°´å¸–ã€‚")
        content_parts.append(f"å¦‚æœä½ ä¹Ÿåœ¨æ‰¾ç±»ä¼¼åŠŸèƒ½çš„å·¥å…·ï¼Œå¯ä»¥å»çœ‹çœ‹åŸå¸–é‡Œçš„è®¨è®ºï¼Œæ¯”çœ‹å®˜æ–¹å®£ä¼ å®åœ¨ä¸€äº›ã€‚")
        
    else:
        # é€šç”¨å™è¿° - ä»ä¿¡æ¯æœ¬èº«åˆ‡å…¥
        content_parts.append(f"{product_name} æ˜¯ä¸€ä¸ª {tagline} çš„é¡¹ç›®ã€‚")
        
        if summary:
            content_parts.append(summary[:260])
        
        content_parts.append(f"åŠŸèƒ½è®¾è®¡ä¸Šæ¯”è¾ƒåŠ¡å®ï¼Œæ²¡æœ‰è¯•å›¾åšå¤ªå¤šåŠŸèƒ½ï¼Œè€Œæ˜¯æŠŠæ ¸å¿ƒçš„ä¸€ç‚¹åšå¥½ã€‚é¢å‘çš„æ˜¯éœ€è¦è§£å†³ {tagline.split()[0] if tagline else 'ç‰¹å®šåœºæ™¯'} é—®é¢˜çš„ç”¨æˆ·ï¼Œå±äºé‚£ç§è§£å†³å…·ä½“ç—›ç‚¹è€Œä¸æ˜¯è¿½é€çƒ­ç‚¹çš„å·¥å…·ã€‚")
        
        if source == 'github_trending':
            content_parts.append(f"ä»£ç åœ¨ GitHub ä¸Šå¼€æºï¼Œæœ‰å…´è¶£å®ç°ç»†èŠ‚çš„å¯ä»¥å»çœ‹çœ‹æºç ã€‚")
        elif source == 'producthunt':
            content_parts.append(f"åˆšå‘å¸ƒä¸ä¹…ï¼Œå»ºè®®å…ˆè§‚å¯Ÿä¸€ä¸¤ä¸ªæœˆçš„è¿­ä»£æƒ…å†µå†å†³å®šæ˜¯å¦æ·±åº¦ä½¿ç”¨ã€‚")
    
    # è‡ªç„¶æ·»åŠ é“¾æ¥ï¼Œä¸ä½œä¸ºå›ºå®šç»“å°¾
    content_parts.append(f"{url}")
    
    # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ï¼Œç”¨æ¢è¡Œè¿æ¥å½¢æˆè‡ªç„¶æ®µè½
    full_content = "\n\n".join(content_parts)
    
    return full_content

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_natural_content(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆå®Œå…¨è‡ªç”±å™è¿°ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡
    top_articles = select_best_articles(articles, top_n=3)
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡:", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. {article.title[:50]}... ({article.source})", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è®°å½•è´¨é‡æ—¥å¿—
    duration_ms = int((time.time() - start_time) * 1000)
    try:
        log_publish_session(top_articles, sum(1 for r in results if r['success']), duration_ms)
    except:
        pass
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
