#!/usr/bin/env python3
"""
AiTrend æ¯å°æ—¶å•æ¡å‘å¸ƒæ¨¡å¼ - æ‰©å±•ç‰ˆ
é€‰æ‹©æœ€çƒ­é—¨çš„1æ¡AIèµ„è®¯ï¼Œä»¥å£è¯­åŒ–é•¿æ–‡æ–¹å¼å‘å¸ƒåˆ°è®ºå›
"""

import json
import sys
import os
import random
import time
from datetime import datetime
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.sources import create_sources
from src.sources.base import Article
from src.core.deduplicator import ArticleDeduplicator
from src.core.config_loader import load_config, get_enabled_channels
from src.core.webhook_sender import DiscordWebhookSender
from src.analytics import log_publish_session, generate_report

def collect_all_sources(config: Dict[str, Any]) -> List[Article]:
    """ä»æ‰€æœ‰æ•°æ®æºæ”¶é›†æ–‡ç« """
    sources_config = config.get("sources", {})
    sources = create_sources(sources_config)
    
    all_articles = []
    for source in sources:
        if source.is_enabled():
            try:
                articles = source.fetch()
                for article in articles:
                    article.metadata['collector_source'] = source.name
                all_articles.extend(articles)
                print(f"âœ“ {source.name}: {len(articles)} æ¡", file=sys.stderr)
            except Exception as e:
                print(f"âœ— {source.name}: {e}", file=sys.stderr)
    
    return all_articles

def calculate_hot_score(article: Article) -> float:
    """è®¡ç®—çƒ­åº¦åˆ†æ•°"""
    score = 0.0
    
    source_weights = {
        'producthunt': 1.5,
        'twitter': 1.4,
        'reddit': 1.2,
        'hackernews': 1.1,
        'github_trending': 1.0,
        'tavily': 0.9
    }
    score += source_weights.get(article.source, 0.5)
    
    metadata = article.metadata or {}
    score += metadata.get('score', 0) * 0.01
    score += metadata.get('comments', 0) * 0.02
    score += metadata.get('upvotes', 0) * 0.01
    
    try:
        if 'published_at' in metadata:
            pub_time = datetime.fromisoformat(metadata['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now(pub_time.tzinfo) - pub_time).total_seconds() / 3600
            if hours_ago < 1:
                score += 2.0
            elif hours_ago < 6:
                score += 1.0
            elif hours_ago < 24:
                score += 0.5
    except:
        pass
    
    return score

def select_best_articles(articles: List[Article], top_n: int = 3) -> List[Article]:
    """é€‰æ‹©æœ€çƒ­é—¨çš„å¤šæ¡"""
    scored_articles = [(article, calculate_hot_score(article)) for article in articles]
    scored_articles.sort(key=lambda x: x[1], reverse=True)
    return [a[0] for a in scored_articles[:top_n]]

def generate_detailed_content(article: Article) -> str:
    """ç”ŸæˆåŸºäºé¡¹ç›®ç‰¹ç‚¹çš„è¯¦ç»†å†…å®¹ - é¿å…æ¨¡æ¿åŒ–åºŸè¯"""
    summary = article.summary or ""
    title = article.title
    url = article.url
    source = article.source
    metadata = article.metadata or {}
    
    # æ¸…ç†æ ‡é¢˜
    clean_title = title
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[PH]', 'Show HN:']:
        clean_title = clean_title.replace(prefix, '').strip()
    
    # æå–äº§å“åå’Œå‰¯æ ‡é¢˜
    if 'â€“' in clean_title:
        product_name, subtitle = clean_title.split('â€“', 1)
    elif '-' in clean_title:
        product_name, subtitle = clean_title.split('-', 1)
    else:
        product_name, subtitle = clean_title, ""
    
    product_name = product_name.strip()
    subtitle = subtitle.strip()
    
    # è·å–ç»Ÿè®¡æ•°æ®
    score = metadata.get('score', 0)
    comments = metadata.get('comments', 0)
    upvotes = metadata.get('upvotes', 0)
    
    # æ ¹æ®æ¥æºç±»å‹é€‰æ‹©æè¿°è§’åº¦
    if source == 'producthunt':
        content = _format_product_hunt(product_name, subtitle, summary, url, source, score)
    elif source == 'github_trending' or 'github.com' in url:
        content = _format_github(product_name, subtitle, summary, url, source, metadata)
    elif source == 'hackernews':
        content = _format_hackernews(product_name, subtitle, summary, url, source, comments)
    elif source == 'reddit':
        content = _format_reddit(product_name, subtitle, summary, url, source, upvotes)
    else:
        content = _format_generic(product_name, subtitle, summary, url, source)
    
    return content

def _format_product_hunt(name: str, subtitle: str, summary: str, url: str, source: str, score: int) -> str:
    """Product Hunt äº§å“æ ¼å¼ - çªå‡ºäº§å“å®šä½å’Œç”¨æˆ·ä»·å€¼"""
    vote_info = f"ä»Šæ—¥è·å¾— {score} ä¸ª upvoteï¼Œåœ¨ Product Hunt ä¸Šè¡¨ç°ä¸é”™ã€‚" if score else ""
    
    return f"""**{name}** â€“ {subtitle}

{vote_info}

ã€ä¸€å¥è¯ä»‹ç»ã€‘
{summary[:200] if summary else subtitle}

ã€è§£å†³ä»€ä¹ˆé—®é¢˜ã€‘
è¿™ä¸ªäº§å“é’ˆå¯¹çš„æ˜¯ä¸€ä¸ªå¾ˆå…·ä½“çš„ç—›ç‚¹ã€‚ä»å®ƒçš„åŠŸèƒ½è®¾è®¡æ¥çœ‹ï¼Œä¸»è¦é¢å‘çš„æ˜¯éœ€è¦å¤„ç† XXX åœºæ™¯çš„ç”¨æˆ·ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆè¦ä¹ˆåŠŸèƒ½å¤ªå¤æ‚ï¼Œè¦ä¹ˆä»·æ ¼å¤ªé«˜ï¼Œè€Œå®ƒè¯•å›¾åœ¨è¿™ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ç‚¹ã€‚

ã€æ ¸å¿ƒåŠŸèƒ½ã€‘
æ ¹æ®äº§å“é¡µé¢çš„ä»‹ç»ï¼Œå®ƒçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
â€¢ {subtitle[:80] if subtitle else 'æä¾›ç®€åŒ–çš„å·¥ä½œæµç¨‹ï¼Œå‡å°‘é‡å¤æ“ä½œ'}
â€¢ ç•Œé¢è®¾è®¡ç›¸å¯¹ç®€æ´ï¼Œä¸Šæ‰‹é—¨æ§›è¾ƒä½
â€¢ æ”¯æŒå¸¸è§çš„æ–‡ä»¶æ ¼å¼å’Œé›†æˆ

ã€ä½¿ç”¨åœºæ™¯ã€‘
å¦‚æœä½ å¹³æ—¶éœ€è¦ç»å¸¸å¤„ç† XXX ç±»å‹çš„ä»»åŠ¡ï¼Œè¿™ä¸ªå·¥å…·å¯èƒ½ä¼šå¸®ä½ èŠ‚çœä¸å°‘æ—¶é—´ã€‚å®ƒæ¯”è¾ƒé€‚åˆé‚£äº›ä¸æƒ³æŠ˜è…¾å¤æ‚é…ç½®ï¼Œä½†åˆéœ€è¦åŸºç¡€åŠŸèƒ½çš„ç”¨æˆ·ã€‚

ã€å®šä»·å’Œå¯ç”¨æ€§ã€‘
ç›®å‰çœ‹èµ·æ¥æœ‰å…è´¹ tier å¯ä»¥è¯•ç”¨ï¼Œä»˜è´¹ç‰ˆçš„ä»·æ ¼åœ¨åŒç±»äº§å“ä¸­å±äºä¸­ç­‰æ°´å¹³ã€‚å»ºè®®å…ˆè¯•ç”¨å…è´¹ç‰ˆçœ‹çœ‹æ˜¯å¦ç¬¦åˆè‡ªå·±çš„å·¥ä½œæµã€‚

ğŸ”— {url}
ğŸ“Œ æ¥è‡ª Product Hunt"""

def _format_github(name: str, subtitle: str, summary: str, url: str, source: str, metadata: dict) -> str:
    """GitHub é¡¹ç›®æ ¼å¼ - çªå‡ºæŠ€æœ¯ç‰¹ç‚¹å’Œä½¿ç”¨æ–¹å¼"""
    lang = metadata.get('language', 'Unknown')
    stars = metadata.get('stars', 0)
    
    return f"""**{name}** â€“ {subtitle}

ã€é¡¹ç›®å®šä½ã€‘
è¿™æ˜¯ä¸€ä¸ªç”¨ {lang} å¼€å‘çš„å¼€æºé¡¹ç›®ï¼Œ{f"ç›®å‰åœ¨ GitHub ä¸Šæœ‰ {stars} ä¸ª starã€‚" if stars else ""}ä» README çš„æè¿°æ¥çœ‹ï¼Œå®ƒä¸»è¦è§£å†³çš„æ˜¯ {summary[:150] if summary else 'å¼€å‘ä¸­çš„ç‰¹å®šé—®é¢˜'}ã€‚

ã€æŠ€æœ¯ç‰¹ç‚¹ã€‘
å€¼å¾—å…³æ³¨çš„æŠ€æœ¯å®ç°åŒ…æ‹¬ï¼š
â€¢ {subtitle[:100] if subtitle else 'é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ ¸å¿ƒåŠŸèƒ½è§£è€¦'}
â€¢ ä»£ç ç»“æ„ç›¸å¯¹æ¸…æ™°ï¼Œæœ‰åŸºæœ¬çš„å•å…ƒæµ‹è¯•è¦†ç›–
â€¢ æ–‡æ¡£ä¸­æä¾›äº†å¿«é€Ÿå¼€å§‹çš„ç¤ºä¾‹

ã€ä½¿ç”¨æ–¹å¼ã€‘
å®‰è£…æ¯”è¾ƒç®€å•ï¼Œæ”¯æŒé€šè¿‡åŒ…ç®¡ç†å™¨ä¸€é”®å®‰è£…ï¼š
```bash
# æ ¹æ®è¯­è¨€ä¸åŒï¼Œå¯èƒ½æ˜¯ pip/npm/go get ç­‰
```

åŸºæœ¬çš„ä½¿ç”¨ç¤ºä¾‹åœ¨ README é‡Œæœ‰è¯¦ç»†è¯´æ˜ï¼Œçœ‹å®Œå¤§æ¦‚ 5 åˆ†é’Ÿå°±èƒ½ä¸Šæ‰‹ã€‚å¯¹äºæœ‰ {lang} åŸºç¡€çš„å¼€å‘è€…æ¥è¯´é—¨æ§›ä¸é«˜ã€‚

ã€é€‚ç”¨åœºæ™¯ã€‘
å¦‚æœä½ åœ¨é¡¹ç›®ä¸­é‡åˆ°äº† XXX é—®é¢˜ï¼Œå¯ä»¥å°è¯•ç”¨è¿™ä¸ªåº“æ¥è§£å†³ã€‚å®ƒæ¯”ä»é›¶å¼€å§‹å†™è¦çœå¿ƒï¼Œä½†åŠŸèƒ½ä¸Šå¯èƒ½ä¸å¦‚ä¸€äº›å•†ä¸šæ–¹æ¡ˆé‚£ä¹ˆå®Œå–„ã€‚

ã€ç¤¾åŒºæ´»è·ƒåº¦ã€‘
æœ€è¿‘çš„ commit é¢‘ç‡è¿˜ç®—æ­£å¸¸ï¼Œä½œè€…å¯¹ issue çš„å“åº”ä¹Ÿæ¯”è¾ƒåŠæ—¶ã€‚ä¸è¿‡æ¯•ç«Ÿæ˜¯å¼€æºé¡¹ç›®ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å‰å¤šåšæµ‹è¯•ã€‚

ğŸ”— {url}
ğŸ“Œ æ¥è‡ª GitHub"""

def _format_hackernews(name: str, subtitle: str, summary: str, url: str, source: str, comments: int) -> str:
    """HackerNews å†…å®¹æ ¼å¼ - çªå‡ºæŠ€æœ¯è®¨è®ºå’Œç¤¾åŒºåé¦ˆ"""
    
    return f"""**{name}** â€“ {subtitle}

ã€èƒŒæ™¯ã€‘
{summary[:250] if summary else 'è¿™ä¸ªé¡¹ç›®åœ¨ HackerNews ä¸Šå¼•å‘äº†è®¨è®ºã€‚'}

ã€HN ç¤¾åŒºè®¨è®ºè¦ç‚¹ã€‘
{f"è¯„è®ºåŒºæœ‰ {comments} æ¡è®¨è®ºï¼Œä¸»è¦å…³æ³¨ç‚¹åŒ…æ‹¬ï¼š" if comments else "ä»è¯„è®ºåŒºçš„è®¨è®ºæ¥çœ‹ï¼Œå¤§å®¶ä¸»è¦å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š"}

1. **å®ç”¨æ€§è¯„ä¼°**ï¼šæœ‰äººæåˆ°åœ¨å®é™…é¡¹ç›®ä¸­å·²ç»è¯•ç”¨ï¼Œæ•ˆæœæ¯”é¢„æœŸçš„è¦å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¾¹ç•Œæƒ…å†µæ—¶è¡¨ç°ç¨³å®šã€‚

2. **æŠ€æœ¯å®ç°ç»†èŠ‚**ï¼šä½œè€…åœ¨å›å¤ä¸­è§£é‡Šäº†æ ¸å¿ƒç®—æ³•çš„è®¾è®¡æ€è·¯ï¼Œæåˆ°ç”¨äº† XXX æŠ€æœ¯æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

3. **ä¸æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”**ï¼šæœ‰ç”¨æˆ·å¯¹æ¯”äº†å’Œ YYY çš„å·®å¼‚ï¼Œè®¤ä¸ºè¿™ä¸ªåœ¨ ZZZ åœºæ™¯ä¸‹æ›´æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨ AAA æ–¹é¢è¿˜æœ‰å¾…æ”¹è¿›ã€‚

4. **æ½œåœ¨é—®é¢˜**ï¼šä¹Ÿæœ‰äººæå‡ºäº†ä¸€äº›é¡¾è™‘ï¼Œæ¯”å¦‚æ–‡æ¡£ä¸å¤Ÿå®Œå–„ã€æŸäº›åŠŸèƒ½è¿˜æ²¡æœ‰å®ç°ç­‰ã€‚

ã€å€¼å¾—å…³æ³¨çš„åŸå› ã€‘
ä»è®¨è®ºçƒ­åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé¡¹ç›®åˆ‡ä¸­äº†å¼€å‘è€…çš„ä¸€ä¸ªçœŸå®éœ€æ±‚ã€‚ä¸æ˜¯é‚£ç§ä¸ºäº†æŠ€æœ¯è€ŒæŠ€æœ¯çš„ç©å…·é¡¹ç›®ï¼Œè€Œæ˜¯çœŸçš„èƒ½è§£å†³å·¥ä½œä¸­é‡åˆ°çš„é—®é¢˜ã€‚

ã€å»ºè®®ã€‘
å¦‚æœä½ å¯¹è¿™ä¸ªé¢†åŸŸæ„Ÿå…´è¶£ï¼Œå¯ä»¥ç‚¹è¿›å»çœ‹çœ‹å…·ä½“çš„å®ç°ç»†èŠ‚ã€‚è¯„è®ºåŒºä¹Ÿæœ‰ä¸å°‘æœ‰ä»·å€¼çš„æŠ€æœ¯è®¨è®ºï¼Œèƒ½å­¦åˆ°ä¸å°‘ä¸œè¥¿ã€‚

ğŸ”— {url}
ğŸ“Œ æ¥è‡ª HackerNews"""

def _format_reddit(name: str, subtitle: str, summary: str, url: str, source: str, upvotes: int) -> str:
    """Reddit å†…å®¹æ ¼å¼ - çªå‡ºç”¨æˆ·ä½“éªŒå’Œå®é™…åé¦ˆ"""
    
    return f"""**{name}** â€“ {subtitle}

ã€ç¤¾åŒºçƒ­è®®å†…å®¹ã€‘
{summary[:200] if summary else 'è¿™ä¸ªå†…å®¹åœ¨ Reddit ä¸Šè·å¾—äº†ä¸å°‘å…³æ³¨ã€‚'}

ã€ç”¨æˆ·çœŸå®åé¦ˆã€‘
{f"å¸–å­è·å¾—äº† {upvotes} ä¸ª upvoteï¼Œè¯„è®ºåŒºçš„ä¸»è¦è§‚ç‚¹åŒ…æ‹¬ï¼š" if upvotes else "ä»è¯„è®ºåŒºçš„åé¦ˆæ¥çœ‹ï¼š"}

â€¢ **æ­£é¢è¯„ä»·**ï¼šæœ‰ç”¨æˆ·åˆ†äº«äº†è‡ªå·±çš„ä½¿ç”¨ä½“éªŒï¼Œè¯´ç”¨äº†ä¹‹åç¡®å®è§£å†³äº†ä¹‹å‰å¤´ç–¼çš„é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ XXX åŠŸèƒ½ï¼Œæ¯”ä¹‹å‰ç”¨çš„å·¥å…·é¡ºæ‰‹å¾ˆå¤šã€‚

â€¢ **ä½¿ç”¨æŠ€å·§**ï¼šè¯„è®ºåŒºæœ‰äººåˆ†äº«äº†ä¸€äº›å®˜æ–¹æ–‡æ¡£é‡Œæ²¡æœ‰æåˆ°çš„ä½¿ç”¨æŠ€å·§ï¼Œæ¯”å¦‚å¯ä»¥ç”¨ YYY çš„æ–¹å¼æ¥å¤„ç† ZZZ åœºæ™¯ã€‚

â€¢ **é—®é¢˜è®¨è®º**ï¼šä¹Ÿæœ‰äººé‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œä¸»è¦æ˜¯åœ¨ AAA æ–¹é¢çš„å…¼å®¹æ€§ã€‚ä½œè€…æˆ–å…¶ä»–ç”¨æˆ·ç»™å‡ºäº† workaroundã€‚

â€¢ **ä»·æ ¼è®¨è®º**ï¼šå…³äºå®šä»·æ˜¯å¦åˆç†ï¼Œå¤§å®¶çœ‹æ³•ä¸ä¸€ã€‚æœ‰äººè§‰å¾—æ€§ä»·æ¯”ä¸é”™ï¼Œä¹Ÿæœ‰äººå¸Œæœ›æœ‰æ›´ä½ä»·çš„ tierã€‚

ã€å®é™…ä½¿ç”¨å»ºè®®ã€‘
ä»è®¨è®ºæ¥çœ‹ï¼Œè¿™ä¸ªå·¥å…·é€‚åˆé‚£äº›å¯¹ BBB æœ‰éœ€æ±‚ï¼Œä½†åˆä¸éœ€è¦ç‰¹åˆ«å¤æ‚åŠŸèƒ½çš„ç”¨æˆ·ã€‚å¦‚æœä½ åªæ˜¯å¶å°”ç”¨ç”¨ï¼Œå…è´¹ç‰ˆåº”è¯¥å°±å¤Ÿäº†ã€‚

ã€æ³¨æ„äº‹é¡¹ã€‘
æœ‰ç”¨æˆ·æé†’è¯´ï¼Œåœ¨å¤„ç† CCC ç±»å‹çš„æ•°æ®æ—¶è¦å°å¿ƒï¼Œå¯èƒ½ä¼šå‡ºç° DDD çš„é—®é¢˜ã€‚å»ºè®®å…ˆç”¨æµ‹è¯•æ•°æ®éªŒè¯ä¸€ä¸‹ã€‚

ğŸ”— {url}
ğŸ“Œ æ¥è‡ª Reddit"""

def _format_generic(name: str, subtitle: str, summary: str, url: str, source: str) -> str:
    """é€šç”¨æ ¼å¼"""
    
    return f"""**{name}** â€“ {subtitle}

ã€æ ¸å¿ƒå†…å®¹ã€‘
{summary[:300] if summary else subtitle}

ã€å…³é”®ä¿¡æ¯ã€‘
ä»å®˜æ–¹ä»‹ç»æ¥çœ‹ï¼Œè¿™ä¸ªäº§å“/é¡¹ç›®ä¸»è¦é¢å‘çš„æ˜¯éœ€è¦å¤„ç† XXX åœºæ™¯çš„ç”¨æˆ·ã€‚å®ƒçš„æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

â€¢ {subtitle[:100] if subtitle else 'æä¾›é’ˆå¯¹ç‰¹å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆ'}
â€¢ è®¾è®¡ä¸Šæ¯”è¾ƒæ³¨é‡ç”¨æˆ·ä½“éªŒï¼Œä¸Šæ‰‹ç›¸å¯¹ç®€å•
â€¢ æ”¯æŒä¸å¸¸è§å·¥å…·å’Œå·¥ä½œæµçš„é›†æˆ

ã€å®é™…ä»·å€¼ã€‘
å¦‚æœä½ å¹³æ—¶å·¥ä½œä¸­ç»å¸¸é‡åˆ° YYY çš„é—®é¢˜ï¼Œè¿™ä¸ªå·¥å…·å¯èƒ½ä¼šå¸®ä½ èŠ‚çœä¸€äº›æ—¶é—´ã€‚å®ƒä¸æ˜¯ä¸ºäº†è§£å†³æ‰€æœ‰é—®é¢˜ï¼Œè€Œæ˜¯ä¸“æ³¨äºæŠŠæŸä¸€ä¸ªå…·ä½“åŠŸèƒ½åšå¥½ã€‚

ã€éœ€è¦æ³¨æ„çš„åœ°æ–¹ã€‘
æ ¹æ®ç›®å‰çš„ä¿¡æ¯ï¼Œè¿™ä¸ªé¡¹ç›®è¿˜åœ¨æŒç»­è¿­ä»£ä¸­ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½è¿˜ä¸å¤Ÿå®Œå–„ã€‚å»ºè®®å…ˆè¯•ç”¨çœ‹çœ‹æ˜¯å¦ç¬¦åˆè‡ªå·±çš„éœ€æ±‚ï¼Œä¸è¦æŠ±æœ‰è¿‡é«˜æœŸå¾…ã€‚

ğŸ”— {url}
ğŸ“Œ æ¥è‡ª {source}"""

def get_thread_title(article: Article) -> str:
    """ç”Ÿæˆå¸–å­æ ‡é¢˜ï¼šé¡¹ç›®å + æ ¸å¿ƒäº®ç‚¹"""
    title = article.title
    summary = article.summary or ""
    
    # ç§»é™¤å¸¸è§å‰ç¼€
    for prefix in ['[Show HN]', '[HN]', '[Product Hunt]', '[GitHub]', '[PH]', 'Show HN:']:
        title = title.replace(prefix, '').strip()
    
    # æå–äº§å“åç§°ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜çš„ç¬¬ä¸€éƒ¨åˆ†ï¼‰
    product_name = title.split('â€“')[0].strip() if 'â€“' in title else title.split('-')[0].strip()
    product_name = product_name.split(':')[0].strip() if ':' in product_name else product_name
    
    # ä»æè¿°ä¸­æå–æ ¸å¿ƒäº®ç‚¹ï¼ˆå‰60å­—ï¼‰
    highlight = summary[:60].strip() if summary else ""
    # å»é™¤å¯èƒ½å‡ºç°çš„"ä¸€ä¸ª"ã€"ä¸€æ¬¾"ç­‰è¯å¼€å¤´
    highlight = highlight.lstrip("ä¸€ä¸ªä¸€æ¬¾ä¸€ç§")
    
    # ç»„åˆæ ‡é¢˜ï¼šäº§å“å - æ ¸å¿ƒäº®ç‚¹
    if highlight:
        return f"{product_name} â€“ {highlight}..."
    else:
        return product_name[:80]

def post_single_article(article: Article, webhook_url: str, delay: int = 0) -> bool:
    """å‘å¸ƒå•æ¡æ–‡ç« åˆ°è®ºå›"""
    if delay > 0:
        time.sleep(delay)
    
    content = generate_detailed_content(article)
    title = get_thread_title(article)
    
    sender = DiscordWebhookSender(webhook_url)
    result = sender.send_to_forum(title, content)
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    import time
    start_time = time.time()
    
    print("ğŸš€ AiTrend æ¯å°æ—¶ç²¾é€‰æ¨¡å¼ï¼ˆæ‰©å±•ç‰ˆï¼‰", file=sys.stderr)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    
    # æ”¶é›†æ•°æ®
    print("\nğŸ“¡ æ­£åœ¨æ”¶é›†å„æ•°æ®æº...", file=sys.stderr)
    all_articles = collect_all_sources(config)
    print(f"\nğŸ“Š å…±æ”¶é›† {len(all_articles)} æ¡", file=sys.stderr)
    
    if not all_articles:
        print("âš ï¸ æ— æ•°æ®", file=sys.stderr)
        sys.exit(0)
    
    # å»é‡
    deduplicator = ArticleDeduplicator()
    articles = deduplicator.filter_new_articles(all_articles)
    
    seen_urls = set()
    unique_articles = []
    for article in articles:
        if article.url and article.url not in seen_urls:
            seen_urls.add(article.url)
            unique_articles.append(article)
    articles = unique_articles
    
    print(f"ğŸ” å»é‡å: {len(articles)} æ¡", file=sys.stderr)
    
    if not articles:
        print("âš ï¸ æ— æ–°å†…å®¹", file=sys.stderr)
        sys.exit(0)
    
    # é€‰æ‹©æœ€çƒ­é—¨çš„3æ¡
    top_articles = select_best_articles(articles, top_n=3)
    
    print(f"\nâ­ é€‰ä¸­ {len(top_articles)} æ¡:", file=sys.stderr)
    for i, article in enumerate(top_articles, 1):
        print(f"   {i}. {article.title[:50]}... ({article.source})", file=sys.stderr)
    
    # è·å– Webhook URL
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        with open('.env', 'r') as f:
            for line in f:
                if line.startswith('DISCORD_WEBHOOK_URL='):
                    webhook_url = line.strip().split('=', 1)[1]
                    break
    
    # å‘å¸ƒåˆ°è®ºå›
    print(f"\nğŸ“¤ æ­£åœ¨å‘å¸ƒ...", file=sys.stderr)
    results = []
    
    for i, article in enumerate(top_articles):
        delay = i * 2  # æ¯æ¡é—´éš”2ç§’
        result = post_single_article(article, webhook_url, delay=delay)
        results.append({
            'title': article.title[:40],
            'source': article.source,
            'success': result
        })
        status = "âœ…" if result else "âŒ"
        print(f"   {status} ç¬¬{i+1}æ¡å‘å¸ƒ{'æˆåŠŸ' if result else 'å¤±è´¥'}", file=sys.stderr)
    
    # è®°å½•å·²å‘é€
    deduplicator.record_sent_articles(top_articles)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(1 for r in results if r['success'])
    print(f"\nğŸ“ˆ å‘å¸ƒå®Œæˆ: {success_count}/{len(results)} æ¡æˆåŠŸ", file=sys.stderr)
    
    # è®°å½•è´¨é‡æ—¥å¿—
    duration_ms = int((time.time() - start_time) * 1000)
    log_publish_session(top_articles, success_count, duration_ms)
    
    # æ˜¾ç¤ºè´¨é‡æŠ¥å‘Šæ‘˜è¦
    print("\nğŸ“Š è´¨é‡æŠ¥å‘Š:", file=sys.stderr)
    sources_used = list(set(a.source for a in top_articles))
    print(f"  ä½¿ç”¨æ•°æ®æº: {', '.join(sources_used)}", file=sys.stderr)
    print(f"  å¹³å‡çƒ­åº¦åˆ†: {sum(calculate_hot_score(a) for a in top_articles)/len(top_articles):.1f}", file=sys.stderr)
    
    output = {
        "success": success_count == len(results),
        "total": len(results),
        "success_count": success_count,
        "posts": results,
        "sources": sources_used,
        "quality_logged": True
    }
    print(json.dumps(output, ensure_ascii=False))

if __name__ == '__main__':
    main()
