"""
LLMå®¢æˆ·ç«¯ - ä½¿ç”¨OpenClawé»˜è®¤å¤§æ¨¡å‹
é€šè¿‡ sessions_spawn è°ƒç”¨
"""

import os
import json
import time
from typing import Dict, Any, Optional

class LLMClient:
    """å¤§æ¨¡å‹å®¢æˆ·ç«¯ - OpenClawé›†æˆç‰ˆ"""
    
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY') or os.getenv('KIMI_API_KEY')
        self.use_external_api = bool(self.api_key)
        
        if self.use_external_api:
            self.base_url = os.getenv('LLM_BASE_URL', 'https://api.openai.com/v1')
            self.model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
    
    def generate(self, 
                 prompt: str, 
                 system_prompt: str = "",
                 temperature: float = 0.7,
                 max_tokens: int = 1000) -> str:
        """ç”Ÿæˆå†…å®¹"""
        
        if self.use_external_api:
            return self._generate_with_api(prompt, system_prompt, temperature, max_tokens)
        else:
            return self._generate_with_openclaw(prompt, system_prompt, max_tokens)
    
    def _generate_with_openclaw(self, prompt: str, system_prompt: str, max_tokens: int) -> str:
        """ä½¿ç”¨æœ¬åœ°è„šæœ¬ç”Ÿæˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        import tempfile
        import subprocess
        import os
        
        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"[System]\n{system_prompt}\n\n[User]\n{prompt}\n\n[Assistant]\n"
        
        print(f"    ğŸ¤– è°ƒç”¨LLMç”Ÿæˆ ({len(full_prompt)} å­—ç¬¦)...", end=' ')
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                f.write(full_prompt)
                prompt_file = f.name
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                output_file = f.name
            
            # è°ƒç”¨ç”Ÿæˆè„šæœ¬
            script_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'scripts', 'llm_generator.py')
            
            result = subprocess.run(
                ['python3', script_path, prompt_file, output_file],
                capture_output=True,
                text=True,
                timeout=120
            )
            
            # è¯»å–è¾“å‡º
            if os.path.exists(output_file):
                with open(output_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
            else:
                content = ""
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(prompt_file)
                os.unlink(output_file)
            except:
                pass
            
            if content:
                print("âœ…")
                return content
            else:
                print("âš ï¸ æ— è¾“å‡ºï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_extract(full_prompt)
            
        except Exception as e:
            print(f"âŒ {e}")
            return self._fallback_extract(full_prompt)
    
    def _generate_with_api(self, prompt: str, system_prompt: str,
                           temperature: float, max_tokens: int) -> str:
        """ä½¿ç”¨å¤–éƒ¨APIç”Ÿæˆ"""
        
        import requests
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            'model': self.model,
            'messages': messages,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        
        try:
            session = requests.Session()
            response = session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            data = response.json()
            return data['choices'][0]['message']['content'].strip()
            
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return ""
    
    def summarize(self, text: str, max_length: int = 500) -> str:
        """æ€»ç»“æ–‡æœ¬"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åˆ†æå¸ˆï¼Œæ“…é•¿ä»é¡¹ç›®æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”¨è‡ªç„¶è¯­è¨€æè¿°ã€‚

é‡è¦çº¦æŸï¼š
1. ç¦æ­¢ç»“æ„åŒ–è¾“å‡º - ä¸è¦ä½¿ç”¨åˆ—è¡¨ã€åºå·ã€ bullet points
2. ç¦æ­¢ç©ºè¯å¥—è¯ - ä¸è¦å†™"é’ˆå¯¹ç—›ç‚¹"ã€"åŠŸèƒ½è®¾è®¡"ã€"æ¶æ„æ¸…æ™°"ç­‰æ¨¡æ¿åŒ–å†…å®¹
3. å¿…é¡»è‡ªç„¶å™è¿° - åƒè·Ÿæœ‹å‹ä»‹ç»ä¸€ä¸ªå·¥å…·ä¸€æ ·ï¼Œå£è¯­åŒ–ã€æµç•…
4. çªå‡ºäº§å“ç‰¹ç‚¹ - å…·ä½“æ˜¯ä»€ä¹ˆã€èƒ½åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
5. çªå‡ºäº®ç‚¹ - æœ€ç‰¹åˆ«çš„åœ°æ–¹ã€æœ€å®ç”¨çš„åŠŸèƒ½
6. ä¿¡æ¯å¯†åº¦é«˜ - æ¯å¥è¯éƒ½è¦æœ‰ä»·å€¼ï¼Œä¸åºŸè¯

è¾“å‡ºé£æ ¼ï¼š
- ç”¨è¿ç»­çš„æ®µè½ï¼Œä¸æ˜¯åˆ—è¡¨
- ç”¨"å®ƒ"ã€"è¿™ä¸ªå·¥å…·"æ¥æŒ‡ä»£äº§å“
- ç›´æ¥è¯´åŠŸèƒ½ï¼Œä¸è¦"æ—¨åœ¨è§£å†³"ã€"è‡´åŠ›äº"
- ä¸¾ä¾‹è¯´æ˜ï¼Œä¸è¦æŠ½è±¡æè¿°"""
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é¡¹ç›®ä¿¡æ¯ï¼Œç”¨è‡ªç„¶å™è¿°çš„æ–¹å¼æè¿°è¿™ä¸ªäº§å“ï¼š

é¡¹ç›®ä¿¡æ¯ï¼š
{text[:3000]}

è¦æ±‚ï¼š
1. æ¸…æ™°æè¿°äº§å“æ˜¯ä»€ä¹ˆã€åšä»€ä¹ˆ
2. çªå‡ºæœ€å€¼å¾—å…³æ³¨çš„ç‰¹ç‚¹å’Œäº®ç‚¹
3. ç”¨è‡ªç„¶å™è¿°ï¼Œä¸è¦åˆ—è¡¨ã€ä¸è¦åºå·
4. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæè¿°å†…å®¹ï¼Œä¸è¦æ ‡é¢˜"""
        
        result = self.generate(prompt, system_prompt, temperature=0.5, max_tokens=max_length)
        
        if not result:
            return self._fallback_extract(text)
        
        return result
    
    def _fallback_extract(self, text: str) -> str:
        """å¤‡ç”¨æå–"""
        lines = text.split('\n')
        name = desc = ""
        features = []
        
        for line in lines:
            if 'é¡¹ç›®åç§°:' in line:
                name = line.split(':', 1)[1].strip()
            elif 'é¡¹ç›®æè¿°:' in line:
                desc = line.split(':', 1)[1].strip()
            elif 'åŠŸèƒ½åˆ—è¡¨:' in line:
                features = [f.strip() for f in line.split(':', 1)[1].split(',')]
        
        if name and desc:
            result = f"{name} {desc}"
            if features:
                result += f"ï¼Œå¯ä»¥{features[0]}"
            return result
        return ""

# å•ä¾‹
_llm_client = None

def get_llm_client() -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
