"""
LLMå®¢æˆ·ç«¯ - ç®€åŒ–ç‰ˆ
é€šè¿‡æ–‡ä»¶é€šä¿¡ä¸launcheräº¤äº’
"""

import os
import json
import tempfile
from typing import Dict, Any

class LLMClient:
    """
    å¤§æ¨¡å‹å®¢æˆ·ç«¯
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. å¤–éƒ¨APIæ¨¡å¼ï¼šç›´æ¥è°ƒç”¨API
    2. OpenClawæ¨¡å¼ï¼šé€šè¿‡æ–‡ä»¶ä¸launcheré€šä¿¡
    """
    
    def __init__(self):
        # ä¼˜å…ˆæ£€æŸ¥Gemini API
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        self.gemini_model = os.getenv('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')
        
        # å…¶ä»–API
        self.openai_api_key = os.getenv('OPENAI_API_KEY') or os.getenv('KIMI_API_KEY')
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªAPI
        if self.gemini_api_key:
            self.api_provider = 'gemini'
        elif self.openai_api_key:
            self.api_provider = 'openai'
            self.base_url = os.getenv('LLM_BASE_URL', 'https://api.openai.com/v1')
            self.model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
        else:
            self.api_provider = None
        
        # OpenClawé€šä¿¡æ–‡ä»¶
        self.request_file = '/tmp/openclaw_llm_request.json'
        self.response_file = '/tmp/openclaw_llm_response.txt'
    
    def generate(self, 
                 prompt: str, 
                 system_prompt: str = "",
                 temperature: float = 0.7,
                 max_tokens: int = 1000) -> str:
        """ç”Ÿæˆå†…å®¹ - å¿…é¡»æœ‰API Keyï¼Œå¦åˆ™æŠ¥é”™"""
        
        if self.api_provider == 'gemini':
            print(f"    ğŸ¤– ä½¿ç”¨Geminiç”Ÿæˆ...", end=' ')
            return self._generate_with_gemini(prompt, system_prompt, temperature, max_tokens)
        elif self.api_provider == 'openai':
            print(f"    ğŸ¤– ä½¿ç”¨OpenAIç”Ÿæˆ...", end=' ')
            return self._generate_with_api(prompt, system_prompt, temperature, max_tokens)
        else:
            raise RuntimeError("æœªé…ç½®LLM API Key (GEMINI_API_KEY/OPENAI_API_KEY/KIMI_API_KEY)ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹")
    
    def _generate_with_api(self, prompt: str, system_prompt: str,
                           temperature: float, max_tokens: int) -> str:
        """ä½¿ç”¨å¤–éƒ¨APIç”Ÿæˆ"""
        return self._generate_with_kimi(prompt, system_prompt, temperature, max_tokens)
    
    def _generate_with_gemini(self, prompt: str, system_prompt: str,
                               temperature: float, max_tokens: int) -> str:
        """ä½¿ç”¨Google Geminiç”Ÿæˆ"""
        
        import requests
        
        # Gemini API endpoint
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.gemini_model}:generateContent"
        
        headers = {
            'Content-Type': 'application/json',
        }
        
        # Gemini API format
        data = {
            "contents": [
                {
                    "parts": [
                        {"text": f"{system_prompt}\n\n{prompt}"}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
            }
        }
        
        try:
            response = requests.post(
                f"{url}?key={self.gemini_api_key}",
                headers=headers,
                json=data,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            
            # Extract text from Gemini response
            if 'candidates' in result and len(result['candidates']) > 0:
                content = result['candidates'][0].get('content', {})
                parts = content.get('parts', [])
                if parts:
                    generated_text = parts[0].get('text', '').strip()
                    print("âœ…")
                    return generated_text
            
            raise RuntimeError("Geminiè¿”å›ç©ºç»“æœ")
            
        except Exception as e:
            print(f"âŒ {e}")
            raise RuntimeError(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}") from e
    
    def summarize(self, text: str, max_length: int = 500) -> str:
        """æ€»ç»“æ–‡æœ¬"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åˆ†æå¸ˆï¼Œæ“…é•¿ä»é¡¹ç›®æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”¨è‡ªç„¶è¯­è¨€æè¿°ã€‚

é‡è¦çº¦æŸï¼š
1. ç¦æ­¢ç»“æ„åŒ–è¾“å‡º - ä¸è¦ä½¿ç”¨åˆ—è¡¨ã€åºå·ã€ bullet points
2. ç¦æ­¢ç©ºè¯å¥—è¯ - ä¸è¦å†™"é’ˆå¯¹ç—›ç‚¹"ã€"åŠŸèƒ½è®¾è®¡"ã€"æ¶æ„æ¸…æ™°"ç­‰æ¨¡æ¿åŒ–å†…å®¹
3. å¿…é¡»è‡ªç„¶å™è¿° - åƒè·Ÿæœ‹å‹ä»‹ç»ä¸€ä¸ªå·¥å…·ä¸€æ ·ï¼Œå£è¯­åŒ–ã€æµç•…
4. çªå‡ºäº§å“ç‰¹ç‚¹ - å…·ä½“æ˜¯ä»€ä¹ˆã€èƒ½åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
5. çªå‡ºäº®ç‚¹ - æœ€ç‰¹åˆ«çš„åœ°æ–¹ã€æœ€å®ç”¨çš„åŠŸèƒ½
6. ä¿¡æ¯å¯†åº¦é«˜ - æ¯å¥è¯éƒ½è¦æœ‰ä»·å€¼ï¼Œä¸åºŸè¯

è¾“å‡ºé£æ ¼ï¼š
- ç”¨è¿ç»­çš„æ®µè½ï¼Œä¸æ˜¯åˆ—è¡¨
- ç”¨"å®ƒ"ã€"è¿™ä¸ªå·¥å…·"æ¥æŒ‡ä»£äº§å“
- ç›´æ¥è¯´åŠŸèƒ½ï¼Œä¸è¦"æ—¨åœ¨è§£å†³"ã€"è‡´åŠ›äº"
- ä¸¾ä¾‹è¯´æ˜ï¼Œä¸è¦æŠ½è±¡æè¿°"""
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é¡¹ç›®ä¿¡æ¯ï¼Œç”¨è‡ªç„¶å™è¿°çš„æ–¹å¼æè¿°è¿™ä¸ªäº§å“ï¼š

é¡¹ç›®ä¿¡æ¯ï¼š
{text[:3000]}

è¦æ±‚ï¼š
1. æ¸…æ™°æè¿°äº§å“æ˜¯ä»€ä¹ˆã€åšä»€ä¹ˆ
2. çªå‡ºæœ€å€¼å¾—å…³æ³¨çš„ç‰¹ç‚¹å’Œäº®ç‚¹
3. ç”¨è‡ªç„¶å™è¿°ï¼Œä¸è¦åˆ—è¡¨ã€ä¸è¦åºå·
4. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæè¿°å†…å®¹ï¼Œä¸è¦æ ‡é¢˜"""
        
        result = self.generate(prompt, system_prompt, temperature=0.5, max_tokens=max_length)
        
        if not result:
            return self._fallback_extract(text)
        
        return result
    
    def _fallback_extract(self, text: str) -> str:
        """å¤‡ç”¨æå– - æ™ºèƒ½æå–å…³é”®ä¿¡æ¯"""
        
        lines = text.split('\n')
        name = ""
        description = ""
        features = []
        install = ""
        usage = ""
        
        for line in lines:
            line = line.strip()
            if line.startswith('é¡¹ç›®åç§°:') or 'é¡¹ç›®åç§°:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    name = parts[1].strip()
            elif line.startswith('é¡¹ç›®æè¿°:') or 'é¡¹ç›®æè¿°:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    description = parts[1].strip()
            elif line.startswith('åŠŸèƒ½åˆ—è¡¨:') or 'åŠŸèƒ½åˆ—è¡¨:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    features = [f.strip() for f in parts[1].split(',')]
            elif line.startswith('å®‰è£…æ–¹å¼:') or 'å®‰è£…æ–¹å¼:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    install = parts[1].strip()
            elif line.startswith('ä½¿ç”¨ç¤ºä¾‹:') or 'ä½¿ç”¨ç¤ºä¾‹:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    usage = parts[1].strip()
        
        # æ„å»ºè‡ªç„¶å™è¿°
        if name and description:
            parts = [f"{name} {description}"]
            
            if features:
                feats_text = "ã€".join(features[:3])
                parts.append(f"å¯ä»¥{feats_text}")
            
            if install:
                parts.append(f"å®‰è£…å‘½ä»¤æ˜¯{install}")
            
            result = "ã€‚".join(parts)
            print(f"âœ… ({len(result)} å­—ç¬¦)")
            return result
        
        print("âš ï¸ æå–å¤±è´¥")
        return ""

# å•ä¾‹
_llm_client = None

def get_llm_client() -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
