"""
LLMå®¢æˆ·ç«¯ - ç®€åŒ–ç‰ˆ
é€šè¿‡æ–‡ä»¶é€šä¿¡ä¸launcheräº¤äº’
"""

import os
import json
import tempfile
from typing import Dict, Any

class LLMClient:
    """
    å¤§æ¨¡å‹å®¢æˆ·ç«¯
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. å¤–éƒ¨APIæ¨¡å¼ï¼šç›´æ¥è°ƒç”¨API
    2. OpenClawæ¨¡å¼ï¼šé€šè¿‡æ–‡ä»¶ä¸launcheré€šä¿¡
    """
    
    def __init__(self):
        self.api_key = os.getenv('OPENAI_API_KEY') or os.getenv('KIMI_API_KEY')
        self.use_external_api = bool(self.api_key)
        
        if self.use_external_api:
            self.base_url = os.getenv('LLM_BASE_URL', 'https://api.openai.com/v1')
            self.model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
        
        # OpenClawé€šä¿¡æ–‡ä»¶
        self.request_file = '/tmp/openclaw_llm_request.json'
        self.response_file = '/tmp/openclaw_llm_response.txt'
    
    def generate(self, 
                 prompt: str, 
                 system_prompt: str = "",
                 temperature: float = 0.7,
                 max_tokens: int = 1000) -> str:
        """ç”Ÿæˆå†…å®¹"""
        
        if self.use_external_api:
            return self._generate_with_api(prompt, system_prompt, temperature, max_tokens)
        else:
            # OpenClawæ¨¡å¼ - ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆç®€å•æå–ï¼‰
            # å› ä¸ºæ— æ³•ç›´æ¥ä»å­è¿›ç¨‹ä¸­è°ƒç”¨sessions_spawn
            print(f"    ğŸ¤– ä½¿ç”¨å¤‡ç”¨ç”Ÿæˆæ–¹æ¡ˆ...", end=' ')
            return self._fallback_extract(prompt)
    
    def _generate_with_api(self, prompt: str, system_prompt: str,
                           temperature: float, max_tokens: int) -> str:
        """ä½¿ç”¨å¤–éƒ¨APIç”Ÿæˆ"""
        
        import requests
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            'model': self.model,
            'messages': messages,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        
        try:
            session = requests.Session()
            response = session.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            data = response.json()
            result = data['choices'][0]['message']['content'].strip()
            print("âœ…")
            return result
            
        except Exception as e:
            print(f"âŒ {e}")
            return self._fallback_extract(prompt)
    
    def summarize(self, text: str, max_length: int = 500) -> str:
        """æ€»ç»“æ–‡æœ¬"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“åˆ†æå¸ˆï¼Œæ“…é•¿ä»é¡¹ç›®æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”¨è‡ªç„¶è¯­è¨€æè¿°ã€‚

é‡è¦çº¦æŸï¼š
1. ç¦æ­¢ç»“æ„åŒ–è¾“å‡º - ä¸è¦ä½¿ç”¨åˆ—è¡¨ã€åºå·ã€ bullet points
2. ç¦æ­¢ç©ºè¯å¥—è¯ - ä¸è¦å†™"é’ˆå¯¹ç—›ç‚¹"ã€"åŠŸèƒ½è®¾è®¡"ã€"æ¶æ„æ¸…æ™°"ç­‰æ¨¡æ¿åŒ–å†…å®¹
3. å¿…é¡»è‡ªç„¶å™è¿° - åƒè·Ÿæœ‹å‹ä»‹ç»ä¸€ä¸ªå·¥å…·ä¸€æ ·ï¼Œå£è¯­åŒ–ã€æµç•…
4. çªå‡ºäº§å“ç‰¹ç‚¹ - å…·ä½“æ˜¯ä»€ä¹ˆã€èƒ½åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
5. çªå‡ºäº®ç‚¹ - æœ€ç‰¹åˆ«çš„åœ°æ–¹ã€æœ€å®ç”¨çš„åŠŸèƒ½
6. ä¿¡æ¯å¯†åº¦é«˜ - æ¯å¥è¯éƒ½è¦æœ‰ä»·å€¼ï¼Œä¸åºŸè¯

è¾“å‡ºé£æ ¼ï¼š
- ç”¨è¿ç»­çš„æ®µè½ï¼Œä¸æ˜¯åˆ—è¡¨
- ç”¨"å®ƒ"ã€"è¿™ä¸ªå·¥å…·"æ¥æŒ‡ä»£äº§å“
- ç›´æ¥è¯´åŠŸèƒ½ï¼Œä¸è¦"æ—¨åœ¨è§£å†³"ã€"è‡´åŠ›äº"
- ä¸¾ä¾‹è¯´æ˜ï¼Œä¸è¦æŠ½è±¡æè¿°"""
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é¡¹ç›®ä¿¡æ¯ï¼Œç”¨è‡ªç„¶å™è¿°çš„æ–¹å¼æè¿°è¿™ä¸ªäº§å“ï¼š

é¡¹ç›®ä¿¡æ¯ï¼š
{text[:3000]}

è¦æ±‚ï¼š
1. æ¸…æ™°æè¿°äº§å“æ˜¯ä»€ä¹ˆã€åšä»€ä¹ˆ
2. çªå‡ºæœ€å€¼å¾—å…³æ³¨çš„ç‰¹ç‚¹å’Œäº®ç‚¹
3. ç”¨è‡ªç„¶å™è¿°ï¼Œä¸è¦åˆ—è¡¨ã€ä¸è¦åºå·
4. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
5. ç›´æ¥è¾“å‡ºæè¿°å†…å®¹ï¼Œä¸è¦æ ‡é¢˜"""
        
        result = self.generate(prompt, system_prompt, temperature=0.5, max_tokens=max_length)
        
        if not result:
            return self._fallback_extract(text)
        
        return result
    
    def _fallback_extract(self, text: str) -> str:
        """å¤‡ç”¨æå– - æ™ºèƒ½æå–å…³é”®ä¿¡æ¯"""
        
        lines = text.split('\n')
        name = ""
        description = ""
        features = []
        install = ""
        usage = ""
        
        for line in lines:
            line = line.strip()
            if line.startswith('é¡¹ç›®åç§°:') or 'é¡¹ç›®åç§°:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    name = parts[1].strip()
            elif line.startswith('é¡¹ç›®æè¿°:') or 'é¡¹ç›®æè¿°:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    description = parts[1].strip()
            elif line.startswith('åŠŸèƒ½åˆ—è¡¨:') or 'åŠŸèƒ½åˆ—è¡¨:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    features = [f.strip() for f in parts[1].split(',')]
            elif line.startswith('å®‰è£…æ–¹å¼:') or 'å®‰è£…æ–¹å¼:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    install = parts[1].strip()
            elif line.startswith('ä½¿ç”¨ç¤ºä¾‹:') or 'ä½¿ç”¨ç¤ºä¾‹:' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    usage = parts[1].strip()
        
        # æ„å»ºè‡ªç„¶å™è¿°
        if name and description:
            parts = [f"{name} {description}"]
            
            if features:
                feats_text = "ã€".join(features[:3])
                parts.append(f"å¯ä»¥{feats_text}")
            
            if install:
                parts.append(f"å®‰è£…å‘½ä»¤æ˜¯{install}")
            
            result = "ã€‚".join(parts)
            print(f"âœ… ({len(result)} å­—ç¬¦)")
            return result
        
        print("âš ï¸ æå–å¤±è´¥")
        return ""

# å•ä¾‹
_llm_client = None

def get_llm_client() -> LLMClient:
    """è·å–LLMå®¢æˆ·ç«¯å•ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
