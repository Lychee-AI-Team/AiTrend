"""
arXiv è®ºæ–‡å†…å®¹ç”Ÿæˆå™¨

ç”Ÿæˆä¸­æ–‡ä»‹ç»æ–‡æœ¬ï¼š
- è®ºæ–‡æ ¸å¿ƒè´¡çŒ®
- æ–¹æ³•æ¦‚è¿°
- å®éªŒç»“æœäº®ç‚¹
- é€‚ç”¨åœºæ™¯

æœ€ç»ˆè¾“å‡ºï¼šä¸­æ–‡
"""

import json
from typing import Dict, Any, Optional, List
from datetime import datetime


class ArxivContentComposer:
    """arXiv è®ºæ–‡ä¸­æ–‡å†…å®¹ç”Ÿæˆå™¨"""
    
    # åˆ†ç±»ä¸­æ–‡æ˜ å°„
    CATEGORY_NAMES = {
        'cs.AI': 'äººå·¥æ™ºèƒ½',
        'cs.CL': 'è®¡ç®—è¯­è¨€å­¦',
        'cs.LG': 'æœºå™¨å­¦ä¹ ',
        'cs.CV': 'è®¡ç®—æœºè§†è§‰',
        'cs.IR': 'ä¿¡æ¯æ£€ç´¢',
        'cs.RO': 'æœºå™¨äººå­¦',
        'cs.CR': 'å¯†ç å­¦ä¸å®‰å…¨',
        'cs.DB': 'æ•°æ®åº“',
        'cs.DC': 'åˆ†å¸ƒå¼è®¡ç®—',
        'cs.HC': 'äººæœºäº¤äº’',
        'cs.NE': 'ç¥ç»ä¸è¿›åŒ–è®¡ç®—',
        'cs.SE': 'è½¯ä»¶å·¥ç¨‹',
        'stat.ML': 'ç»Ÿè®¡æœºå™¨å­¦ä¹ '
    }
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
    def _get_category_name(self, category: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡åç§°"""
        return self.CATEGORY_NAMES.get(category, category)
    
    def _extract_first_sentence(self, text: str, max_length: int = 200) -> str:
        """æå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ¦‚è¿°"""
        # æŒ‰å¥å·åˆ†å‰²ï¼Œå–ç¬¬ä¸€å¥
        sentences = text.split('.')
        if sentences:
            first = sentences[0].strip()
            if len(first) > max_length:
                first = first[:max_length] + "..."
            return first
        return text[:max_length]
    
    def compose(self, paper: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆè®ºæ–‡çš„ä¸­æ–‡ä»‹ç»
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            
        Returns:
            ä¸­æ–‡ä»‹ç»æ–‡æœ¬
        """
        title = paper.get('title', '')
        summary = paper.get('summary', '')
        authors = paper.get('authors', [])
        categories = paper.get('categories', [])
        primary_cat = paper.get('primary_category', '')
        published = paper.get('published_str', '')
        abs_url = paper.get('abs_url', '')
        pdf_url = paper.get('pdf_url', '')
        
        # è·å–åˆ†ç±»ä¸­æ–‡å
        cat_names = [self._get_category_name(cat) for cat in categories[:3]]
        primary_cat_name = self._get_category_name(primary_cat)
        
        # ä½œè€…ä¿¡æ¯ï¼ˆæœ€å¤š3ä½ï¼‰
        author_str = ', '.join(authors[:3])
        if len(authors) > 3:
            author_str += f" ç­‰ {len(authors)} ä½ä½œè€…"
        
        # æå–æ ¸å¿ƒæ‘˜è¦ï¼ˆç®€åŒ–ï¼‰
        # å»é™¤ LaTeX ç¬¦å·
        clean_summary = summary.replace('$', '').replace('\\', '')
        # å–å‰200å­—
        brief_summary = clean_summary[:200].strip()
        if len(clean_summary) > 200:
            brief_summary += "..."
        
        # æ„å»ºå†…å®¹
        parts = []
        
        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        parts.append(f"**{title}**")
        parts.append("")
        
        # å…ƒä¿¡æ¯
        parts.append(f"ğŸ“š {primary_cat_name} | {published} | {author_str}")
        parts.append("")
        
        # è®ºæ–‡æ¦‚è¿°
        parts.append("**ç ”ç©¶æ¦‚è¿°**")
        parts.append(brief_summary)
        parts.append("")
        
        # æ ¸å¿ƒè´¡çŒ®ï¼ˆä»æ‘˜è¦æ¨æ–­ï¼‰
        parts.append("**æ ¸å¿ƒè´¡çŒ®**")
        # ç®€å•å¯å‘å¼æå–
        if 'propose' in summary.lower() or 'introduce' in summary.lower():
            parts.append("æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ã€‚")
        elif 'improve' in summary.lower() or 'better' in summary.lower():
            parts.append("æœ¬ç ”ç©¶åœ¨æ€§èƒ½ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚")
        elif 'benchmark' in summary.lower() or 'dataset' in summary.lower():
            parts.append("æœ¬æ–‡æ„å»ºäº†æ–°çš„åŸºå‡†æµ‹è¯•æˆ–æ•°æ®é›†ï¼Œæ¨åŠ¨äº†é¢†åŸŸå‘å±•ã€‚")
        elif 'survey' in summary.lower() or 'review' in summary.lower():
            parts.append("è¿™æ˜¯ä¸€ç¯‡ç»¼è¿°æ€§è®ºæ–‡ï¼Œç³»ç»Ÿæ¢³ç†äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚")
        else:
            parts.append("æœ¬æ–‡é’ˆå¯¹è¯¥é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜æå‡ºäº†åˆ›æ–°æ€§è§£å†³æ–¹æ¡ˆã€‚")
        
        parts.append("")
        
        # é€‚ç”¨åœºæ™¯
        parts.append("**é€‚ç”¨åœºæ™¯**")
        if 'cs.CV' in categories:
            parts.append("é€‚ç”¨äºå›¾åƒè¯†åˆ«ã€è§†è§‰ç†è§£ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚")
        elif 'cs.CL' in categories:
            parts.append("é€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ–‡æœ¬ç†è§£ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚")
        elif 'cs.LG' in categories:
            parts.append("é€‚ç”¨äºæœºå™¨å­¦ä¹ ç†è®ºç ”ç©¶ã€ç®—æ³•ä¼˜åŒ–ã€æ¨¡å‹æ”¹è¿›ç­‰ã€‚")
        elif 'cs.AI' in categories:
            parts.append("é€‚ç”¨äºäººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡ä¸åº”ç”¨ã€æ™ºèƒ½å†³ç­–ç­‰åœºæ™¯ã€‚")
        else:
            parts.append("é€‚ç”¨äºç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå·¥ç¨‹å®è·µã€‚")
        
        parts.append("")
        
        # é“¾æ¥
        parts.append("**è·å–è®ºæ–‡**")
        parts.append(f"ğŸ“„ è®ºæ–‡é¡µé¢: {abs_url}")
        if pdf_url:
            parts.append(f"ğŸ“¥ PDFä¸‹è½½: {pdf_url}")
        
        return '\n'.join(parts)
    
    def compose_narrative(self, paper: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆå™è¿°å¼ä¸­æ–‡ä»‹ç»ï¼ˆæ›´è‡ªç„¶çš„æ–‡æœ¬ï¼‰
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯
            
        Returns:
            è‡ªç„¶å™è¿°å¼ä¸­æ–‡æ–‡æœ¬
        """
        title = paper.get('title', '')
        summary = paper.get('summary', '')
        authors = paper.get('authors', [])
        categories = paper.get('categories', [])
        primary_cat = paper.get('primary_category', '')
        published = paper.get('published_str', '')
        abs_url = paper.get('abs_url', '')
        
        # åˆ†ç±»å
        cat_names = [self._get_category_name(cat) for cat in categories[:2]]
        primary_cat_name = self._get_category_name(primary_cat)
        
        # ä½œè€…ï¼ˆæœ€å¤š2ä½ï¼‰
        author_display = ', '.join(authors[:2])
        if len(authors) > 2:
            author_display += " ç­‰"
        
        # æå–æ ¸å¿ƒä¸»é¢˜å’Œæ–¹æ³•
        topic = self._extract_topic(summary)
        method_desc = self._extract_method_description(summary)
        
        # ç”Ÿæˆå†…å®¹ï¼ˆå™è¿°å¼ï¼‰
        lines = []
        
        # å¼€å¤´ï¼šç›´æ¥ä»‹ç»è®ºæ–‡ï¼ˆç®€æ´ï¼‰
        lines.append(f"è¿™ç¯‡ {primary_cat_name} è®ºæ–‡ç”± {author_display} æå‡ºï¼Œèšç„¦ {topic}ã€‚")
        
        # æ–¹æ³•/è´¡çŒ®ï¼ˆä¸­æ–‡æè¿°ï¼‰
        contribution = self._generate_contribution_text(summary, categories)
        lines.append(contribution)
        
        # å®éªŒ/ç»“æœï¼ˆå¦‚æœ‰ï¼‰
        if self._has_experiments(summary):
            lines.append("å®éªŒéªŒè¯è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½æ€§èƒ½ã€‚")
        
        # ä»·å€¼/æ„ä¹‰
        value = self._generate_value_text(primary_cat_name, categories)
        lines.append(value)
        
        # é“¾æ¥
        lines.append("")
        lines.append(f"ğŸ“„ è®ºæ–‡: {abs_url}")
        
        return '\n'.join(lines)
    
    def _extract_topic(self, summary: str) -> str:
        """ä»æ‘˜è¦æå–ç ”ç©¶ä¸»é¢˜"""
        summary_lower = summary.lower()
        
        # å…³é”®è¯åˆ°ä¸­æ–‡çš„æ˜ å°„
        topics = [
            ('large language model', 'å¤§è¯­è¨€æ¨¡å‹'),
            ('llm', 'å¤§è¯­è¨€æ¨¡å‹'),
            ('transformer', 'Transformeræ¶æ„'),
            ('diffusion model', 'æ‰©æ•£æ¨¡å‹'),
            ('neural network', 'ç¥ç»ç½‘ç»œ'),
            ('deep learning', 'æ·±åº¦å­¦ä¹ '),
            ('reinforcement learning', 'å¼ºåŒ–å­¦ä¹ '),
            ('computer vision', 'è®¡ç®—æœºè§†è§‰'),
            ('video generation', 'è§†é¢‘ç”Ÿæˆ'),
            ('image generation', 'å›¾åƒç”Ÿæˆ'),
            ('natural language processing', 'è‡ªç„¶è¯­è¨€å¤„ç†'),
            ('nlp', 'è‡ªç„¶è¯­è¨€å¤„ç†'),
            ('machine learning', 'æœºå™¨å­¦ä¹ '),
            ('artificial intelligence', 'äººå·¥æ™ºèƒ½'),
            ('generative', 'ç”Ÿæˆå¼AI'),
            ('multimodal', 'å¤šæ¨¡æ€å­¦ä¹ '),
            ('optimization', 'ä¼˜åŒ–æ–¹æ³•'),
            ('robotics', 'æœºå™¨äººå­¦'),
            ('prompt', 'æç¤ºå·¥ç¨‹'),
        ]
        
        for eng, chn in topics:
            if eng in summary_lower:
                return chn
        
        return "è¯¥é¢†åŸŸå‰æ²¿é—®é¢˜"
    
    def _extract_method_description(self, summary: str) -> str:
        """æå–æ–¹æ³•æè¿°"""
        # æ¸…ç†å¹¶ç®€åŒ–æ‘˜è¦
        clean = summary.replace('$', '').replace('\\', '').replace('\n', ' ')
        
        # è¯†åˆ«æ–¹æ³•ç±»å‹
        if 'propose' in summary.lower() or 'introduce' in summary.lower():
            return "æå‡ºæ–°æ–¹æ³•"
        elif 'improve' in summary.lower() or 'enhance' in summary.lower():
            return "æ”¹è¿›ç°æœ‰æ–¹æ¡ˆ"
        elif 'survey' in summary.lower() or 'review' in summary.lower():
            return "ç³»ç»Ÿæ€§ç»¼è¿°"
        else:
            return "æ·±å…¥ç ”ç©¶"
    
    def _generate_contribution_text(self, summary: str, categories: List[str]) -> str:
        """ç”Ÿæˆè´¡çŒ®æè¿°"""
        summary_lower = summary.lower()
        
        # æ ¹æ®æ‘˜è¦å†…å®¹é€‰æ‹©æè¿°
        if 'propose' in summary_lower or 'present' in summary_lower:
            if 'cs.CV' in categories:
                return "æå‡ºäº†ä¸€ç§è§†è§‰å¤„ç†æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ç”Ÿæˆè´¨é‡ä¸ä¸€è‡´æ€§ã€‚"
            elif 'cs.CL' in categories:
                return "æå‡ºäº†ä¸€ç§è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šæœ‰æ‰€åˆ›æ–°ã€‚"
            elif 'cs.LG' in categories:
                return "æå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œæ”¹è¿›äº†æ¨¡å‹è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚"
            else:
                return "æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¡†æ¶ï¼Œé’ˆå¯¹ç°æœ‰æŒ‘æˆ˜ç»™å‡ºäº†è§£å†³æ–¹æ¡ˆã€‚"
        
        elif 'improve' in summary_lower or 'better' in summary_lower:
            return "åœ¨ç°æœ‰æ–¹æ³•åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œæå‡äº†æ€§èƒ½å’Œç¨³å®šæ€§ã€‚"
        
        elif 'survey' in summary_lower or 'review' in summary_lower:
            return "å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢³ç†ï¼Œæ€»ç»“äº†å½“å‰è¿›å±•å’Œæœªæ¥æ–¹å‘ã€‚"
        
        else:
            return "é’ˆå¯¹æ ¸å¿ƒé—®é¢˜å±•å¼€ç ”ç©¶ï¼Œç»™å‡ºäº†æœ‰ä»·å€¼çš„ç†è®ºæˆ–å®è·µè´¡çŒ®ã€‚"
    
    def _has_experiments(self, summary: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å®éªŒéƒ¨åˆ†"""
        keywords = ['experiment', 'result', 'benchmark', 'evaluate', 'dataset']
        return any(kw in summary.lower() for kw in keywords)
    
    def _generate_value_text(self, primary_cat: str, categories: List[str]) -> str:
        """ç”Ÿæˆä»·å€¼æè¿°"""
        if 'è®¡ç®—æœºè§†è§‰' in primary_cat:
            return "å¯¹è§†è§‰ç†è§£å’Œç”Ÿæˆé¢†åŸŸå…·æœ‰å‚è€ƒä»·å€¼ï¼Œé€‚åˆå…³æ³¨å›¾åƒè§†é¢‘æŠ€æœ¯çš„ç ”ç©¶è€…ã€‚"
        elif 'è®¡ç®—è¯­è¨€å­¦' in primary_cat or 'è‡ªç„¶è¯­è¨€å¤„ç†' in primary_cat:
            return "å¯¹è¯­è¨€æŠ€æœ¯é¢†åŸŸæœ‰æ‰€è´¡çŒ®ï¼Œé€‚åˆä»äº‹æ–‡æœ¬ç†è§£å’Œç”Ÿæˆçš„ç ”ç©¶äººå‘˜ã€‚"
        elif 'æœºå™¨å­¦ä¹ ' in primary_cat:
            return "ä¸ºæœºå™¨å­¦ä¹ ç†è®ºå’Œå®è·µæä¾›äº†æ–°æ€è·¯ï¼Œé€‚åˆç®—æ³•ç ”ç©¶è€…å…³æ³¨ã€‚"
        elif 'äººå·¥æ™ºèƒ½' in primary_cat:
            return "æ¨åŠ¨äº†AIæŠ€æœ¯å‘å±•ï¼Œå¯¹ç ”ç©¶å’Œåº”ç”¨å‡æœ‰å‚è€ƒæ„ä¹‰ã€‚"
        else:
            return "ä¸ºè¯¥é¢†åŸŸçš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒï¼Œå€¼å¾—å…³æ³¨ã€‚"


if __name__ == "__main__":
    # æµ‹è¯•
    test_paper = {
        'title': 'Attention Is All You Need',
        'summary': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.',
        'authors': ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar'],
        'categories': ['cs.CL', 'cs.LG'],
        'primary_category': 'cs.CL',
        'published_str': '2024-01-15',
        'abs_url': 'https://arxiv.org/abs/1706.03762',
        'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf'
    }
    
    composer = ArxivContentComposer()
    
    print("=== ç»“æ„åŒ–æ ¼å¼ ===")
    print(composer.compose(test_paper))
    print("\n=== å™è¿°å¼æ ¼å¼ ===")
    print(composer.compose_narrative(test_paper))
