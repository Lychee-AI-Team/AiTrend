#!/usr/bin/env python3
"""
Reddit ä¿¡æ¯æºæ¨¡å—
ä» Reddit è·å–çƒ­é—¨æŠ€æœ¯è®¨è®º
"""

import os
import re
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any
from modules.logger import get_logger
from modules.sources.base import BaseSource

logger = get_logger()

class Reddit(BaseSource):
    """
    Reddit ä¿¡æ¯æºæ¨¡å—
    
    åŠŸèƒ½ï¼š
    - è·å–æŒ‡å®š subreddit çš„çƒ­é—¨å¸–å­
    - è¿‡æ»¤æŠ€æœ¯ç›¸å…³å†…å®¹
    - æå–é«˜èµè¯„è®º
    
    æŒ–æ˜æ ‡å‡†ï¼š
    - æŠ•ç¥¨æ•° > 50
    - ä¸æ˜¯é‡å¤å†…å®¹
    - é“¾æ¥æœ‰æ•ˆ
    - æ—¶é—´ï¼šæœ€è¿‘7å¤©
    
    ä¸ HN/PH çš„å·®å¼‚ï¼š
    - è‰æ ¹ç¤¾åŒºè®¨è®º
    - çœŸå®ç”¨æˆ·åé¦ˆ
    - å¯èƒ½åŒ…å«æ•™ç¨‹/ç»éªŒåˆ†äº«
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.subreddits = config.get('subreddits', ['MachineLearning', 'LocalLLaMA', 'artificial', 'technology'])
        self.min_upvotes = config.get('min_upvotes', 50)
        self.max_candidates = config.get('max_candidates', 10)
        self.time_window = config.get('time_window', 7)  # å¤©æ•°
        
        # Reddit JSON APIï¼ˆæ— éœ€è®¤è¯ï¼Œåªè¯»ï¼‰
        self.base_url = "https://www.reddit.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        logger.info(f"Reddit æ¨¡å—åˆå§‹åŒ–")
        logger.info(f"  - ç›®æ ‡ç¤¾åŒº: {', '.join(self.subreddits)}")
        logger.info(f"  - æœ€å°æŠ•ç¥¨: {self.min_upvotes}")
        logger.info(f"  - æ—¶é—´çª—å£: {self.time_window}å¤©")
    
    def is_enabled(self) -> bool:
        """Reddit ä½¿ç”¨å…¬å¼€APIï¼Œæ— éœ€è®¤è¯"""
        return True
    
    def discover(self) -> List[Dict[str, Any]]:
        """
        å‘ç°çƒ­é—¨å¸–å­
        
        è¿”å› Reddit ä¸ŠæŠ•ç¥¨æ•°è¾¾æ ‡çš„æŠ€æœ¯ç›¸å…³å¸–å­
        """
        logger.section("ğŸ“¡ ä» Reddit æŒ–æ˜çƒ­é—¨å¸–å­")
        
        all_posts = []
        
        # éå†æ¯ä¸ª subreddit
        for subreddit in self.subreddits:
            try:
                logger.info(f"  è·å– r/{subreddit}...")
                posts = self._fetch_subreddit_posts(subreddit)
                logger.info(f"    è·å– {len(posts)} ä¸ªå¸–å­")
                all_posts.extend(posts)
            except Exception as e:
                logger.error(f"    è·å–å¤±è´¥: {e}")
        
        # å»é‡ï¼ˆæŒ‰ URLï¼‰
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            url = post.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_posts.append(post)
        
        # æŒ‰æŠ•ç¥¨æ•°æ’åº
        sorted_posts = sorted(unique_posts, key=lambda x: x.get('upvotes', 0), reverse=True)
        
        # é™åˆ¶æ•°é‡
        result = sorted_posts[:self.max_candidates]
        
        logger.info(f"âœ… æ€»è®¡å‘ç° {len(result)} ä¸ªå€™é€‰å¸–å­")
        
        return result
    
    def _fetch_subreddit_posts(self, subreddit: str) -> List[Dict]:
        """è·å–æŒ‡å®š subreddit çš„çƒ­é—¨å¸–å­"""
        
        # ä½¿ç”¨ Reddit JSON API
        url = f"{self.base_url}/r/{subreddit}/hot.json"
        
        params = {
            'limit': 25  # è·å–å‰25ä¸ª
        }
        
        try:
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            posts_data = data.get('data', {}).get('children', [])
            
            candidates = []
            
            for child in posts_data:
                post = child.get('data', {})
                
                # è·³è¿‡ç½®é¡¶å¸–å’Œå¹¿å‘Š
                if post.get('stickied') or post.get('is_promoted'):
                    continue
                
                upvotes = post.get('ups', 0)
                num_comments = post.get('num_comments', 0)
                title = post.get('title', '')
                url_link = post.get('url', '')
                permalink = post.get('permalink', '')
                created_utc = post.get('created_utc', 0)
                
                # è¿‡æ»¤æŠ•ç¥¨æ•°
                if upvotes < self.min_upvotes:
                    continue
                
                # æ£€æŸ¥æ—¶é—´ï¼ˆæœ€è¿‘7å¤©ï¼‰
                post_time = datetime.fromtimestamp(created_utc)
                if (datetime.now() - post_time).days > self.time_window:
                    continue
                
                # è·³è¿‡è‡ªæ‰˜ç®¡å†…å®¹ï¼ˆæ²¡æœ‰å¤–éƒ¨é“¾æ¥ï¼‰
                if url_link.startswith('/r/'):
                    continue
                
                # è·å–é«˜èµè¯„è®º
                top_comments = self._fetch_top_comments(permalink, limit=2)
                
                candidate = {
                    'name': self._extract_project_name(title, url_link),
                    'title': title,
                    'url': url_link,
                    'reddit_url': f"https://www.reddit.com{permalink}",
                    'upvotes': upvotes,
                    'comments': num_comments,
                    'subreddit': subreddit,
                    'top_comments': top_comments,
                    'created_at': post_time.isoformat(),
                    'source_type': 'reddit',
                    'source_name': 'Reddit'
                }
                
                candidates.append(candidate)
            
            return candidates
            
        except Exception as e:
            logger.error(f"  è¯·æ±‚ r/{subreddit} å¤±è´¥: {e}")
            return []
    
    def _fetch_top_comments(self, permalink: str, limit: int = 2) -> List[str]:
        """è·å–å¸–å­çš„çƒ­é—¨è¯„è®º"""
        
        url = f"{self.base_url}{permalink}.json"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # ç¬¬äºŒä¸ªå…ƒç´ åŒ…å«è¯„è®º
            if len(data) < 2:
                return []
            
            comments_data = data[1].get('data', {}).get('children', [])
            
            comments = []
            for child in comments_data[:5]:  # æ£€æŸ¥å‰5æ¡
                comment_data = child.get('data', {})
                
                # è·³è¿‡ MoreComments
                if child.get('kind') != 't1':
                    continue
                
                body = comment_data.get('body', '')
                ups = comment_data.get('ups', 0)
                
                # æ¸…ç† markdown å’Œ HTML
                body = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', body)  # ç§»é™¤ markdown é“¾æ¥
                body = re.sub(r'[*_#]', '', body)  # ç§»é™¤ markdown æ ¼å¼
                body = body.strip()
                
                if body and len(body) > 30 and ups > 5:  # æœ‰æ„ä¹‰çš„è¯„è®º
                    comments.append({
                        'text': body[:250],
                        'upvotes': ups
                    })
            
            # æŒ‰æŠ•ç¥¨æ’åº
            comments.sort(key=lambda x: x['upvotes'], reverse=True)
            
            return [c['text'] for c in comments[:limit]]
            
        except Exception as e:
            logger.debug(f"  è·å–è¯„è®ºå¤±è´¥: {e}")
            return []
    
    def _extract_project_name(self, title: str, url: str) -> str:
        """ä»æ ‡é¢˜æˆ– URL æå–é¡¹ç›®å"""
        # å°è¯•ä»æ ‡é¢˜æå–
        # æ¨¡å¼: "Project Name - description"
        match = re.match(r'^([^-â€“:]+)', title)
        if match:
            name = match.group(1).strip()
            if len(name) < 50:
                return name[:50]
        
        # ä» URL æå–
        if 'github.com' in url:
            parts = url.split('/')
            if len(parts) >= 3:
                return parts[-1][:50] if parts[-1] else parts[-2][:50]
        
        # é»˜è®¤è¿”å›æ ‡é¢˜å‰50å­—
        return title[:50]
    
    def get_details(self, candidate: Dict) -> Dict[str, Any]:
        """è·å–å¸–å­è¯¦ç»†ä¿¡æ¯"""
        return candidate

# æµ‹è¯•
if __name__ == '__main__':
    print("="*60)
    print("Reddit ä¿¡æ¯æºæ¨¡å—æµ‹è¯•")
    print("="*60)
    
    config = {
        'subreddits': ['MachineLearning', 'technology'],
        'min_upvotes': 30,
        'max_candidates': 5
    }
    
    source = Reddit(config)
    candidates = source.discover()
    
    print(f"\nå‘ç° {len(candidates)} ä¸ªå€™é€‰å¸–å­:")
    for i, c in enumerate(candidates, 1):
        print(f"\n{i}. {c['name']}")
        print(f"   æ ‡é¢˜: {c['title'][:60]}...")
        print(f"   æŠ•ç¥¨: {c['upvotes']}, è¯„è®º: {c['comments']}")
        print(f"   ç¤¾åŒº: r/{c['subreddit']}")
        print(f"   URL: {c['url']}")
        if c['top_comments']:
            print(f"   è¯„è®º: {c['top_comments'][0][:100]}...")
