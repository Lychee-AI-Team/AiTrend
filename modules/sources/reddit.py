#!/usr/bin/env python3
"""
Reddit ä¿¡æ¯æºæ¨¡å— - Pushshift API ç‰ˆæœ¬
ä½¿ç”¨ Pushshift æ— éœ€ OAuth å³å¯è®¿é—® Reddit æ•°æ®
"""

import os
import re
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any
from modules.logger import get_logger
from modules.sources.base import BaseSource

logger = get_logger()

class Reddit(BaseSource):
    """
    Reddit ä¿¡æ¯æºæ¨¡å—ï¼ˆPushshift APIï¼‰
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ Pushshift API è·å– Reddit å¸–å­
    - æ— éœ€ OAuth è®¤è¯
    - æ”¯æŒå¤š subreddit èšåˆ
    - æå–çƒ­é—¨è®¨è®ºå†…å®¹
    
    æŒ–æ˜æ ‡å‡†ï¼š
    - æŠ•ç¥¨æ•° > 50
    - è¯„è®ºæ•° > 10
    - æ—¶é—´ï¼šæœ€è¿‘7å¤©
    - æŠ€æœ¯ç›¸å…³å…³é”®è¯
    
    ä¸ HN/PH çš„å·®å¼‚ï¼š
    - è‰æ ¹ç¤¾åŒºè®¨è®º
    - çœŸå®ç”¨æˆ·ç»éªŒåˆ†äº«
    - æ›´ casual çš„è®¨è®ºæ°›å›´
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.subreddits = config.get('subreddits', ['MachineLearning', 'LocalLLaMA', 'artificial', 'technology'])
        self.min_upvotes = config.get('min_upvotes', 50)
        self.min_comments = config.get('min_comments', 10)
        self.max_candidates = config.get('max_candidates', 10)
        self.time_window = config.get('time_window', 7)  # å¤©æ•°
        
        # Pushshift APIï¼ˆæ— éœ€è®¤è¯ï¼‰
        self.pushshift_url = "https://api.pullpush.io/reddit/submission/search"
        self.reddit_url = "https://www.reddit.com"
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        logger.info(f"Reddit æ¨¡å—åˆå§‹åŒ– (Pushshift API)")
        logger.info(f"  - ç›®æ ‡ç¤¾åŒº: {', '.join(self.subreddits)}")
        logger.info(f"  - æœ€å°æŠ•ç¥¨: {self.min_upvotes}")
        logger.info(f"  - æœ€å°è¯„è®º: {self.min_comments}")
    
    def is_enabled(self) -> bool:
        """Pushshift æ— éœ€è®¤è¯ï¼Œå§‹ç»ˆå¯ç”¨"""
        return True
    
    def discover(self) -> List[Dict[str, Any]]:
        """
        å‘ç°çƒ­é—¨å¸–å­
        
        ä½¿ç”¨ Pushshift API è·å– Reddit å¸–å­
        """
        logger.section("ğŸ“¡ ä» Reddit (Pushshift) æŒ–æ˜çƒ­é—¨å¸–å­")
        
        all_posts = []
        
        # éå†æ¯ä¸ª subreddit
        for subreddit in self.subreddits:
            try:
                logger.info(f"  è·å– r/{subreddit}...")
                posts = self._fetch_subreddit_posts(subreddit)
                logger.info(f"    è·å– {len(posts)} ä¸ªå¸–å­")
                all_posts.extend(posts)
            except Exception as e:
                logger.error(f"    è·å–å¤±è´¥: {e}")
        
        # å»é‡ï¼ˆæŒ‰ URLï¼‰
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            url = post.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_posts.append(post)
        
        # æŒ‰æŠ•ç¥¨æ•°æ’åº
        sorted_posts = sorted(unique_posts, key=lambda x: x.get('upvotes', 0), reverse=True)
        
        # é™åˆ¶æ•°é‡
        result = sorted_posts[:self.max_candidates]
        
        logger.info(f"âœ… æ€»è®¡å‘ç° {len(result)} ä¸ªå€™é€‰å¸–å­")
        
        return result
    
    def _fetch_subreddit_posts(self, subreddit: str) -> List[Dict]:
        """ä½¿ç”¨ Pushshift API è·å–å¸–å­"""
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        now = int(datetime.now().timestamp())
        days_ago = now - (self.time_window * 24 * 60 * 60)
        
        params = {
            'subreddit': subreddit,
            'sort': 'desc',
            'sort_type': 'score',
            'score': f">{self.min_upvotes}",
            'num_comments': f">{self.min_comments}",
            'after': days_ago,
            'before': now,
            'size': 25,
            'fields': 'title,url,permalink,score,num_comments,created_utc,selftext'
        }
        
        try:
            response = self.session.get(self.pushshift_url, params=params, timeout=20)
            response.raise_for_status()
            
            data = response.json()
            posts_data = data.get('data', [])
            
            candidates = []
            
            for post in posts_data:
                upvotes = post.get('score', 0)
                num_comments = post.get('num_comments', 0)
                title = post.get('title', '')
                url_link = post.get('url', '')
                permalink = post.get('permalink', '')
                created_utc = post.get('created_utc', 0)
                selftext = post.get('selftext', '')
                
                # è·³è¿‡è‡ªæ‰˜ç®¡å†…å®¹
                if not url_link or url_link.startswith('/r/'):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€æœ¯ç›¸å…³
                if not self._is_tech_related(title, selftext):
                    continue
                
                candidate = {
                    'name': self._extract_project_name(title, url_link),
                    'title': title,
                    'url': url_link,
                    'reddit_url': f"https://www.reddit.com{permalink}",
                    'upvotes': upvotes,
                    'comments': num_comments,
                    'subreddit': subreddit,
                    'created_at': datetime.fromtimestamp(created_utc).isoformat(),
                    'source_type': 'reddit',
                    'source_name': 'Reddit'
                }
                
                candidates.append(candidate)
            
            return candidates
            
        except Exception as e:
            logger.error(f"  Pushshift è¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def _is_tech_related(self, title: str, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€æœ¯ç›¸å…³å†…å®¹"""
        content = f"{title} {text}".lower()
        
        tech_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'llm', 'gpt', 'claude', 'gemini',
            'github', 'open source', 'developer', 'programming',
            'python', 'javascript', 'rust', 'go',
            'startup', 'tech', 'software', 'app'
        ]
        
        return any(keyword in content for keyword in tech_keywords)
    
    def _extract_project_name(self, title: str, url: str) -> str:
        """ä»æ ‡é¢˜æˆ– URL æå–é¡¹ç›®å"""
        # å°è¯•ä»æ ‡é¢˜æå–
        match = re.match(r'^([^-â€“:]+)', title)
        if match:
            name = match.group(1).strip()
            if len(name) < 50:
                return name[:50]
        
        # ä» URL æå–
        if 'github.com' in url:
            parts = url.split('/')
            if len(parts) >= 3:
                return parts[-1][:50] if parts[-1] else parts[-2][:50]
        
        # é»˜è®¤è¿”å›æ ‡é¢˜å‰50å­—
        return title[:50]
    
    def get_details(self, candidate: Dict) -> Dict[str, Any]:
        """è·å–å¸–å­è¯¦ç»†ä¿¡æ¯"""
        return candidate

# æµ‹è¯•
if __name__ == '__main__':
    print("="*60)
    print("Reddit ä¿¡æ¯æºæ¨¡å—æµ‹è¯• (Pushshift)")
    print("="*60)
    
    config = {
        'subreddits': ['MachineLearning', 'technology'],
        'min_upvotes': 30,
        'min_comments': 5,
        'max_candidates': 5
    }
    
    source = Reddit(config)
    candidates = source.discover()
    
    print(f"\nå‘ç° {len(candidates)} ä¸ªå€™é€‰å¸–å­:")
    for i, c in enumerate(candidates, 1):
        print(f"\n{i}. {c['name']}")
        print(f"   æ ‡é¢˜: {c['title'][:60]}...")
        print(f"   æŠ•ç¥¨: {c['upvotes']}, è¯„è®º: {c['comments']}")
        print(f"   ç¤¾åŒº: r/{c['subreddit']}")
        print(f"   URL: {c['url']}")
