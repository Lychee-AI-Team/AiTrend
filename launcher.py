"""
AiTrend æ™ºèƒ½å¯åŠ¨ä¸­æ¢ v4

æ”¯æŒå¤šç§è§¦å‘æœºåˆ¶ï¼š
1. å‘½ä»¤è¡Œå‚æ•°è§¦å‘
2. å®šæ—¶ä»»åŠ¡è§¦å‘
3. æ‰‹åŠ¨æŒ‡å®šå¹³å°å’Œæ•°é‡
4. æ™ºèƒ½è¡¥å…¨æœºåˆ¶ï¼ˆç¡®ä¿æ¯ä¸ªå¹³å°è‡³å°‘3æ¡ï¼‰
"""

import yaml
import json
import importlib
import time
import argparse
import sys
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from modules.trace_logger import TraceLogger, get_trace_logger

# ä¿¡æ¯æºæ¨¡å—æ˜ å°„è¡¨
SOURCE_MAP = {
    'github_trend': ('modules.sources.github_trend', 'GithubTrend'),
    'producthunt': ('modules.sources.producthunt', 'Producthunt'),
    'hackernews': ('modules.sources.hackernews', 'Hackernews'),
    'reddit': ('modules.sources.reddit', 'Reddit'),
    'arxiv_papers': ('modules.sources.arxiv_papers', 'ArxivPapers'),
    'twitter': ('modules.sources.twitter', 'TwitterSource'),
}

# å¹³å°åç§°æ˜ å°„
PLATFORM_NAMES = {
    'github': 'github_trend',
    'github_trend': 'github_trend',
    'producthunt': 'producthunt',
    'ph': 'producthunt',
    'hackernews': 'hackernews',
    'hn': 'hackernews',
    'reddit': 'reddit',
    'arxiv': 'arxiv_papers',
    'twitter': 'twitter',
    'x': 'twitter',
}


class Launcher:
    """æ™ºèƒ½å¯åŠ¨ä¸­æ¢ - æ”¯æŒå¤šç§è§¦å‘æœºåˆ¶"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.sources = []
        self.processors = []
        self.output_module = None
        self.publishers = []
        self.trace_logger = get_trace_logger()
        
    def _load_config(self, path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            return self._default_config()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é…ç½®å¤±è´¥: {e}")
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'sources': {k: {'enabled': True} for k in SOURCE_MAP.keys()},
            'processors': {},
            'output': {'style': 'natural_narrative'},
            'publishers': {'forum': {'enabled': True}},
            'system': {'max_iterations': 10, 'enable_trace': True}
        }
    
    def init_modules(self, specific_sources: List[str] = None):
        """åˆå§‹åŒ–æ¨¡å—ï¼ˆå¯æŒ‡å®šç‰¹å®šæºï¼‰"""
        print("ğŸš€ å¯åŠ¨ä¸­æ¢åˆå§‹åŒ–...")
        
        sources_to_init = specific_sources or list(self.config.get('sources', {}).keys())
        
        # åˆå§‹åŒ–ä¿¡æ¯æº
        print("\nğŸ“¡ åˆå§‹åŒ–ä¿¡æ¯æºæ¨¡å—...")
        for source_name in sources_to_init:
            source_config = self.config.get('sources', {}).get(source_name, {})
            if not source_config.get('enabled', False):
                continue
                
            try:
                if source_name in SOURCE_MAP:
                    module_path, class_name = SOURCE_MAP[source_name]
                    module = importlib.import_module(module_path)
                    source_class = getattr(module, class_name)
                    source_instance = source_class(source_config)
                    self.sources.append(source_instance)
                    print(f"  âœ… {source_name}")
                else:
                    print(f"  âŒ {source_name}: æœªçŸ¥æ¨¡å—")
            except Exception as e:
                print(f"  âŒ {source_name}: {e}")
        
        # åˆå§‹åŒ–è¾“å‡ºæ¨¡å—
        print("\nâœï¸ åˆå§‹åŒ–è¾“å‡ºæ¨¡å—...")
        try:
            from modules.output.narrative_composer import NarrativeComposer
            self.output_module = NarrativeComposer(self.config.get('output', {}))
            print(f"  âœ… narrative_composer")
        except Exception as e:
            print(f"  âš ï¸ {e}")
        
        print(f"\nğŸ“Š æ¨¡å—åŠ è½½: {len(self.sources)} ä¸ªæº")
    
    def ensure_minimum_content(self, min_per_source: int = 3) -> List[Dict]:
        """
        ç¡®ä¿æ¯ä¸ªå¹³å°è‡³å°‘è·å–æŒ‡å®šæ•°é‡çš„å†…å®¹
        
        Args:
            min_per_source: æ¯ä¸ªæºçš„æœ€å°å†…å®¹æ•°ï¼Œé»˜è®¤3
        
        Returns:
            æ‰€æœ‰å†…å®¹åˆ—è¡¨
        """
        all_candidates = []
        enable_trace = self.config.get('system', {}).get('enable_trace', True)
        
        print("\n" + "="*60)
        print(f"æ™ºèƒ½å†…å®¹é‡‡é›† (æ¯æºè‡³å°‘{min_per_source}æ¡)")
        print("="*60)
        
        for source in self.sources:
            try:
                source_name = getattr(source, 'name', source.__class__.__name__)
                print(f"\nğŸ“¡ {source_name} æŒ–æ˜...")
                
                candidates = []
                attempts = 0
                max_attempts = 3
                
                # å°è¯•å¤šæ¬¡è·å–è¶³å¤Ÿå†…å®¹
                while len(candidates) < min_per_source and attempts < max_attempts:
                    attempts += 1
                    
                    # åŠ¨æ€è°ƒæ•´é…ç½®ä»¥è·å–æ›´å¤šå†…å®¹
                    if hasattr(source, 'config'):
                        original_max = source.config.get('max_results', 10)
                        # ä¸´æ—¶å¢åŠ è·å–æ•°é‡
                        source.config['max_results'] = min_per_source * 3
                    
                    new_candidates = source.discover()
                    
                    # æ¢å¤åŸå§‹é…ç½®
                    if hasattr(source, 'config') and 'max_results' in source.config:
                        source.config['max_results'] = original_max
                    
                    # è¿‡æ»¤å·²å­˜åœ¨çš„
                    existing_ids = {c.get('url', '') for c in candidates}
                    for c in new_candidates:
                        if c.get('url', '') not in existing_ids:
                            c['source_name'] = source_name
                            if enable_trace:
                                c['trace_id'] = self.trace_logger.generate_trace_id(c)
                                self.trace_logger.create_trace(c['trace_id'], c)
                                self.trace_logger.log_source_discover(
                                    c['trace_id'], source_name, len(new_candidates)
                                )
                            candidates.append(c)
                            existing_ids.add(c.get('url', ''))
                    
                    if len(candidates) < min_per_source:
                        print(f"  âš ï¸ ç¬¬{attempts}æ¬¡è·å–: {len(candidates)}/{min_per_source} æ¡")
                        time.sleep(1)
                    else:
                        break
                
                # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œè®°å½•è­¦å‘Šä½†ä»ç„¶ä½¿ç”¨
                if len(candidates) < min_per_source:
                    print(f"  âš ï¸ æœ€ç»ˆè·å–: {len(candidates)}/{min_per_source} æ¡ (ä¸è¶³)")
                else:
                    print(f"  âœ… è·å– {len(candidates)} æ¡ (æ»¡è¶³è¦æ±‚)")
                
                all_candidates.extend(candidates[:max(min_per_source * 2, len(candidates))])
                
            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š æ€»è®¡: {len(all_candidates)} æ¡å†…å®¹")
        return all_candidates
    
    def process_content(self, candidates: List[Dict], max_total: int = None) -> List[Dict]:
        """å¤„ç†å†…å®¹ï¼ˆç”Ÿæˆä¸­æ–‡ä»‹ç»ï¼‰- ç¡®ä¿æ¯ä¸ªå¹³å°éƒ½æœ‰å†…å®¹"""
        results = []
        enable_trace = self.config.get('system', {}).get('enable_trace', True)
        
        # æŒ‰å¹³å°åˆ†ç»„ï¼Œç¡®ä¿æ¯ä¸ªå¹³å°éƒ½æœ‰å†…å®¹è¢«å¤„ç†
        source_groups = {}
        for c in candidates:
            src = c.get('source_name', 'Unknown')
            if src not in source_groups:
                source_groups[src] = []
            source_groups[src].append(c)
        
        # è®¡ç®—æ¯ä¸ªå¹³å°çš„æœ€å¤§æ•°é‡
        num_sources = len(source_groups)
        if max_total and num_sources > 0:
            per_source = max_total // num_sources
            # ä»æ¯ä¸ªå¹³å°å–ç›¸åŒæ•°é‡
            balanced_candidates = []
            for src, items in source_groups.items():
                balanced_candidates.extend(items[:per_source])
            candidates = balanced_candidates
            print(f"   å¹³è¡¡åˆ†é…: {num_sources}ä¸ªå¹³å°, æ¯å¹³å°æœ€å¤š{per_source}æ¡")
        
        print(f"   å¾…å¤„ç†: {len(candidates)} æ¡å†…å®¹")
        
        print("\n" + "="*60)
        print("å†…å®¹å¤„ç†ä¸ç”Ÿæˆ")
        print("="*60)
        
        for i, candidate in enumerate(candidates, 1):
            trace_id = candidate.get('trace_id', 'unknown')
            name = candidate.get('name', 'Unknown')[:40]
            source = candidate.get('source_name', 'Unknown')
            
            print(f"\n[{i}/{len(candidates)}] {source}: {name}")
            print(f"   Trace: {trace_id}")
            
            try:
                # Twitterè´¨é‡ç­›é€‰
                if source == 'Twitter' and candidate.get('meets_data_threshold'):
                    from modules.processors.twitter_quality_filter import TwitterQualityFilter
                    filter_proc = TwitterQualityFilter()
                    filtered = filter_proc.process(candidate)
                    if not filtered:
                        print(f"   âŒ æœªé€šè¿‡è´¨é‡ç­›é€‰")
                        continue
                    candidate = filtered
                
                # ç”Ÿæˆå†…å®¹ - ä½¿ç”¨LLMç”Ÿæˆä¸­æ–‡æ€»ç»“
                if 'arxiv_id' in candidate:
                    # arXivè®ºæ–‡ä½¿ç”¨ä¸“ç”¨composer
                    from modules.output.arxiv_composer import ArxivContentComposer
                    composer = ArxivContentComposer()
                    final_content = composer.compose_narrative(candidate)
                elif source == 'Twitter':
                    # Twitterä½¿ç”¨ä¸“ç”¨composer
                    from modules.output.twitter_composer import TwitterContentComposer
                    composer = TwitterContentComposer()
                    final_content = composer.compose_narrative(candidate)
                else:
                    # GitHub/PH/HN/Redditç­‰ä½¿ç”¨LLMç”Ÿæˆä¸­æ–‡æ€»ç»“
                    final_content = self._generate_chinese_summary(candidate)
                
                # æ·»åŠ trace_id
                final_content_with_id = self._add_trace_id(final_content, trace_id)
                
                if enable_trace:
                    self.trace_logger.set_final_output(trace_id, final_content)
                
                print(f"   âœ… ç”Ÿæˆå®Œæˆ ({len(final_content)}å­—)")
                
                results.append({
                    'name': candidate.get('name', ''),
                    'source': source,
                    'content': final_content_with_id,
                    'raw_content': final_content,
                    'url': candidate.get('url', ''),
                    'trace_id': trace_id,
                })
                
            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
        
        return results
    
    def publish(self, contents: List[Dict]):
        """å‘å¸ƒå†…å®¹"""
        if not contents:
            print("\nâš ï¸ æ— å†…å®¹éœ€è¦å‘å¸ƒ")
            return
        
        print("\n" + "="*60)
        print("å†…å®¹å‘å¸ƒ")
        print("="*60)
        
        for pub_name, pub_config in self.config.get('publishers', {}).items():
            if not pub_config.get('enabled', False):
                continue
            
            try:
                print(f"\nğŸ“¤ {pub_name}...")
                
                if pub_name == 'forum':
                    self._publish_to_forum(contents, pub_config)
                elif pub_name == 'text':
                    self._publish_to_text(contents, pub_config)
                    
            except Exception as e:
                print(f"  âŒ {e}")
    
    def _publish_to_forum(self, contents: List[Dict], config: Dict):
        """å‘å¸ƒåˆ°è®ºå›"""
        from publishers import create_publisher
        
        try:
            publisher = create_publisher('forum', config)
            
            forums_contents = []
            for c in contents:
                forums_contents.append({
                    'name': c['name'],
                    'content': c['content'],
                    'url': c['url'],
                    'source': c['source']
                })
            
            published = publisher.publish_batch(forums_contents)
            print(f"  âœ… {published}/{len(contents)} æ¡æˆåŠŸ")
            
            for c in contents:
                if c.get('trace_id'):
                    self.trace_logger.log_publish(
                        c['trace_id'], 'forum_publisher', published > 0
                    )
            
        except Exception as e:
            print(f"  âŒ {e}")
    
    def _publish_to_text(self, contents: List[Dict], config: Dict):
        """å‘å¸ƒåˆ°æ–‡å­—é¢‘é“"""
        from publishers import create_publisher
        
        try:
            publisher = create_publisher('text', config)
            
            text_contents = []
            for c in contents:
                text_contents.append({
                    'name': c['name'],
                    'content': c['content'],
                    'url': c['url'],
                    'source': c['source']
                })
            
            published = publisher.publish_batch(text_contents)
            print(f"  âœ… {published}/{len(contents)} æ¡æˆåŠŸ")
            
        except Exception as e:
            print(f"  âŒ {e}")
    
    def _generate_chinese_summary(self, candidate: Dict) -> str:
        """
        ä½¿ç”¨LLMç”Ÿæˆä¸­æ–‡å†…å®¹æ€»ç»“
        
        é’ˆå¯¹GitHub/PH/HN/Redditç­‰å¹³å°ç”Ÿæˆä¸­æ–‡ä»‹ç»
        """
        import os
        import requests
        
        source = candidate.get('source_name', 'Unknown')
        name = candidate.get('name', '')
        description = candidate.get('description', '')
        url = candidate.get('url', '')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰API Key (æ”¯æŒGemini/OpenAI/Kimi)
        api_key = os.getenv('GEMINI_API_KEY') or os.getenv('OPENAI_API_KEY') or os.getenv('KIMI_API_KEY')
        
        if not api_key:
            raise RuntimeError(f"æœªé…ç½®LLM API Key (GEMINI_API_KEY/OPENAI_API_KEY/KIMI_API_KEY)ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹: {source}/{name}")
        
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯æŠ€æœ¯å†…å®¹ç¼–è¾‘ï¼Œæ“…é•¿ç”¨è‡ªç„¶çš„ä¸­æ–‡ä»‹ç»å¼€æºé¡¹ç›®ã€‚
è¦æ±‚ï¼š
1. ç”¨è¿ç»­æ®µè½å™è¿°ï¼Œä¸è¦åˆ—è¡¨ã€åºå·
2. è¯´æ˜é¡¹ç›®æ˜¯ä»€ä¹ˆã€èƒ½åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
3. çªå‡ºæœ€ç‰¹åˆ«çš„åŠŸèƒ½æˆ–äº®ç‚¹
4. 300å­—ä»¥å†…ï¼Œå£è¯­åŒ–è¡¨è¾¾
5. ä¸è¦"æœ€è¿‘"ã€"åˆšåˆš"ç­‰æ—¶é—´è¯"""

        user_prompt = f"è¯·ä»‹ç»ä»¥ä¸‹{source}é¡¹ç›®ï¼š\n\né¡¹ç›®åç§°ï¼š{name}\n\né¡¹ç›®æè¿°ï¼š{description[:1000]}"

        print(f"   ğŸ¤– LLMç”Ÿæˆä¸­æ–‡æ€»ç»“...", end=' ')
        
        try:
            # ä½¿ç”¨é¡¹ç›®ä¸­å·²æœ‰çš„llm_clientæ¨¡å—
            print(f"   ğŸ¤– ä½¿ç”¨OpenClawå¤§æ¨¡å‹ç”Ÿæˆ...", end=' ')
            
            from modules.llm_client import LLMClient
            import time
            
            client = LLMClient()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé€Ÿç‡é™åˆ¶
            time.sleep(1)
            
            chinese_content = client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.5,
                max_tokens=800
            )
            
            if not chinese_content or len(chinese_content) <= 50:
                raise RuntimeError(f"LLMç”Ÿæˆå†…å®¹å¤±è´¥æˆ–å†…å®¹å¤ªçŸ­: {source}/{name}")
            
            # ç¡®ä¿åŒ…å«é“¾æ¥
            if url not in chinese_content:
                chinese_content += f"\n\nğŸ”— {url}"
            print("âœ…")
            return chinese_content
                
        except Exception as e:
            # LLMè°ƒç”¨å¤±è´¥ï¼Œç«‹å³æŠ¥é”™é€€å‡º
            raise RuntimeError(f"LLMè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹: {source}/{name}. é”™è¯¯: {e}") from e
    
    def _fallback_compose(self, candidate: Dict) -> str:
        """å¤‡ç”¨å†…å®¹åˆæˆï¼ˆå½“LLMå¤±è´¥æ—¶ä½¿ç”¨ï¼‰- ç”Ÿæˆä¸­æ–‡æè¿°"""
        import re
        
        name = candidate.get('name', '')
        source = candidate.get('source_name', 'Unknown')
        url = candidate.get('url', '')
        description = candidate.get('description', '') or candidate.get('title', '')
        
        # ä»è‹±æ–‡æè¿°ä¸­æå–å…³é”®ä¿¡æ¯
        # æ¸…ç†æè¿°
        desc_clean = re.sub(r'http\S+', '', description)  # ç§»é™¤é“¾æ¥
        desc_clean = re.sub(r'[#*`]', '', desc_clean)     # ç§»é™¤markdown
        desc_clean = desc_clean.strip()
        
        # æå–å…³é”®è¯
        keywords = []
        tech_keywords = ['AI', 'ML', 'LLM', 'GPT', 'API', 'tool', 'framework', 'library']
        for kw in tech_keywords:
            if kw.lower() in desc_clean.lower():
                keywords.append(kw)
        
        # æ„å»ºä¸­æ–‡æè¿°
        parts = [f"**{name}**"]
        parts.append("")
        
        # æ ¹æ®æ¥æºç”Ÿæˆä¸åŒçš„æè¿°
        if 'github' in source.lower():
            parts.append(f"è¿™æ˜¯ä¸€ä¸ªGitHubä¸Šçš„å¼€æºé¡¹ç›®ã€‚{desc_clean[:150]}..." if len(desc_clean) > 150 else f"è¿™æ˜¯ä¸€ä¸ªGitHubä¸Šçš„å¼€æºé¡¹ç›®ã€‚{desc_clean}")
            if keywords:
                parts.append(f"é¡¹ç›®æ¶‰åŠ{ 'ã€'.join(keywords[:3]) }ç­‰æŠ€æœ¯ã€‚")
            parts.append("é€‚åˆå¼€å‘è€…å…³æ³¨å’Œå­¦ä¹ ã€‚")
        elif 'producthunt' in source.lower():
            parts.append(f"è¿™æ˜¯ä¸€ä¸ªProduct Huntä¸Šçš„æ–°äº§å“ã€‚{desc_clean[:150]}..." if len(desc_clean) > 150 else f"è¿™æ˜¯ä¸€ä¸ªProduct Huntä¸Šçš„æ–°äº§å“ã€‚{desc_clean}")
            parts.append("å€¼å¾—å…³æ³¨å…¶äº§å“è®¾è®¡å’Œç”¨æˆ·åé¦ˆã€‚")
        elif 'hackernews' in source.lower() or 'hn' in source.lower():
            parts.append(f"è¿™æ˜¯HackerNewsä¸Šçš„çƒ­é—¨è®¨è®ºã€‚{desc_clean[:150]}..." if len(desc_clean) > 150 else f"è¿™æ˜¯HackerNewsä¸Šçš„çƒ­é—¨è®¨è®ºã€‚{desc_clean}")
            parts.append("æŠ€æœ¯ç¤¾åŒºæ­£åœ¨å…³æ³¨è¿™ä¸ªè¯é¢˜ã€‚")
        elif 'reddit' in source.lower():
            parts.append(f"è¿™æ˜¯Redditä¸Šçš„çƒ­é—¨å¸–å­ã€‚{desc_clean[:150]}..." if len(desc_clean) > 150 else f"è¿™æ˜¯Redditä¸Šçš„çƒ­é—¨å¸–å­ã€‚{desc_clean}")
            parts.append("ç¤¾åŒºç”¨æˆ·æ­£åœ¨è®¨è®ºè¿™ä¸ªå†…å®¹ã€‚")
        else:
            parts.append(f"è¿™æ˜¯æ¥è‡ª{source}çš„å†…å®¹ã€‚{desc_clean[:150]}..." if len(desc_clean) > 150 else f"è¿™æ˜¯æ¥è‡ª{source}çš„å†…å®¹ã€‚{desc_clean}")
        
        parts.append("")
        parts.append(f"ğŸ”— {url}")
        
        return '\n'.join(parts)
    
    def _generic_compose(self, candidate: Dict) -> str:
        """é€šç”¨å†…å®¹åˆæˆï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹ï¼‰"""
        return self._generate_chinese_summary(candidate)
    
    def _add_trace_id(self, content: str, trace_id: str) -> str:
        """æ·»åŠ è¿½è¸ªID"""
        footer = f"\n\n{trace_id}"
        return content + footer


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='AiTrend æ™ºèƒ½å¯åŠ¨ä¸­æ¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python3 launcher.py                    # è¿è¡Œæ‰€æœ‰æºï¼Œæ¯æºè‡³å°‘3æ¡
  python3 launcher.py --source github    # åªè¿è¡ŒGitHub
  python3 launcher.py --source twitter --min 5   # Twitterè‡³å°‘5æ¡
  python3 launcher.py --all --min 3      # æ‰€æœ‰æºï¼Œæ¯æº3æ¡
  python3 launcher.py --diagnose AIT-xxx # è¯Šæ–­æŒ‡å®šID
        '''
    )
    
    parser.add_argument('--source', '-s', action='append',
                       help='æŒ‡å®šä¿¡æ¯æºï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼Œå¦‚: github twitterï¼‰')
    parser.add_argument('--min', '-m', type=int, default=3,
                       help='æ¯æºæœ€å°å†…å®¹æ•°ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--max-total', type=int,
                       help='æ€»å†…å®¹æ•°ä¸Šé™')
    parser.add_argument('--all', action='store_true',
                       help='å¯ç”¨æ‰€æœ‰é…ç½®çš„æº')
    parser.add_argument('--diagnose', metavar='TRACE_ID',
                       help='è¯Šæ–­æŒ‡å®šè¿½è¸ªID')
    parser.add_argument('--recent', action='store_true',
                       help='åˆ—å‡ºæœ€è¿‘çš„è¿½è¸ªè®°å½•')
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œï¼ˆä¸å‘å¸ƒï¼‰')
    
    return parser.parse_args()


def main():
    """ä¸»å…¥å£"""
    args = parse_args()
    
    # è¯Šæ–­æ¨¡å¼
    if args.diagnose:
        print(get_trace_logger().diagnose(args.diagnose))
        return
    
    # æœ€è¿‘è®°å½•æ¨¡å¼
    if args.recent:
        traces = get_trace_logger().list_recent(20)
        print("ğŸ“‹ æœ€è¿‘çš„è¿½è¸ªè®°å½•:\n")
        print(f"{'è¿½è¸ªID':<30} {'ä¿¡æ¯æº':<12} {'çŠ¶æ€':<8} {'åç§°':<40}")
        print("-" * 95)
        for t in traces:
            status_icon = "âœ…" if t['status'] == 'completed' else "âŒ" if t['status'] == 'error' else "â³"
            name = t['name'][:38] if len(t['name']) > 38 else t['name']
            print(f"{t['trace_id']:<30} {t['source']:<12} {status_icon} {t['status']:<6} {name}")
        return
    
    print("="*60)
    print("ğŸ¯ AiTrend æ™ºèƒ½å¯åŠ¨ä¸­æ¢ v4")
    print("="*60)
    
    # ç¡®å®šè¦è¿è¡Œçš„æº
    sources_to_run = None
    if args.source:
        # è§£æå¹³å°åç§°åˆ«å
        sources_to_run = []
        for s in args.source:
            normalized = PLATFORM_NAMES.get(s.lower(), s)
            if normalized in SOURCE_MAP:
                sources_to_run.append(normalized)
            else:
                print(f"âš ï¸ æœªçŸ¥å¹³å°: {s}")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env()
    
    # åˆ›å»ºå¯åŠ¨å™¨
    launcher = Launcher()
    launcher.init_modules(sources_to_run)
    
    if not launcher.sources:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„ä¿¡æ¯æº")
        return
    
    # ç¡®ä¿æ¯ä¸ªå¹³å°è‡³å°‘è·å–æŒ‡å®šæ•°é‡
    candidates = launcher.ensure_minimum_content(min_per_source=args.min)
    
    if not candidates:
        print("\nâŒ æœªè·å–åˆ°ä»»ä½•å†…å®¹")
        return
    
    # å¤„ç†å†…å®¹
    results = launcher.process_content(candidates, max_total=args.max_total)
    
    if not results:
        print("\nâŒ æœªç”Ÿæˆä»»ä½•å†…å®¹")
        return
    
    # å‘å¸ƒï¼ˆå¦‚æœä¸æ˜¯dry-runï¼‰
    if not args.dry_run:
        launcher.publish(results)
    else:
        print("\nğŸ“ Dry-runæ¨¡å¼ï¼Œè·³è¿‡å‘å¸ƒ")
        print(f"   å°†å‘å¸ƒ {len(results)} æ¡å†…å®¹")
    
    # ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æ‰§è¡Œç»Ÿè®¡")
    print("="*60)
    
    source_counts = {}
    for r in results:
        src = r['source']
        source_counts[src] = source_counts.get(src, 0) + 1
    
    print("\nå„å¹³å°å‘å¸ƒæ•°é‡:")
    for src, count in sorted(source_counts.items()):
        status = "âœ…" if count >= args.min else "âš ï¸"
        print(f"  {status} {src}: {count} æ¡")
    
    print(f"\næ€»è®¡: {len(results)} æ¡å†…å®¹")
    print("="*60)


def load_env():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    import os
    env_path = '.env'
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if '=' in line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    os.environ[key] = value
        print("âœ… ç¯å¢ƒå˜é‡å·²åŠ è½½")


if __name__ == '__main__':
    main()
