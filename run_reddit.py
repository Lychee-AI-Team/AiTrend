#!/usr/bin/env python3
"""
Reddit æµç¨‹ - ç›´æ¥å‘å¸ƒåˆ°è®ºå›
"""

import os
import sys
import time
from typing import List, Dict, Any

sys.path.insert(0, '.')

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = '.env'
if os.path.exists(env_path):
    with open(env_path, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from modules.logger import get_logger
from publishers import create_publisher

logger = get_logger()

def fetch_from_reddit() -> List[Dict]:
    """ä» Reddit (Pushshift) è·å–çƒ­é—¨å¸–å­"""
    logger.section("ğŸ“¡ ä» Reddit (Pushshift) æŒ–æ˜çƒ­é—¨å¸–å­")
    
    from modules.sources.reddit import Reddit
    
    config = {
        'subreddits': ['MachineLearning', 'LocalLLaMA', 'artificial', 'technology'],
        'min_upvotes': 50,
        'min_comments': 10,
        'max_candidates': 5,
        'time_window': 7
    }
    
    source = Reddit(config)
    candidates = source.discover()
    
    for c in candidates:
        c['source_name'] = 'Reddit'
    
    return candidates

def build_prompt(candidate: Dict, index: int) -> str:
    """æ„å»ºLLMæç¤ºè¯"""
    name = candidate.get('name', '')
    title = candidate.get('title', '')
    url = candidate.get('url', '')
    reddit_url = candidate.get('reddit_url', '')
    upvotes = candidate.get('upvotes', 0)
    comments = candidate.get('comments', 0)
    subreddit = candidate.get('subreddit', '')
    top_comments = candidate.get('top_comments', [])
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = [f"æ ‡é¢˜: {title}"]
    
    if upvotes:
        context_parts.append(f"Reddit æŠ•ç¥¨: {upvotes}")
    
    if comments:
        context_parts.append(f"è¯„è®ºæ•°: {comments}")
    
    if subreddit:
        context_parts.append(f"ç¤¾åŒº: r/{subreddit}")
    
    if top_comments:
        context_parts.append(f"çƒ­è¯„: {top_comments[0][:200]}")
    
    context = "\n".join(context_parts)
    
    # å¤šæ ·åŒ–å¼€å¤´
    styles = ["ä»ç¤¾åŒºåå“åˆ‡å…¥", "ä»å®é™…ç”¨é€”åˆ‡å…¥", "ä»æŠ€æœ¯äº®ç‚¹åˆ‡å…¥"]
    style = styles[index % len(styles)]
    
    return f"""ä»‹ç»ä»¥ä¸‹ Reddit çƒ­é—¨å¸–å­ï¼š

{context}

è¦æ±‚ï¼š
1. âŒ ç¦æ­¢"æœ€è¿‘å‘ç°"ã€"ä»Šå¤©çœ‹åˆ°"ç­‰å¥—è¯å¼€å¤´
2. âŒ ç¦æ­¢ç¬¬ä¸€ç¬¬äºŒã€é¦–å…ˆå…¶æ¬¡ç­‰åºå·
3. âŒ ç¦æ­¢åˆ—è¡¨ç¬¦å·ï¼ˆ- * â€¢ï¼‰
4. âŒ ç¦æ­¢é‡å¤ç”¨è¯
5. âŒ ç¦æ­¢ç©ºè¯ï¼šé’ˆå¯¹ç—›ç‚¹ã€åŠŸèƒ½è®¾è®¡ã€æ¶æ„æ¸…æ™°ã€æ—¨åœ¨è§£å†³
6. âœ… ç›´æ¥æè¿°å†…å®¹æ˜¯ä»€ä¹ˆã€Redditç¤¾åŒºä¸ºä»€ä¹ˆè®¨è®ºå®ƒ
7. âœ… æåŠr/{subreddit}ç¤¾åŒºçš„ç‰¹ç‚¹
8. âœ… è¿ç»­æ®µè½ï¼Œ300å­—ä»¥å†…
9. âœ… æœ€åå¿…é¡»åŒ…å«å†…å®¹é“¾æ¥å’ŒRedditè®¨è®ºé“¾æ¥

å†…å®¹é“¾æ¥: {url}
Redditè®¨è®º: {reddit_url}

å¼€å¤´é£æ ¼: {style}"""

def generate_contents(candidates: List[Dict]) -> List[Dict]:
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹"""
    logger.section("ğŸ“ ç”Ÿæˆå†…å®¹ï¼ˆå‰3ä¸ªå¸–å­ï¼‰")
    
    generated = []
    
    for i, candidate in enumerate(candidates[:3], 1):
        name = candidate.get('name', '')
        logger.info(f"\n{i}. {name}")
        
        prompt = build_prompt(candidate, i-1)
        
        # è°ƒç”¨å¤§æ¨¡å‹ï¼ˆä½¿ç”¨ subprocessï¼‰
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write(prompt)
            task_file = f.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            output_file = f.name
        
        script_content = f'''
import sys
sys.path.insert(0, '.')

with open("{task_file}", "r", encoding="utf-8") as f:
    task = f.read()

from tools import sessions_spawn

result = sessions_spawn(task=task, timeout_seconds=120)

with open("{output_file}", "w", encoding="utf-8") as f:
    f.write(result if result else "")
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
            f.write(script_content)
            script_file = f.name
        
        try:
            subprocess.run(
                ['python3', script_file],
                capture_output=True,
                text=True,
                timeout=180,
                cwd='/home/ubuntu/.openclaw/workspace/AiTrend'
            )
            
            with open(output_file, 'r', encoding='utf-8') as f:
                result = f.read().strip()
            
            for f in [task_file, output_file, script_file]:
                try:
                    if os.path.exists(f):
                        os.unlink(f)
                except:
                    pass
            
            if result:
                url = candidate.get('url', '')
                reddit_url = candidate.get('reddit_url', '')
                
                if url and url not in result:
                    result += f"\n\n{url}"
                if reddit_url and reddit_url not in result:
                    result += f"\nReddit: {reddit_url}"
                
                logger.info(f"   âœ… ç”ŸæˆæˆåŠŸ ({len(result)} å­—ç¬¦)")
                generated.append({
                    'name': name,
                    'content': result,
                    'url': url,
                    'source': 'Reddit'
                })
            else:
                logger.error(f"   âŒ ç”Ÿæˆå¤±è´¥: æ— è¾“å‡º")
        except Exception as e:
            logger.error(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        time.sleep(2)
    
    return generated

def publish_contents(contents: List[Dict]) -> int:
    """ç›´æ¥å‘å¸ƒåˆ° Discord è®ºå›"""
    logger.section("ğŸ“¤ ç›´æ¥å‘å¸ƒåˆ° Discord è®ºå›")
    
    config = {
        'webhook_url': os.getenv('DISCORD_WEBHOOK_URL'),
        'thread_name': '{name} â€“ {source}',
        'username': 'AiTrend',
        'delay': 2
    }
    
    publisher = create_publisher('forum', config)
    
    if not publisher:
        logger.error("âŒ åˆ›å»ºå‘å¸ƒæ¨¡å—å¤±è´¥")
        return 0
    
    return publisher.publish_batch(contents)

def main():
    """ä¸»æµç¨‹"""
    logger.section("ğŸ¯ Reddit æµç¨‹å¯åŠ¨")
    
    # 1. è·å–å¸–å­
    candidates = fetch_from_reddit()
    
    if not candidates:
        logger.error("âŒ æœªè·å–åˆ°å¸–å­")
        return
    
    # 2. ç”Ÿæˆå†…å®¹
    generated = generate_contents(candidates)
    
    logger.info(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(generated)} æ¡å†…å®¹")
    
    # 3. ç›´æ¥å‘å¸ƒåˆ°è®ºå›
    if generated:
        published = publish_contents(generated)
        logger.section(f"âœ… æµç¨‹å®Œæˆï¼å·²å‘å¸ƒ {published} æ¡åˆ°è®ºå›")
    else:
        logger.warning("âš ï¸ æ²¡æœ‰å†…å®¹å¯å‘å¸ƒ")

if __name__ == '__main__':
    main()
